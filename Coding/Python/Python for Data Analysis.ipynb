{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=1, b=2, c=3\n",
      "a=4, b=5, c=6\n",
      "a=7, b=8, c=9\n"
     ]
    }
   ],
   "source": [
    "# A common use of variable unpacking is iterating over sequences of tuples or lists:\n",
    "seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "for a, b, c in seq:\n",
    "    print(f'a={a}, b={b}, c={c}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'red', 'baz', 'dwarf', 'foo']\n",
      "['red', 'baz', 'dwarf', 'foo']\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Elements can be removed by value using remove(), which removes the first occurrence of the value\n",
    "b_list = []  # Initialize empty list\n",
    "b_list.append('foo')\n",
    "b_list.extend(['red', 'baz', 'dwarf', 'foo'])\n",
    "print(b_list)  # ['foo', 'red', 'baz', 'dwarf', 'foo']\n",
    "\n",
    "b_list.remove('foo')  # Removes first occurrence of 'foo'\n",
    "print(b_list)  # ['red', 'baz', 'dwarf', 'foo']\n",
    "\n",
    "# List membership check with 'in' and 'not in' keywords\n",
    "print('dwarf' in b_list)  # True\n",
    "print('dwarf' not in b_list)  # False\n",
    "\n",
    "# List membership is slower than checking with dicts and sets, as lists use linear search while dicts/sets use hash tables (constant time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, None, 'foo', 7, 8, (2, 3)]\n",
      "[4, None, 'foo', 7, 8, (2, 3)]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Concatenating lists with + creates a new list\n",
    "print([4, None, 'foo'] + [7, 8, (2, 3)])  # [4, None, 'foo', 7, 8, (2, 3)]\n",
    "\n",
    "# Use extend() to append multiple elements to an existing list\n",
    "x = [4, None, 'foo']\n",
    "x.extend([7, 8, (2, 3)])\n",
    "print(x)  # [4, None, 'foo', 7, 8, (2, 3)]\n",
    "\n",
    "# Example: list_of_lists containing multiple lists\n",
    "list_of_lists = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# extend() is more efficient than concatenation with + for large lists\n",
    "everything = []\n",
    "for chunk in list_of_lists:\n",
    "    everything.extend(chunk)\n",
    "\n",
    "print(everything)  # [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Avoid this if performance is a concern\n",
    "everything = []\n",
    "for chunk in list_of_lists:\n",
    "    everything = everything + chunk\n",
    "\n",
    "print(everything)  # [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# To summarize:\n",
    "\n",
    "# insert() is used for adding elements to the beginning of a list (or at any specific position).\n",
    "\n",
    "# extend() is used for adding elements to the end of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 7]\n",
      "['He', 'saw', 'six', 'small', 'foxes']\n"
     ]
    }
   ],
   "source": [
    "# Sorting a list in place using sort():\n",
    "a = [7, 2, 5, 1, 3]\n",
    "a.sort()  # Sorts the list in place\n",
    "print(a)  # Output: [1, 2, 3, 5, 7]\n",
    "\n",
    "# Sorting with a custom key (e.g., sorting by string length):\n",
    "b = ['saw', 'small', 'He', 'foxes', 'six']\n",
    "b.sort(key=len)  # Sorts by length of each string\n",
    "print(b)  # Output: ['He', 'saw', 'six', 'small', 'foxes']\n",
    "\n",
    "# Note: sort() modifies the list in place, and we can use a custom key for sorting.\n",
    "# The sorted() function (coming soon) creates a sorted copy of a sequence without modifying the original list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n",
      "[1, 2, 2, 2, 3, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Binary search and maintaining a sorted list using the bisect module\n",
    "import bisect\n",
    "\n",
    "# Example list\n",
    "c = [1, 2, 2, 2, 3, 4, 7]\n",
    "\n",
    "# bisect.bisect finds the position to insert an element to keep the list sorted\n",
    "print(bisect.bisect(c, 2))  # Output: 4 (position to insert 2)\n",
    "print(bisect.bisect(c, 5))  # Output: 6 (position to insert 5)\n",
    "\n",
    "# bisect.insort inserts the element at the correct position to maintain sorted order\n",
    "bisect.insort(c, 6)\n",
    "print(c)  # Output: [1, 2, 2, 2, 3, 4, 6, 7]\n",
    "\n",
    "# Important Note: The bisect functions do not check if the list is already sorted.\n",
    "# Using them on an unsorted list will work, but may give incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 5]\n",
      "[7, 2, 3, 6, 3, 5, 6, 0, 1]\n",
      "[7, 2, 3, 6, 3]\n",
      "[6, 3, 5, 6, 0, 1]\n",
      "[5, 6, 0, 1]\n",
      "[6, 3, 5, 6]\n",
      "[7, 3, 3, 6, 1]\n",
      "[1, 0, 6, 5, 3, 6, 3, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "# Slicing in Python: Selecting sections of sequences like lists\n",
    "\n",
    "# Basic slicing (start:stop)\n",
    "seq = [7, 2, 3, 7, 5, 6, 0, 1]\n",
    "print(seq[1:5])  # Output: [2, 3, 7, 5]\n",
    "\n",
    "# Slicing can also be assigned to\n",
    "seq[3:4] = [6, 3]\n",
    "print(seq)  # Output: [7, 2, 3, 6, 3, 5, 6, 0, 1]\n",
    "\n",
    "# Slicing without start or stop\n",
    "print(seq[:5])  # Output: [7, 2, 3, 6, 3]\n",
    "print(seq[3:])  # Output: [6, 3, 5, 6, 0, 1]\n",
    "\n",
    "# Negative indices slice from the end\n",
    "print(seq[-4:])  # Output: [5, 6, 0, 1]\n",
    "print(seq[-6:-2])  # Output: [6, 3, 5, 6]\n",
    "\n",
    "# Step in slicing, e.g., taking every other element\n",
    "print(seq[::2])  # Output: [7, 3, 3, 6, 1]\n",
    "\n",
    "# Reverse a sequence using step -1\n",
    "print(seq[::-1])  # Output: [1, 0, 6, 5, 3, 6, 3, 2, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 10 (547651295.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31msome_list = ['foo', 'bar', 'baz']\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 10\n"
     ]
    }
   ],
   "source": [
    "# Using enumerate to track index while iterating\n",
    "\n",
    "# Without enumerate, you'd manually track the index\n",
    "i = 0\n",
    "for value in collection:\n",
    "    # do something with value\n",
    "    i += 1\n",
    "\n",
    "# With enumerate, it returns index-value pairs\n",
    "for i, value in enumerate(collection):\n",
    "    # do something with value\n",
    "\n",
    "# Example: Mapping list values to their indices using enumerate\n",
    "some_list = ['foo', 'bar', 'baz']\n",
    "mapping = {}\n",
    "\n",
    "for i, v in enumerate(some_list):\n",
    "    mapping[v] = i\n",
    "\n",
    "print(mapping)  # Output: {'foo': 0, 'bar': 1, 'baz': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sorted function returns a new sorted list from the elements of any sequence.\n",
    "\n",
    "# Example: Sorting a list of numbers\n",
    "sorted_list = sorted([7, 1, 2, 6, 0, 3, 2])\n",
    "print(sorted_list)  # Output: [0, 1, 2, 2, 3, 6, 7]\n",
    "\n",
    "# Example: Sorting a string (returns list of characters sorted)\n",
    "sorted_string = sorted('horse race')\n",
    "print(sorted_string)  # Output: [' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\n",
    "\n",
    "# sorted function has the same arguments as the sort method on lists, such as:\n",
    "# key= (for sorting by a custom criteria) and reverse= (for descending order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The zip function pairs up the elements of multiple sequences (lists, tuples, etc.) to create tuples.\n",
    "# It creates an iterator of tuples.\n",
    "\n",
    "# Example: zipping two lists\n",
    "seq1 = ['foo', 'bar', 'baz']\n",
    "seq2 = ['one', 'two', 'three']\n",
    "zipped = zip(seq1, seq2)\n",
    "print(list(zipped))  # Output: [('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\n",
    "\n",
    "# zip can handle an arbitrary number of sequences, and the number of tuples produced\n",
    "# is determined by the shortest sequence:\n",
    "seq3 = [False, True]\n",
    "print(list(zip(seq1, seq2, seq3)))  # Output: [('foo', 'one', False), ('bar', 'two', True)]\n",
    "\n",
    "# A common use of zip is to iterate over multiple sequences simultaneously:\n",
    "for i, (a, b) in enumerate(zip(seq1, seq2)):\n",
    "    print(f'{i}: {a}, {b}')\n",
    "# Output:\n",
    "# 0: foo, one\n",
    "# 1: bar, two\n",
    "# 2: baz, three\n",
    "\n",
    "# \"Unzipping\" a sequence of tuples:\n",
    "pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens'), ('Schilling', 'Curt')]\n",
    "first_names, last_names = zip(*pitchers)\n",
    "print(first_names)  # Output: ('Nolan', 'Roger', 'Schilling')\n",
    "print(last_names)   # Output: ('Ryan', 'Clemens', 'Curt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reversed function iterates over the elements of a sequence in reverse order.\n",
    "# It returns a generator, which doesn't create the reversed sequence until materialized.\n",
    "\n",
    "# Example: using reversed on a range\n",
    "print(list(reversed(range(10))))  # Output: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "\n",
    "# reversed can also be used in a for loop:\n",
    "for num in reversed(range(5)):\n",
    "    print(num)\n",
    "# Output:\n",
    "# 4\n",
    "# 3\n",
    "# 2\n",
    "# 1\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary\n",
    "empty_dict = {}\n",
    "d1 = {'a': 'some value', 'b': [1, 2, 3, 4]}\n",
    "\n",
    "# Accessing elements using keys\n",
    "print(d1['a'])  # Output: 'some value'\n",
    "print(d1['b'])  # Output: [1, 2, 3, 4]\n",
    "\n",
    "# Adding new key-value pairs\n",
    "d1[7] = 'an integer'\n",
    "print(d1)  # Output: {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n",
    "\n",
    "# Checking if a key exists in the dictionary\n",
    "print('b' in d1)  # Output: True\n",
    "\n",
    "# Deleting elements using `del` or `pop`\n",
    "del d1[7]  # Removes the key-value pair for key 7\n",
    "print(d1)  # Output: {'a': 'some value', 'b': [1, 2, 3, 4]}\n",
    "\n",
    "# Using pop to delete and get the value\n",
    "ret = d1.pop('b')  # Removes and returns the value associated with key 'b'\n",
    "print(ret)  # Output: [1, 2, 3, 4]\n",
    "print(d1)  # Output: {'a': 'some value'}\n",
    "\n",
    "# Getting keys and values as lists\n",
    "print(list(d1.keys()))  # Output: ['a']\n",
    "print(list(d1.values()))  # Output: ['some value']\n",
    "\n",
    "# Merging dictionaries using `update`\n",
    "d1.update({'b': 'foo', 'c': 12})\n",
    "print(d1)  # Output: {'a': 'some value', 'b': 'foo', 'c': 12}\n",
    "\n",
    "key_list = ['a', 'b', 'c', 'd'] # loop with zip\n",
    "value_list = [1, 2, 3, 4]\n",
    "\n",
    "mapping = {}\n",
    "for key, value in zip(key_list, value_list):\n",
    "    mapping[key] = value\n",
    "\n",
    "print(mapping)  # Output: {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n",
    "\n",
    "mapping = dict(zip(range(5), reversed(range(5)))) # dict with zip\n",
    "print(mapping)  # Output: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Example list of words\n",
    "words = ['apple', 'bat', 'bar', 'atom', 'book']\n",
    "\n",
    "# Using a standard dictionary with get() and setdefault()\n",
    "by_letter = {}\n",
    "\n",
    "# Loop through words and categorize them by the first letter\n",
    "for word in words:\n",
    "    letter = word[0]\n",
    "    # Using get() to check for the letter and provide a default value (empty list)\n",
    "    by_letter[letter] = by_letter.get(letter, []) + [word]\n",
    "\n",
    "print(\"Using get():\")\n",
    "print(by_letter)  # Output: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n",
    "\n",
    "# Another approach with setdefault()\n",
    "by_letter = {}\n",
    "for word in words:\n",
    "    letter = word[0]\n",
    "    # Using setdefault() to initialize the list if not already present\n",
    "    by_letter.setdefault(letter, []).append(word)\n",
    "\n",
    "print(\"\\nUsing setdefault():\")\n",
    "print(by_letter)  # Output: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n",
    "\n",
    "# Using defaultdict from collections\n",
    "by_letter = defaultdict(list)\n",
    "for word in words:\n",
    "    by_letter[word[0]].append(word)\n",
    "\n",
    "print(\"\\nUsing defaultdict():\")\n",
    "print(dict(by_letter))  # Output: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n",
    "\n",
    "# Example using pop() with a default value\n",
    "by_letter = {'a': ['apple'], 'b': ['bat']}\n",
    "letter = 'c'\n",
    "# pop() will return the default value if the key does not exist\n",
    "result = by_letter.pop(letter, 'No words found')\n",
    "print(f\"\\nUsing pop() with default value for '{letter}': {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating and Modifying Dictionaries\n",
    "d = {'a': 'some value', 'b': [1, 2, 3, 4]}\n",
    "print(d)\n",
    "\n",
    "# Modify or add new elements\n",
    "d['new_key'] = 'new_value'\n",
    "print(d)\n",
    "\n",
    "# Deleting elements\n",
    "del d['a']\n",
    "print(d)\n",
    "\n",
    "# Using pop (removes and returns the value)\n",
    "value = d.pop('b')\n",
    "print(f\"Popped value: {value}\")\n",
    "print(d)\n",
    "\n",
    "# 2. Checking for Key Existence\n",
    "print('new_key' in d)  # Returns True if 'new_key' is in the dictionary\n",
    "\n",
    "# 3. Accessing Keys and Values\n",
    "print(list(d.keys()))   # List of all keys\n",
    "print(list(d.values())) # List of all values\n",
    "\n",
    "# 4. Default Values with get()\n",
    "default_value = d.get('nonexistent_key', 'default_value')\n",
    "print(f\"Default value for nonexistent_key: {default_value}\")\n",
    "\n",
    "# 5. Creating Dictionaries from Sequences\n",
    "keys = ['a', 'b', 'c']\n",
    "values = [1, 2, 3]\n",
    "mapping = dict(zip(keys, values))\n",
    "print(mapping)\n",
    "\n",
    "# 6. Using setdefault() to Set Default Values\n",
    "by_letter = {}\n",
    "words = ['apple', 'bat', 'bar', 'atom', 'book']\n",
    "for word in words:\n",
    "    letter = word[0]\n",
    "    by_letter.setdefault(letter, []).append(word)\n",
    "print(by_letter)\n",
    "\n",
    "# 7. defaultdict for automatic default values\n",
    "from collections import defaultdict\n",
    "by_letter_default = defaultdict(list)\n",
    "for word in words:\n",
    "    by_letter_default[word[0]].append(word)\n",
    "print(by_letter_default)\n",
    "\n",
    "# 8. Valid Dict Key Types (Hashability)\n",
    "try:\n",
    "    # Using an unhashable list as a key\n",
    "    invalid_dict = {}\n",
    "    invalid_dict[[1, 2, 3]] = 'value'  # This will raise an error\n",
    "except TypeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 9. Checking if an object is hashable\n",
    "print(hash('string'))  # Valid hashable object\n",
    "print(hash((1, 2, (2, 3))))  # Valid hashable tuple\n",
    "try:\n",
    "    print(hash((1, 2, [2, 3])))  # This will fail because lists are mutable\n",
    "except TypeError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sets\n",
    "set_example = set([2, 2, 2, 1, 3, 3])  # {1, 2, 3}\n",
    "print(\"Set created using set function:\", set_example)\n",
    "\n",
    "set_literal = {2, 2, 2, 1, 3, 3}  # {1, 2, 3}\n",
    "print(\"Set created using set literal:\", set_literal)\n",
    "\n",
    "# Set Operations\n",
    "a = {1, 2, 3, 4, 5}\n",
    "b = {3, 4, 5, 6, 7, 8}\n",
    "\n",
    "# Union (a ∪ b)\n",
    "union_result = a.union(b)\n",
    "print(\"Union (a.union(b)):\", union_result)\n",
    "\n",
    "union_result_operator = a | b\n",
    "print(\"Union (a | b):\", union_result_operator)\n",
    "\n",
    "# Intersection (a ∩ b)\n",
    "intersection_result = a.intersection(b)\n",
    "print(\"Intersection (a.intersection(b)):\", intersection_result)\n",
    "\n",
    "intersection_result_operator = a & b\n",
    "print(\"Intersection (a & b):\", intersection_result_operator)\n",
    "\n",
    "# Difference (a - b)\n",
    "difference_result = a.difference(b)\n",
    "print(\"Difference (a.difference(b)):\", difference_result)\n",
    "\n",
    "difference_result_operator = a - b\n",
    "print(\"Difference (a - b):\", difference_result_operator)\n",
    "\n",
    "# Symmetric Difference (a Δ b)\n",
    "symmetric_difference_result = a.symmetric_difference(b)\n",
    "print(\"Symmetric Difference (a.symmetric_difference(b)):\", symmetric_difference_result)\n",
    "\n",
    "symmetric_difference_result_operator = a ^ b\n",
    "print(\"Symmetric Difference (a ^ b):\", symmetric_difference_result_operator)\n",
    "\n",
    "# Subset and Superset\n",
    "is_subset = {1, 2, 3}.issubset(a)\n",
    "print(\"Is {1, 2, 3} a subset of a?\", is_subset)\n",
    "\n",
    "is_superset = a.issuperset({1, 2, 3})\n",
    "print(\"Is a a superset of {1, 2, 3}?\", is_superset)\n",
    "\n",
    "# Adding and Removing Elements\n",
    "a.add(6)  # Adding an element to set a\n",
    "print(\"After adding 6:\", a)\n",
    "\n",
    "a.remove(6)  # Removing an element from set a\n",
    "print(\"After removing 6:\", a)\n",
    "\n",
    "# Set Copy and Update\n",
    "c = a.copy()  # Copying a set\n",
    "c |= b        # Union of c and b\n",
    "print(\"Union of c and b (c |= b):\", c)\n",
    "\n",
    "d = a.copy()  # Copying a set\n",
    "d &= b        # Intersection of d and b\n",
    "print(\"Intersection of d and b (d &= b):\", d)\n",
    "\n",
    "# Working with immutable elements (hashable types)\n",
    "# Tuples can be used as set elements, while lists cannot because they are mutable\n",
    "my_data = [1, 2, 3, 4]\n",
    "my_set = {tuple(my_data)}  # Convert list to tuple for immutability\n",
    "print(\"Set containing tuple:\", my_set)\n",
    "\n",
    "# Checking equality of sets\n",
    "are_sets_equal = {1, 2, 3} == {3, 2, 1}\n",
    "print(\"Are {1, 2, 3} and {3, 2, 1} equal?\", are_sets_equal)\n",
    "\n",
    "# Pop an element (removes arbitrary element from the set)\n",
    "popped_element = a.pop()\n",
    "print(f\"Popped element: {popped_element}, Remaining set: {a}\")\n",
    "\n",
    "# List Comprehension Example\n",
    "strings = ['a', 'as', 'bat', 'car', 'dove', 'python']\n",
    "\n",
    "# Convert strings to uppercase if their length is greater than 2\n",
    "upper_strings = [x.upper() for x in strings if len(x) > 2]\n",
    "print(\"List Comprehension (Uppercase strings with length > 2):\", upper_strings)\n",
    "\n",
    "# Set Comprehension Example\n",
    "# Create a set of unique string lengths\n",
    "unique_lengths = {len(x) for x in strings}\n",
    "print(\"Set Comprehension (Unique string lengths):\", unique_lengths)\n",
    "\n",
    "# Alternative approach using map function (equivalent to set comprehension)\n",
    "unique_lengths_map = set(map(len, strings))\n",
    "print(\"Set using map function:\", unique_lengths_map)\n",
    "\n",
    "# Dict Comprehension Example\n",
    "# Create a dictionary that maps each string to its index in the original list\n",
    "loc_mapping = {val: index for index, val in enumerate(strings)}\n",
    "print(\"Dict Comprehension (String to index mapping):\", loc_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested List Comprehensions in Python\n",
    "\n",
    "# Suppose we have a list of lists containing some English and Spanish names:\n",
    "all_data = [['John', 'Emily', 'Michael', 'Mary', 'Steven'],\n",
    "            ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']]\n",
    "\n",
    "# You might have gotten these names from a couple of files and decided to organize them by language.\n",
    "# Now, suppose we wanted to get a single list containing all names with two or more 'e's in them.\n",
    "\n",
    "# We could do this with a simple for loop:\n",
    "names_of_interest = []\n",
    "for names in all_data:\n",
    "    enough_es = [name for name in names if name.count('e') >= 2]\n",
    "    names_of_interest.extend(enough_es)\n",
    "\n",
    "# Alternatively, we can wrap this whole operation up in a single nested list comprehension:\n",
    "result = [name for names in all_data for name in names if name.count('e') >= 2]\n",
    "print(result)  # Output: ['Steven']\n",
    "\n",
    "# At first, nested list comprehensions are a bit hard to wrap your head around.\n",
    "# The 'for' parts of the list comprehension are arranged according to the order of nesting,\n",
    "# and any filter condition is put at the end as before.\n",
    "\n",
    "# Here is another example where we \"flatten\" a list of tuples of integers into a simple list of integers:\n",
    "some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "\n",
    "# Using nested list comprehension to flatten the list of tuples:\n",
    "flattened = [x for tup in some_tuples for x in tup]\n",
    "print(flattened)  # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Keep in mind that the order of the 'for' expressions would be the same if you wrote a nested for loop:\n",
    "flattened_loop = []\n",
    "for tup in some_tuples:\n",
    "    for x in tup:\n",
    "        flattened_loop.append(x)\n",
    "print(flattened_loop)  # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# You can have arbitrarily many levels of nesting, though if you have more than two or three levels of nesting,\n",
    "# you should probably start to question whether this makes sense from a code readability standpoint.\n",
    "\n",
    "# It’s important to distinguish the syntax just shown from a list comprehension inside a list comprehension,\n",
    "# which is also perfectly valid:\n",
    "nested_result = [[x for x in tup] for tup in some_tuples]\n",
    "print(nested_result)  # Output: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# This produces a list of lists, rather than a flattened list of all of the inner elements.\n",
    "\n",
    "# Key Takeaways:\n",
    "# 1. Nested list comprehensions are concise but can become hard to read with too many levels of nesting.\n",
    "# 2. The order of 'for' clauses follows the same order as nested 'for' loops.\n",
    "# 3. Filter conditions are placed at the end of the comprehension.\n",
    "# 4. Use nested list comprehensions for simple transformations and flattening, but prefer traditional loops for complex logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Functions in Python\n",
    "\n",
    "# Functions are the primary and most important method of code organization and reuse in Python.\n",
    "# As a rule of thumb, if you anticipate needing to repeat the same or very similar code more than once,\n",
    "# it may be worth writing a reusable function. Functions also improve readability by giving a name to a group of statements.\n",
    "\n",
    "# Functions are declared with the `def` keyword and returned from with the `return` keyword:\n",
    "def my_function(x, y, z=1.5):\n",
    "    \"\"\"\n",
    "    A simple function that performs a calculation based on the values of x, y, and z.\n",
    "    - x, y: positional arguments\n",
    "    - z: keyword argument with a default value of 1.5\n",
    "    \"\"\"\n",
    "    if z > 1:\n",
    "        return z * (x + y)  # Return the product of z and (x + y) if z > 1\n",
    "    else:\n",
    "        return z / (x + y)  # Return the division of z by (x + y) if z <= 1\n",
    "\n",
    "# There is no issue with having multiple return statements.\n",
    "# If Python reaches the end of a function without encountering a return statement, `None` is returned automatically.\n",
    "\n",
    "# Example calls to the function:\n",
    "result1 = my_function(5, 6, z=0.7)  # z is explicitly passed as 0.7\n",
    "result2 = my_function(3.14, 7, 3.5)  # z is passed as 3.5 (positional)\n",
    "result3 = my_function(10, 20)        # z uses the default value of 1.5\n",
    "\n",
    "print(result1)  # Output: 0.7 / (5 + 6) = 0.0636...\n",
    "print(result2)  # Output: 3.5 * (3.14 + 7) = 35.49\n",
    "print(result3)  # Output: 1.5 * (10 + 20) = 45.0\n",
    "\n",
    "# Each function can have positional arguments and keyword arguments.\n",
    "# Keyword arguments are most commonly used to specify default values or optional arguments.\n",
    "# In the function above:\n",
    "# - x and y are positional arguments.\n",
    "# - z is a keyword argument with a default value of 1.5.\n",
    "\n",
    "# The main restriction on function arguments is that keyword arguments must follow positional arguments (if any).\n",
    "# However, you can specify keyword arguments in any order:\n",
    "result4 = my_function(x=5, y=6, z=7)  # Using keywords for all arguments\n",
    "result5 = my_function(y=6, x=5, z=7)  # Order of x and y doesn't matter when using keywords\n",
    "\n",
    "print(result4)  # Output: 7 * (5 + 6) = 77\n",
    "print(result5)  # Output: 7 * (5 + 6) = 77\n",
    "\n",
    "# It is also possible to use keywords for passing positional arguments:\n",
    "result6 = my_function(x=5, y=6)  # z uses the default value of 1.5\n",
    "print(result6)  # Output: 1.5 * (5 + 6) = 16.5\n",
    "\n",
    "# Using keywords for positional arguments can sometimes improve readability,\n",
    "# especially when dealing with functions that have many arguments.\n",
    "\n",
    "# Key Takeaways:\n",
    "# 1. Use functions to organize and reuse code.\n",
    "# 2. Positional arguments must come before keyword arguments.\n",
    "# 3. Keyword arguments can be specified in any order and are often used for optional parameters.\n",
    "# 4. Functions can have multiple return statements, and `None` is returned if no return statement is reached.\n",
    "# 5. Using keywords for arguments can improve readability, especially in functions with many parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 4 - NumPy Basics: Arrays and Vectorized Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79 ms ± 70.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "69 ms ± 728 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "NumPy array size: 8.0 MB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 4. Key Features -------------------------------------------------------------\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Memory efficiency\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumPy array size:\u001b[39m\u001b[33m\"\u001b[39m, np_arr.nbytes / \u001b[32m1e6\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMB\u001b[39m\u001b[33m\"\u001b[39m)      \u001b[38;5;66;03m# ~8MB (int64)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPython list size: \u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msys\u001b[49m.getsizeof(py_list)/\u001b[32m1e6\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# ~9MB + element overhead\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Broadcasting example\u001b[39;00m\n\u001b[32m     35\u001b[39m a = np.array([[\u001b[32m1\u001b[39m], [\u001b[32m10\u001b[39m], [\u001b[32m100\u001b[39m]])\n",
      "\u001b[31mNameError\u001b[39m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# NumPy Fundamentals: High-Performance Array Computing\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# 1. Creating Arrays ----------------------------------------------------------\n",
    "# Basic array creation\n",
    "arr = np.array([1, 2, 3, 4, 5])          # 1D array\n",
    "matrix = np.array([[1, 2], [3, 4]])       # 2D array\n",
    "zeros = np.zeros((3, 3))                  # 3x3 zero matrix\n",
    "ranged = np.arange(0, 10, 0.5)            # Like range() but with decimals\n",
    "\n",
    "# 2. Vectorized Operations ----------------------------------------------------\n",
    "# Fast element-wise operations without loops\n",
    "arr = np.arange(1, 6)\n",
    "squares = arr ** 2                         # [1, 4, 9, 16, 25]\n",
    "sqrt_matrix = np.sqrt(matrix)              # Element-wise square root\n",
    "\n",
    "# 3. Performance Benchmark vs Python Lists ------------------------------------\n",
    "size = 1_000_000\n",
    "np_arr = np.arange(size)\n",
    "py_list = list(range(size))\n",
    "\n",
    "# Vectorized operation\n",
    "%timeit -n 100 np_arr * 2                 # Typical result: ~2ms per loop\n",
    "\n",
    "# List comprehension equivalent\n",
    "%timeit -n 100 [x*2 for x in py_list]     # Typical result: ~50ms per loop\n",
    "\n",
    "# 4. Key Features -------------------------------------------------------------\n",
    "# Memory efficiency\n",
    "print(\"NumPy array size:\", np_arr.nbytes / 1e6, \"MB\")      # ~8MB (int64)\n",
    "print(\"Python list size: \", sys.getsizeof(py_list)/1e6, \"MB\") # ~9MB + element overhead\n",
    "\n",
    "# Broadcasting example\n",
    "a = np.array([[1], [10], [100]])\n",
    "b = np.array([1, 2, 3])\n",
    "print(a + b)  # [[2, 3, 4], [11,12,13], [101,102,103]]\n",
    "\n",
    "# 5. Core Advantages ----------------------------------------------------------\n",
    "\"\"\"\n",
    "1. Contiguous Memory: Data stored in single memory block for CPU-friendly access\n",
    "2. Vectorization: Operations applied to entire arrays (C-optimized backend)\n",
    "3. Broadcasting: Smart handling of different shaped arrays\n",
    "4. UFuncs: Fast mathematical operations (np.sin, np.exp, etc)\n",
    "5. Memory Efficiency: 4-10x less memory than Python lists for numbers\n",
    "6. Ecosystem Foundation: Used by Pandas, SciPy, Scikit-learn, etc\n",
    "\"\"\"\n",
    "\n",
    "# 6. Common Operations -------------------------------------------------------\n",
    "# Aggregations\n",
    "print(\"Mean:\", np.mean(np_arr))\n",
    "print(\"Max:\", np.max(np_arr))\n",
    "\n",
    "# Filtering\n",
    "filtered = np_arr[np_arr > 500_000]\n",
    "\n",
    "# Reshaping\n",
    "matrix_3d = np_arr.reshape((100, 100, 100))\n",
    "\n",
    "# 7. Real-world Use Case ------------------------------------------------------\n",
    "# Image processing example (3D array: height × width × RGB channels)\n",
    "fake_image = np.random.randint(0, 256, (1080, 1920, 3), dtype=np.uint8)\n",
    "grayscale = fake_image.mean(axis=2)  # Convert to grayscale in one operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy ndarray: Multidimensional Array Fundamentals\n",
    "import numpy as np\n",
    "\n",
    "# 1. Creating ndarrays --------------------------------------------------------\n",
    "# From Python list (notice automatic type conversion)\n",
    "data = np.array([[1, 2, 3], [4, 5.5, 6]])  # ints get converted to floats\n",
    "print(\"2D array:\\n\", data)\n",
    "print(\"Shape:\", data.shape)  # (2, 3)\n",
    "print(\"Data type:\", data.dtype)  # float64\n",
    "\n",
    "# Specialized array creation\n",
    "zeros = np.zeros((2, 4))        # 2x4 array of 0s\n",
    "ones = np.ones((3, 2))          # 3x2 array of 1s \n",
    "rand_array = np.random.randn(2, 3)  # 2x3 normal distribution\n",
    "\n",
    "# 2. Key Characteristics ------------------------------------------------------\n",
    "\"\"\"\n",
    "- Homogeneous: All elements same type (try mixing types to see automatic upcasting)\n",
    "- Fixed size: Size can't change without making new array\n",
    "- Contiguous memory: Enables vectorized operations\n",
    "- Optimized operations: Implemented in C for speed\n",
    "\"\"\"\n",
    "\n",
    "# 3. Vectorized Operations ----------------------------------------------------\n",
    "# Element-wise operations (no loops!)\n",
    "print(\"\\nOriginal array:\\n\", rand_array)\n",
    "print(\"\\nMultiply by 10:\\n\", rand_array * 10)\n",
    "print(\"\\nAdd arrays:\\n\", rand_array + rand_array)\n",
    "print(\"\\nExponential:\\n\", np.exp(rand_array))\n",
    "\n",
    "# 4. Performance Demonstration ------------------------------------------------\n",
    "large_arr = np.arange(1_000_000)\n",
    "large_list = list(range(1_000_000))\n",
    "\n",
    "# Vectorized operation\n",
    "%timeit -n 100 large_arr * 2  # ~2ms on typical hardware\n",
    "\n",
    "# Python loop equivalent\n",
    "%timeit -n 100 [x*2 for x in large_list]  # ~50ms - 25x slower!\n",
    "\n",
    "# 5. Important Attributes -----------------------------------------------------\n",
    "arr = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"\\nArray attributes:\")\n",
    "print(\"Dimensions:\", arr.ndim)     # 2\n",
    "print(\"Shape:\", arr.shape)        # (3, 2)\n",
    "print(\"Data type:\", arr.dtype)    # int64\n",
    "print(\"Total elements:\", arr.size) # 6\n",
    "print(\"Memory usage:\", arr.nbytes, \"bytes\")  # 48 bytes (6 elements * 8 bytes each)\n",
    "\n",
    "# 6. Type Management ----------------------------------------------------------\n",
    "# Explicit type specification\n",
    "int_array = np.array([1.5, 2.7, 3.9], dtype=np.int32)  # Truncates to integers\n",
    "print(\"\\nType conversion:\", int_array)  # [1 2 3]\n",
    "\n",
    "# 7. Why Homogeneous Matters --------------------------------------------------\n",
    "mixed_array = np.array([1, 2.5, '3'])  # All elements become strings\n",
    "print(\"\\nType coercion:\", mixed_array.dtype)  # <U32 (Unicode string type)\n",
    "\n",
    "# 8. Best Practices -----------------------------------------------------------\n",
    "\"\"\"\n",
    "- Always use np.array() instead of Python lists for numerical data\n",
    "- Pre-allocate arrays when possible (np.zeros/np.empty)\n",
    "- Use vectorized operations instead of loops\n",
    "- Be mindful of dtype choices (int32 vs float64 etc)\n",
    "- Avoid mixing data types in arrays\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy Array Creation Methods\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Basic Array Creation\n",
    "# ---------------------------\n",
    "\n",
    "# From Python lists\n",
    "list_data = [6, 7.5, 8, 0, 1]\n",
    "arr1d = np.array(list_data)\n",
    "print(\"1D Array from list:\\n\", arr1d)\n",
    "# Output: [6.  7.5 8.  0.  1. ]\n",
    "\n",
    "# From nested lists (creates 2D array)\n",
    "matrix_data = [[1, 2, 3, 4], [5, 6, 7, 8]]\n",
    "arr2d = np.array(matrix_data)\n",
    "print(\"\\n2D Array from nested lists:\\n\", arr2d)\n",
    "\"\"\"\n",
    "Output:\n",
    "[[1 2 3 4]\n",
    " [5 6 7 8]]\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Specialized Constructors\n",
    "# ---------------------------\n",
    "\n",
    "# Zeros array (float64 by default)\n",
    "zeros_1d = np.zeros(5)\n",
    "zeros_2d = np.zeros((3, 2))\n",
    "print(\"\\nZeros arrays:\")\n",
    "print(\"1D:\", zeros_1d)\n",
    "print(\"2D:\\n\", zeros_2d)\n",
    "\n",
    "# Ones array with specific dtype\n",
    "ones_int = np.ones((2, 3), dtype=np.int32)\n",
    "print(\"\\nOnes array with int32:\\n\", ones_int)\n",
    "\n",
    "# Empty array (contains memory garbage!)\n",
    "empty_arr = np.empty((2, 2))  # Uninitialized\n",
    "print(\"\\nEmpty array (values may vary):\\n\", empty_arr)\n",
    "\n",
    "# Arange (array version of range)\n",
    "range_arr = np.arange(10, 25, 3)  # Start, stop, step\n",
    "print(\"\\nArange array:\", range_arr)  # [10 13 16 19 22]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Advanced Creation Methods\n",
    "# ---------------------------\n",
    "\n",
    "# Full array with fill value\n",
    "full_arr = np.full((2, 3), 7)  # 2x3 filled with 7s\n",
    "print(\"\\nFull array:\\n\", full_arr)\n",
    "\n",
    "# Identity matrices\n",
    "eye_matrix = np.eye(3)       # 3x3 identity\n",
    "identity_matrix = np.identity(4)  # 4x4 identity\n",
    "print(\"\\nIdentity matrices:\")\n",
    "print(\"3x3:\\n\", eye_matrix)\n",
    "print(\"4x4:\\n\", identity_matrix)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Array Copying Methods\n",
    "# ---------------------------\n",
    "\n",
    "# Create template array\n",
    "template = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Ones/zeros like template\n",
    "ones_like = np.ones_like(template)\n",
    "zeros_like = np.zeros_like(template)\n",
    "print(\"\\nArray cloning methods:\")\n",
    "print(\"Ones like template:\\n\", ones_like)\n",
    "print(\"Zeros like template:\\n\", zeros_like)\n",
    "\n",
    "# Asarray vs Array (memory comparison)\n",
    "original = [1, 2, 3]\n",
    "arr_copy = np.array(original)    # Creates copy\n",
    "arr_view = np.asarray(original)  # May create view\n",
    "print(\"\\nMemory comparison:\")\n",
    "print(\"Array copy id:\", id(arr_copy))\n",
    "print(\"Asarray view id:\", id(arr_view))\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Data Type Management\n",
    "# ---------------------------\n",
    "\n",
    "# Type inference and control\n",
    "mixed_types = np.array([1, 2.5, 3])  # Upcasts to float64\n",
    "forced_type = np.array([1, 2, 3], dtype=np.float32)\n",
    "print(\"\\nData type examples:\")\n",
    "print(\"Inferred dtype:\", mixed_types.dtype)   # float64\n",
    "print(\"Forced dtype:\", forced_type.dtype)    # float32\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Specialized Arrays\n",
    "# ---------------------------\n",
    "\n",
    "# Linearly spaced arrays\n",
    "linspace_arr = np.linspace(0, 100, 5)  # 5 values 0-100\n",
    "print(\"\\nLinspace array:\", linspace_arr)  # [0. 25. 50. 75. 100.]\n",
    "\n",
    "# Random arrays\n",
    "random_arr = np.random.rand(2, 3)  # Uniform distribution\n",
    "print(\"\\nRandom array:\\n\", random_arr)\n",
    "\n",
    "# ---------------------------\n",
    "# Key Takeaways & Best Practices\n",
    "# ---------------------------\n",
    "\"\"\"\n",
    "**Array Creation Guide:**\n",
    "1. Use np.array() for converting existing data\n",
    "2. Prefer np.zeros()/np.ones() for initialized arrays\n",
    "3. Use np.empty() for uninitialized arrays (caution!)\n",
    "4. np.arange() for numerical sequences\n",
    "5. np.full() for constant-filled arrays\n",
    "6. *_like functions for cloning shapes/dtypes\n",
    "\n",
    "**Best Practices:**\n",
    "- Always specify dtype when precision matters\n",
    "- Use asarray() to avoid unnecessary copies\n",
    "- Prefer linspace over arange for floating point ranges\n",
    "- Initialize with zeros/ones unless performance critical\n",
    "- Check .flags.owndata to verify array ownership\n",
    "\n",
    "**Common dtypes:**\n",
    "- np.int32: 32-bit integer\n",
    "- np.float64: Double precision float (default)\n",
    "- np.bool_: Boolean values\n",
    "- np.complex128: Complex numbers\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Verification Methods\n",
    "# ---------------------------\n",
    "arr = np.array([[1, 2], [3, 4]])\n",
    "print(\"\\nArray verification:\")\n",
    "print(\"Dimensions:\", arr.ndim)        # 2\n",
    "print(\"Shape:\", arr.shape)           # (2, 2)\n",
    "print(\"Data type:\", arr.dtype)       # int64\n",
    "print(\"Total elements:\", arr.size)    # 4\n",
    "print(\"Memory usage:\", arr.nbytes, \"bytes\")  # 32 bytes (4 elements * 8 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy Data Types (dtypes) Comprehensive Guide\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Basic dtype Operations\n",
    "# =============================================================================\n",
    "# Default dtype inference\n",
    "arr_default_int = np.array([1, 2, 3])           # int64\n",
    "arr_default_float = np.array([1.0, 2.5, 3.7])    # float64\n",
    "arr_bool = np.array([True, False, True])         # bool\n",
    "\n",
    "# Explicit dtype declaration\n",
    "arr_int16 = np.array([1, 2, 3], dtype=np.int16)\n",
    "arr_float32 = np.array([1, 2, 3], dtype=np.float32)\n",
    "arr_uint8 = np.array([255, 0, 127], dtype=np.uint8)\n",
    "\n",
    "print(\"\\nBasic dtypes:\")\n",
    "print(f\"Default int: {arr_default_int.dtype}\")      # int64\n",
    "print(f\"Explicit float32: {arr_float32.dtype}\")     # float32\n",
    "print(f\"Uint8 values: {arr_uint8}\")                # [255   0 127]\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Type Conversion & Casting\n",
    "# =============================================================================\n",
    "# Safe conversion\n",
    "original_ints = np.array([1, 2, 3, 4], dtype=np.int32)\n",
    "converted_floats = original_ints.astype(np.float64)\n",
    "\n",
    "# Lossy conversion (truncation)\n",
    "decimals = np.array([3.7, -1.2, 2.5])\n",
    "truncated_ints = decimals.astype(np.int32)\n",
    "\n",
    "# String conversion\n",
    "numeric_strings = np.array(['1.25', '-9.6', '42'])\n",
    "converted_floats = numeric_strings.astype(np.float64)\n",
    "\n",
    "print(\"\\nType Conversion Results:\")\n",
    "print(f\"Int to float: {converted_floats}\")        # [1.0, 2.0, 3.0, 4.0]\n",
    "print(f\"Float truncation: {truncated_ints}\")      # [ 3 -1  2]\n",
    "print(f\"String conversion: {converted_floats}\")   # [ 1.25 -9.6  42.  ]\n",
    "\n",
    "# =============================================================================\n",
    "# 3. dtype Best Practices & Pitfalls\n",
    "# =============================================================================\n",
    "# Memory optimization example\n",
    "large_ints = np.ones(1000000, dtype=np.int64)  # 8MB\n",
    "small_ints = large_ints.astype(np.int8)        # 1MB (but potential overflow!)\n",
    "\n",
    "print(f\"\\nMemory savings: {large_ints.nbytes/1e6}MB -> {small_ints.nbytes/1e6}MB\")\n",
    "\n",
    "# Overflow demonstration\n",
    "overflow_arr = np.array([250, 251, 252], dtype=np.uint8)\n",
    "overflow_arr += 100  # Values wrap around (250+100=94)\n",
    "print(f\"Overflow example: {overflow_arr}\")  # [94 95 96]\n",
    "\n",
    "# String truncation example\n",
    "names = np.array(['Alice', 'Bob', 'Charlie'], dtype='S3')\n",
    "print(f\"String truncation: {names}\")  # [b'Ali' b'Bob' b'Cha']\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Advanced dtype Usage\n",
    "# =============================================================================\n",
    "# Structured dtype for complex data\n",
    "person_dtype = np.dtype([\n",
    "    ('name', 'U32'),  # Unicode string (32 chars)\n",
    "    ('age', 'i4'),    # 4-byte integer\n",
    "    ('height', 'f4'), # 4-byte float\n",
    "    ('active', '?')   # Boolean\n",
    "])\n",
    "\n",
    "people = np.array([\n",
    "    ('Alice', 30, 1.65, True),\n",
    "    ('Bob', 25, 1.80, False)\n",
    "], dtype=person_dtype)\n",
    "\n",
    "print(\"\\nStructured dtype example:\")\n",
    "print(people[0])  # ('Alice', 30, 1.65, True)\n",
    "\n",
    "# Memory-mapped arrays for large datasets\n",
    "try:\n",
    "    # Create/Write\n",
    "    mmap_arr = np.memmap('temp.dat', dtype=np.float32, mode='w+', shape=(5,))\n",
    "    mmap_arr[:] = np.random.randn(5)\n",
    "    print(\"\\nMemory-mapped array:\", mmap_arr)\n",
    "    \n",
    "    # Read back\n",
    "    mmap_read = np.memmap('temp.dat', dtype=np.float32, mode='r', shape=(5,))\n",
    "    print(\"Read from disk:\", mmap_read)\n",
    "finally:\n",
    "    del mmap_arr  # Flush to disk\n",
    "    del mmap_read\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Type Safety & Error Handling\n",
    "# =============================================================================\n",
    "# Safe conversion function\n",
    "def safe_convert(arr, new_type):\n",
    "    if np.issubdtype(arr.dtype, np.number) and np.issubdtype(new_type, np.number):\n",
    "        return arr.astype(new_type)\n",
    "    raise ValueError(\"Non-numeric conversion\")\n",
    "\n",
    "# Mixed data handling\n",
    "mixed_data = np.array([1, \"two\", 3.0], dtype=np.object_)\n",
    "print(\"\\nMixed data handling:\")\n",
    "try:\n",
    "    safe_convert(mixed_data, np.float64)\n",
    "except ValueError as e:\n",
    "    print(f\"Conversion error: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Key Takeaways & When to Use\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "**NumPy dtype Cheat Sheet:**\n",
    "\n",
    "1. **Common dtypes:**\n",
    "   - Integer: int8/16/32/64, uint8/16/32/64\n",
    "   - Float: float16/32/64/128\n",
    "   - Complex: complex64/128/256\n",
    "   - Others: bool_, object_, string_, unicode_\n",
    "\n",
    "2. **Best Practices:**\n",
    "   - Use smallest dtype that fits data range\n",
    "   - Prefer float32 for ML/DL, float64 for precision\n",
    "   - Use astype() carefully (creates copies)\n",
    "   - Handle strings with object dtype or dedicated string dtypes\n",
    "\n",
    "3. **Pitfalls:**\n",
    "   - Overflow in integer types\n",
    "   - Precision loss in float conversions\n",
    "   - String truncation in fixed-width dtypes\n",
    "   - Accidental data copies with astype()\n",
    "\n",
    "4. **Advanced Features:**\n",
    "   - Structured dtypes for complex records\n",
    "   - Memory mapping for large datasets\n",
    "   - Custom dtype creation\n",
    "\"\"\"\n",
    "\n",
    "# Cleanup temporary file\n",
    "import os\n",
    "if os.path.exists('temp.dat'):\n",
    "    os.remove('temp.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy Boolean Arrays, Sorting, and Quantiles\n",
    "import numpy as np\n",
    "\n",
    "# =============================================\n",
    "# 1. Boolean Array Methods\n",
    "# =============================================\n",
    "# Create random data and boolean mask\n",
    "data = np.random.randn(100)\n",
    "positive_mask = data > 0\n",
    "\n",
    "print(\"Boolean array operations:\")\n",
    "print(f\"Number of positive values: {positive_mask.sum()}\")\n",
    "print(f\"Any positive values? {positive_mask.any()}\")\n",
    "print(f\"All positive values? {positive_mask.all()}\\n\")\n",
    "\n",
    "# Edge cases\n",
    "all_true = np.array([True, True, True])\n",
    "all_false = np.array([False, False, False])\n",
    "print(\"All True array - any:\", all_true.any(), \"all:\", all_true.all())\n",
    "print(\"All False array - any:\", all_false.any(), \"all:\", all_false.all())\n",
    "\n",
    "# Non-boolean coercion\n",
    "numbers = np.array([0, 1, -2, 3])\n",
    "print(\"\\nNon-boolean array evaluation:\")\n",
    "print(\"Any non-zero?\", numbers.any())\n",
    "print(\"All non-zero?\", numbers.all())\n",
    "\n",
    "# =============================================\n",
    "# 2. Sorting Methods\n",
    "# =============================================\n",
    "# 1D sorting\n",
    "arr = np.random.randn(6)\n",
    "print(\"\\nOriginal 1D array:\", np.round(arr, 4))\n",
    "arr.sort()\n",
    "print(\"Sorted in-place:\", np.round(arr, 4))\n",
    "\n",
    "# 2D sorting\n",
    "matrix = np.random.randn(5, 3)\n",
    "print(\"\\nOriginal 2D array:\")\n",
    "print(np.round(matrix, 4))\n",
    "\n",
    "matrix.sort(1)  # Sort each row\n",
    "print(\"\\nSorted along rows (axis=1):\")\n",
    "print(np.round(matrix, 4))\n",
    "\n",
    "matrix.sort(0)  # Sort each column\n",
    "print(\"\\nSorted along columns (axis=0):\")\n",
    "print(np.round(matrix, 4))\n",
    "\n",
    "# Non-destructive sort\n",
    "unsorted = np.random.randn(5)\n",
    "sorted_copy = np.sort(unsorted)\n",
    "print(\"\\nOriginal array remains unchanged:\", np.round(unsorted, 4))\n",
    "\n",
    "# =============================================\n",
    "# 3. Quantile Calculation\n",
    "# =============================================\n",
    "# Generate and sort large dataset\n",
    "large_data = np.random.randn(1000)\n",
    "large_data.sort()\n",
    "\n",
    "# Calculate quantiles\n",
    "percentiles = [5, 25, 50, 75, 95]\n",
    "quantiles = [large_data[int(p/100 * len(large_data))] for p in percentiles]\n",
    "\n",
    "print(\"\\nQuantiles:\")\n",
    "for p, q in zip(percentiles, quantiles):\n",
    "    print(f\"{p}th percentile: {q:.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Performance Considerations\n",
    "# =============================================\n",
    "# Timing different sort methods\n",
    "large_array = np.random.rand(1_000_000)\n",
    "\n",
    "# In-place sort timing\n",
    "%timeit large_array.sort()          # ~10ms\n",
    "\n",
    "# Copy sort timing\n",
    "%timeit np.sort(large_array)        # ~15ms \n",
    "\n",
    "# =============================================\n",
    "# Key Takeaways\n",
    "# =============================================\n",
    "\"\"\"\n",
    "NUMPY BOOLEAN & SORTING ESSENTIALS:\n",
    "\n",
    "Boolean Arrays:\n",
    "- True = 1, False = 0 in arithmetic operations\n",
    "- sum() counts True values\n",
    "- any() = ∃ True, all() = ∀ True\n",
    "- Non-zero values evaluate as True\n",
    "\n",
    "Sorting Methods:\n",
    "1. arr.sort():\n",
    "   - In-place modification\n",
    "   - Returns None\n",
    "   - Axis parameter for multidimensional arrays\n",
    "   \n",
    "2. np.sort(arr):\n",
    "   - Returns sorted copy\n",
    "   - Original array preserved\n",
    "   - More memory intensive\n",
    "\n",
    "Quantile Calculation:\n",
    "1. Sort data\n",
    "2. Access value at index = len(data) * percentile/100\n",
    "3. More accurate methods exist (linear interpolation), \n",
    "   but sorted approach is simple\n",
    "\n",
    "Best Practices:\n",
    "- Use in-place sorting for memory efficiency\n",
    "- Prefer boolean arrays over lists for masking\n",
    "- Use np.quantile() for precise percentile calculations\n",
    "- Reserve axis=0 for columns, axis=1 for rows\n",
    "\n",
    "Common Pitfalls:\n",
    "- Forgetting sort is in-place\n",
    "- Mixing axis directions (0=columns, 1=rows)\n",
    "- Assuming all() returns True for empty arrays (returns True!)\n",
    "\"\"\"\n",
    "\n",
    "# =============================================\n",
    "# 5. Advanced: Structured Array Sorting\n",
    "# =============================================\n",
    "# Sort structured data by specific field\n",
    "dtype = [('name', 'S10'), ('age', int), ('score', float)]\n",
    "people = np.array([('Alice', 25, 88.5), ('Bob', 32, 94.0), ('Charlie', 28, 76.5)], dtype=dtype)\n",
    "people.sort(order='age')\n",
    "print(\"\\nStructured array sorted by age:\")\n",
    "print(people)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNIQUE VALUES ===\n",
      "Names: ['Bob' 'Joe' 'Will']\n",
      "Integers: [1 2 3 4]\n",
      "Pure Python equivalent: [np.str_('Bob'), np.str_('Joe'), np.str_('Will')]\n",
      "\n",
      "=== MEMBERSHIP TESTING ===\n",
      "Original array: [6 0 0 3 2 5 6]\n",
      "Membership mask: [ True False False  True  True False  True]\n",
      "Filtered values: [6 3 2 6]\n",
      "\n",
      "=== SET OPERATIONS ===\n",
      "Array x: [1 2 3 4]\n",
      "Array y: [3 4 5 6]\n",
      "Intersection: [3 4]\n",
      "Union: [1 2 3 4 5 6]\n",
      "Elements in x not in y: [1 2]\n",
      "Symmetric difference: [1 2 5 6]\n",
      "\n",
      "=== PERFORMANCE TIP ===\n",
      "Calculating unique values for 10,000 elements...\n",
      "NumPy unique() completed instantly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Local\\Temp\\ipykernel_5004\\3342881199.py:24: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n",
      "  mask = np.in1d(values, [2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "NUMPY SET OPERATIONS DEMO\n",
    "=========================\n",
    "This executable script demonstrates various NumPy set operations.\"\"\"\n",
    "\n",
    "# Sample data arrays\n",
    "names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\n",
    "ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])\n",
    "values = np.array([6, 0, 0, 3, 2, 5, 6])\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([3, 4, 5, 6])\n",
    "\n",
    "def main():\n",
    "    # Demo 1: Basic unique values\n",
    "    print(\"=== UNIQUE VALUES ===\")\n",
    "    print(\"Names:\", np.unique(names))\n",
    "    print(\"Integers:\", np.unique(ints))\n",
    "    print(\"Pure Python equivalent:\", sorted(set(names)))\n",
    "    \n",
    "    # Demo 2: Membership testing\n",
    "    print(\"\\n=== MEMBERSHIP TESTING ===\")\n",
    "    mask = np.in1d(values, [2, 3, 6])\n",
    "    print(\"Original array:\", values)\n",
    "    print(\"Membership mask:\", mask)\n",
    "    print(\"Filtered values:\", values[mask])\n",
    "    \n",
    "    # Demo 3: Set operations\n",
    "    print(\"\\n=== SET OPERATIONS ===\")\n",
    "    print(\"Array x:\", x)\n",
    "    print(\"Array y:\", y)\n",
    "    print(\"Intersection:\", np.intersect1d(x, y))\n",
    "    print(\"Union:\", np.union1d(x, y))\n",
    "    print(\"Elements in x not in y:\", np.setdiff1d(x, y))\n",
    "    print(\"Symmetric difference:\", np.setxor1d(x, y))\n",
    "    \n",
    "    # Demo 4: Performance comparison\n",
    "    print(\"\\n=== PERFORMANCE TIP ===\")\n",
    "    large_array = np.random.randint(0, 1000, 10000)\n",
    "    print(\"Calculating unique values for 10,000 elements...\")\n",
    "    _ = np.unique(large_array)\n",
    "    print(\"NumPy unique() completed instantly!\")\n",
    "\n",
    "\"\"\"\n",
    "KEY TAKEAWAYS 💡\n",
    "================\n",
    "🌟 np.unique() faster than sorted(set()) for large data\n",
    "🌟 Set functions work with 1D arrays only\n",
    "🌟 in1d() creates boolean masks for filtering\n",
    "🌟 Results always sorted in NumPy\n",
    "🌟 Great for data cleaning/preprocessing\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔷 File I/O Results 🔷\n",
      "Loaded array: [0 1 2 3 4 5 6 7 8 9]\n",
      "Archive contents: ['arr1', 'arr2']\n",
      "\n",
      "🔷 Linear Algebra Results 🔷\n",
      "Matrix product:\n",
      " [[19 22]\n",
      " [43 50]]\n",
      "\n",
      "QR R matrix:\n",
      " [[-3.16227766 -4.42718872]\n",
      " [ 0.         -0.63245553]]\n",
      "\n",
      "Eigenvalues: [-0.37228132  5.37228132]\n",
      "\n",
      "NUMPY.LINALG FUNCTION CHEATSHEET\n",
      "--------------------------------\n",
      "| Function | Description                      | Example Use Case           |\n",
      "|----------|----------------------------------|----------------------------|\n",
      "| inv()    | Matrix inverse                   | Solving linear equations   |\n",
      "| qr()     | QR decomposition                | Matrix factorization       |\n",
      "| det()    | Determinant                     | Matrix invertibility check |\n",
      "| eig()    | Eigenvalues/vectors             | Spectral analysis          |\n",
      "| svd()    | Singular Value Decomposition    | Dimensionality reduction   |\n",
      "| solve()  | Solve linear system Ax = b      | Optimization problems      |\n",
      "\n",
      "KEY TAKEAWAYS 🔑\n",
      "================\n",
      "📁 File I/O:\n",
      "- Use .npy for single arrays, .npz for archives\n",
      "- savez_compressed reduces file size dramatically\n",
      "- 10-100x faster than text formats for large data\n",
      "\n",
      "🧮 Linear Algebra:\n",
      "- @ operator preferred for matrix multiplication\n",
      "- Built on optimized BLAS/LAPACK implementations\n",
      "- det() helps check matrix invertibility\n",
      "- SVD/Eigendecomposition crucial for ML algorithms\n",
      "\n",
      "💡 Pro Tips:\n",
      "- Use np.memmap for memory-mapped large arrays\n",
      "- pinv() handles non-square matrices\n",
      "- lstsq() for regression problems\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUMPY FILE I/O & LINEAR ALGEBRA GUIDE\n",
    "=====================================\n",
    "This script demonstrates NumPy's file operations and linear algebra capabilities.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, qr, det, eig\n",
    "\n",
    "# ---------------------------\n",
    "# 4.4 File Input/Output Demo\n",
    "# ---------------------------\n",
    "\n",
    "def file_io_demo():\n",
    "    # Single array save/load\n",
    "    arr = np.arange(10)\n",
    "    np.save('single_array.npy', arr)\n",
    "    loaded_arr = np.load('single_array.npy')\n",
    "    \n",
    "    # Multiple arrays archive\n",
    "    np.savez('multi_arrays.npz', arr1=arr, arr2=arr*2)\n",
    "    archive = np.load('multi_arrays.npz')\n",
    "    \n",
    "    # Compressed archive\n",
    "    np.savez_compressed('compressed_arrays.npz', big_array=np.random.randn(1000, 1000))\n",
    "    \n",
    "    return {\n",
    "        'loaded_single': loaded_arr,\n",
    "        'archive_contents': list(archive.keys()),\n",
    "        'compressed_size': len(archive['arr1'])\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 4.5 Linear Algebra Demo\n",
    "# ---------------------------\n",
    "\n",
    "def linear_algebra_demo():\n",
    "    # Matrix creation\n",
    "    A = np.array([[1, 2], [3, 4]])\n",
    "    B = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    # Matrix multiplication\n",
    "    dot_product = A.dot(B)\n",
    "    at_operator = A @ B\n",
    "    \n",
    "    # Matrix operations\n",
    "    matrix_inv = inv(A)\n",
    "    q, r = qr(A)\n",
    "    eigenvalues, eigenvectors = eig(A)\n",
    "    \n",
    "    return {\n",
    "        'dot_product': dot_product,\n",
    "        'matrix_inverse': matrix_inv,\n",
    "        'qr_r': r,\n",
    "        'eigenvalues': eigenvalues\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Key Takeaways & Help Table\n",
    "# ---------------------------\n",
    "\n",
    "LIN_ALG_FUNCTIONS = \"\"\"\n",
    "NUMPY.LINALG FUNCTION CHEATSHEET\n",
    "--------------------------------\n",
    "| Function | Description                      | Example Use Case           |\n",
    "|----------|----------------------------------|----------------------------|\n",
    "| inv()    | Matrix inverse                   | Solving linear equations   |\n",
    "| qr()     | QR decomposition                | Matrix factorization       |\n",
    "| det()    | Determinant                     | Matrix invertibility check |\n",
    "| eig()    | Eigenvalues/vectors             | Spectral analysis          |\n",
    "| svd()    | Singular Value Decomposition    | Dimensionality reduction   |\n",
    "| solve()  | Solve linear system Ax = b      | Optimization problems      |\"\"\"\n",
    "\n",
    "KEY_TAKEAWAYS = \"\"\"\n",
    "KEY TAKEAWAYS 🔑\n",
    "================\n",
    "📁 File I/O:\n",
    "- Use .npy for single arrays, .npz for archives\n",
    "- savez_compressed reduces file size dramatically\n",
    "- 10-100x faster than text formats for large data\n",
    "\n",
    "🧮 Linear Algebra:\n",
    "- @ operator preferred for matrix multiplication\n",
    "- Built on optimized BLAS/LAPACK implementations\n",
    "- det() helps check matrix invertibility\n",
    "- SVD/Eigendecomposition crucial for ML algorithms\n",
    "\n",
    "💡 Pro Tips:\n",
    "- Use np.memmap for memory-mapped large arrays\n",
    "- pinv() handles non-square matrices\n",
    "- lstsq() for regression problems\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔷 File I/O Results 🔷\")\n",
    "    file_results = file_io_demo()\n",
    "    print(f\"Loaded array: {file_results['loaded_single']}\")\n",
    "    print(f\"Archive contents: {file_results['archive_contents']}\")\n",
    "    \n",
    "    print(\"\\n🔷 Linear Algebra Results 🔷\")\n",
    "    lin_alg_results = linear_algebra_demo()\n",
    "    print(\"Matrix product:\\n\", lin_alg_results['dot_product'])\n",
    "    print(\"\\nQR R matrix:\\n\", lin_alg_results['qr_r'])\n",
    "    print(\"\\nEigenvalues:\", lin_alg_results['eigenvalues'])\n",
    "    \n",
    "    print(LIN_ALG_FUNCTIONS)\n",
    "    print(KEY_TAKEAWAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NUMPY RANDOM & RANDOM WALKS DEMO\n",
    "==================================\n",
    "Demonstrates pseudorandom number generation and random walk simulations.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import timeit\n",
    "\n",
    "# ---------------------------\n",
    "# 4.6 Pseudorandom Number Generation\n",
    "# ---------------------------\n",
    "\n",
    "def random_demo():\n",
    "    # Seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Generate random samples\n",
    "    samples = np.random.normal(size=(4, 4))\n",
    "    rng = np.random.RandomState(456)\n",
    "    custom_samples = rng.randn(5)\n",
    "    \n",
    "    # Performance comparison\n",
    "    py_time = timeit('[np.random.normal() for _ in range(1000)]', number=1000)\n",
    "    np_time = timeit('np.random.normal(size=1000)', globals=globals(), number=1000)\n",
    "    \n",
    "    return {\n",
    "        'seeded_samples': samples,\n",
    "        'custom_rng_samples': custom_samples,\n",
    "        'py_vs_np_times': (py_time, np_time)\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 4.7 Random Walk Simulation\n",
    "# ---------------------------\n",
    "\n",
    "def random_walk_demo():\n",
    "    # Single walk\n",
    "    nsteps = 1000\n",
    "    steps = np.where(np.random.randint(0, 2, nsteps), 1, -1)\n",
    "    walk = steps.cumsum()\n",
    "    \n",
    "    # Multiple walks\n",
    "    nwalks = 5000\n",
    "    draws = np.random.randint(0, 2, (nwalks, nsteps))\n",
    "    walks = np.where(draws, 1, -1).cumsum(axis=1)\n",
    "    \n",
    "    # Analysis\n",
    "    hits30 = (np.abs(walks) >= 30).any(axis=1)\n",
    "    crossing_times = (np.abs(walks[hits30]) >= 30).argmax(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'walk_plot_data': walk[:100],\n",
    "        'max_walk': walks.max(),\n",
    "        'min_walk': walks.min(),\n",
    "        'crossing_time_avg': crossing_times.mean()\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization & Tables\n",
    "# ---------------------------\n",
    "\n",
    "RANDOM_FUNCTIONS = \"\"\"\n",
    "NUMPY.RANDOM FUNCTION CHEATSHEET\n",
    "---------------------------------\n",
    "| Function      | Description                      | Example Use               |\n",
    "|---------------|----------------------------------|---------------------------|\n",
    "| seed()        | Set global random seed          | np.random.seed(123)       |\n",
    "| RandomState() | Create isolated RNG             | rng = np.random.RandomState()|\n",
    "| rand()        | Uniform distribution [0,1)      | np.random.rand(2,3)       |\n",
    "| randn()       | Standard normal distribution    | np.random.randn(100)      |\n",
    "| randint()     | Random integers                 | np.random.randint(0,10,5) |\n",
    "| normal()      | Normal distribution             | np.random.normal(mean, std, size)|\n",
    "| binomial()    | Binomial distribution           | np.random.binomial(n,p,size) |\"\"\"\n",
    "\n",
    "KEY_TAKEAWAYS = \"\"\"\n",
    "KEY INSIGHTS 🔍\n",
    "===============\n",
    "🎲 Random Generation:\n",
    "- NumPy is 10-100x faster than pure Python for large samples\n",
    "- Use RandomState for reproducible, isolated streams\n",
    "- Seed management is crucial for reproducible results\n",
    "\n",
    "🚶 Random Walks:\n",
    "- Vectorized operations enable efficient simulations\n",
    "- cumsum() is powerful for path calculations\n",
    "- argmax() finds first occurrence efficiently\n",
    "- Any()/All() help analyze multidimensional results\n",
    "\n",
    "📊 Statistical Analysis:\n",
    "- 68-95-99.7 rule applies to normal distributions\n",
    "- Crossing times help understand walk behavior\n",
    "- Multiple walks enable probabilistic forecasting\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    random_data = random_demo()\n",
    "    walk_data = random_walk_demo()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"🔶 Random Number Generation Results 🔶\")\n",
    "    print(\"Seeded normal samples:\\n\", random_data['seeded_samples'])\n",
    "    print(f\"\\nPython vs NumPy times: {random_data['py_vs_np_times'][0]:.2f}s vs {random_data['py_vs_np_times'][1]:.2f}s\")\n",
    "    \n",
    "    print(\"\\n🔶 Random Walk Analysis 🔶\")\n",
    "    print(f\"Max walk position: {walk_data['max_walk']}\")\n",
    "    print(f\"Min walk position: {walk_data['min_walk']}\")\n",
    "    print(f\"Average crossing time: {walk_data['crossing_time_avg']:.1f} steps\")\n",
    "    \n",
    "    # Plot first 100 steps\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(walk_data['walk_plot_data'])\n",
    "    plt.title(\"First 100 Steps of Random Walk\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Position\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(RANDOM_FUNCTIONS)\n",
    "    print(KEY_TAKEAWAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 5 - PANDAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PANDAS SERIES ESSENTIALS\n",
    "=========================\n",
    "A comprehensive guide to pandas Series with practical examples.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create basic Series with default index\n",
    "basic_series = pd.Series([4, 7, -5, 3])\n",
    "print(\"🔷 Basic Series:\\n\", basic_series)\n",
    "\n",
    "# Series with custom index\n",
    "indexed_series = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\n",
    "print(\"\\n🔷 Custom Index Series:\\n\", indexed_series)\n",
    "\n",
    "# Accessing elements\n",
    "print(\"\\n🔷 Element Access:\")\n",
    "print(\"Value at 'a':\", indexed_series['a'])\n",
    "indexed_series['d'] = 6  # Modify value\n",
    "print(\"Modified Series:\\n\", indexed_series[['c', 'a', 'd']])\n",
    "\n",
    "# Series from dictionary\n",
    "state_data = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "states_series = pd.Series(state_data)\n",
    "print(\"\\n🔷 From Dictionary:\\n\", states_series)\n",
    "\n",
    "# Handling missing data\n",
    "states_list = ['California', 'Ohio', 'Oregon', 'Texas']\n",
    "missing_series = pd.Series(state_data, index=states_list)\n",
    "print(\"\\n🔷 Series with Missing Data:\\n\", missing_series)\n",
    "print(\"\\nNull Check:\\n\", missing_series.isnull())\n",
    "\n",
    "# Vector operations\n",
    "print(\"\\n🔷 Vector Operations:\")\n",
    "print(\"Filtered Series:\\n\", indexed_series[indexed_series > 0])\n",
    "print(\"Scalar Multiplication:\\n\", indexed_series * 2)\n",
    "print(\"Exponential Transformation:\\n\", np.exp(indexed_series))\n",
    "\n",
    "# Index alignment\n",
    "combined_series = states_series + missing_series\n",
    "print(\"\\n🔷 Aligned Addition:\\n\", combined_series)\n",
    "\n",
    "# Naming conventions\n",
    "missing_series.name = \"State Population\"\n",
    "missing_series.index.name = \"US States\"\n",
    "print(\"\\n🔷 Named Series:\\n\", missing_series)\n",
    "\n",
    "# Index modification\n",
    "basic_series.index = ['Bob', 'Steve', 'Jeff', 'Ryan']\n",
    "print(\"\\n🔷 Reindexed Series:\\n\", basic_series)\n",
    "\n",
    "# Cheatsheet Table\n",
    "SERIES_FUNCTIONS = \"\"\"\n",
    "📋 SERIES OPERATIONS CHEATSHEET\n",
    "--------------------------------\n",
    "| Method            | Description                 | Example                   |\n",
    "|-------------------|-----------------------------|---------------------------|\n",
    "| pd.Series()       | Create new Series           | pd.Series(data, index)    |\n",
    "| .values           | Get NumPy array             | series.values             |\n",
    "| .index            | Get index object            | series.index              |\n",
    "| .isnull()         | Detect missing values       | series.isnull()           |\n",
    "| .notnull()        | Detect non-missing values   | series.notnull()          |\n",
    "| .name             | Series name attribute       | series.name = \"Revenue\"   |\n",
    "| .index.name       | Index name attribute        | series.index.name = \"City\"|\n",
    "| [label]           | Label-based indexing        | series['London']          |\n",
    "| .loc[]            | Explicit label-based access | series.loc['Paris']       |\"\"\"\n",
    "\n",
    "KEY_TAKEAWAYS = \"\"\"\n",
    "🌟 KEY INSIGHTS 🌟\n",
    "------------------\n",
    "1. Index Flexibility: Series can have any hashable type as index (strings, dates, etc)\n",
    "2. Data Alignment: Automatic index matching in operations (like SQL JOIN)\n",
    "3. Missing Data: NaN represents missing values - use isnull()/notnull() to detect\n",
    "4. Vectorization: Operate on entire series without loops (NumPy-style)\n",
    "5. Hybrid Nature: Combines dictionary-like access with array operations\n",
    "6. Size Immutability: Can't change size, but can modify values and index\n",
    "7. Name Metadata: Add contextual information with name attributes\n",
    "\n",
    "💡 Pro Tips:\n",
    "- Convert between Series and dict with to_dict()/from_dict()\n",
    "- Use .copy() when creating modified copies to preserve original data\n",
    "- Prefer .loc[] for explicit label-based indexing\n",
    "- Handle missing data early with .dropna() or .fillna()\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(SERIES_FUNCTIONS)\n",
    "    print(KEY_TAKEAWAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔷 Basic DataFrame:\n",
      "    state  year  pop\n",
      "0    Ohio  2000  1.5\n",
      "1    Ohio  2001  1.7\n",
      "2    Ohio  2002  3.6\n",
      "3  Nevada  2001  2.4\n",
      "4  Nevada  2002  2.9\n",
      "\n",
      "🔷 DataFrame with Custom Index:\n",
      "       year   state  pop debt\n",
      "one    2000    Ohio  1.5  NaN\n",
      "two    2001    Ohio  1.7  NaN\n",
      "three  2002    Ohio  3.6  NaN\n",
      "four   2001  Nevada  2.4  NaN\n",
      "five   2002  Nevada  2.9  NaN\n",
      "six    2003  Nevada  3.2  NaN\n",
      "\n",
      "🔷 Modified DataFrame:\n",
      "       year   state  pop  debt\n",
      "one    2000    Ohio  1.5   NaN\n",
      "two    2001    Ohio  1.7  -1.2\n",
      "three  2002    Ohio  3.6   NaN\n",
      "four   2001  Nevada  2.4  -1.5\n",
      "five   2002  Nevada  2.9  -1.7\n",
      "six    2003  Nevada  3.2   NaN\n",
      "\n",
      "🔷 DataFrame Properties:\n",
      "Year column sample:\n",
      " entry\n",
      "one      2000\n",
      "two      2001\n",
      "three    2002\n",
      "four     2001\n",
      "five     2002\n",
      "Name: year, dtype: int64\n",
      "\n",
      "Row 'three' data:\n",
      " info\n",
      "year     2002\n",
      "state    Ohio\n",
      "pop       3.6\n",
      "debt      NaN\n",
      "Name: three, dtype: object\n",
      "\n",
      "Values array sample:\n",
      " [[2000 'Ohio' 1.5 nan]\n",
      " [2001 'Ohio' 1.7 nan]]\n",
      "\n",
      "🔷 Transposed Nested Dict DataFrame:\n",
      "        2001  2002  2000\n",
      "Nevada   2.4   2.9   NaN\n",
      "Ohio     1.7   3.6   1.5\n",
      "\n",
      "📋 DATAFRAME CREATION CHEATSHEET\n",
      "---------------------------------\n",
      "| Source                 | Example                          |\n",
      "|------------------------|----------------------------------|\n",
      "| Dict of lists          | pd.DataFrame({'col': [1,2,3]})   |\n",
      "| List of dicts          | pd.DataFrame([{'a':1}, {'a':2}])|\n",
      "| Nested dictionary      | pd.DataFrame({'A': {1: 'a'}})   |\n",
      "| 2D NumPy array         | pd.DataFrame(np.array([[1,2]])) |\n",
      "| CSV file               | pd.read_csv('data.csv')          |\n",
      "| Excel file             | pd.read_excel('data.xlsx')       |\n",
      "\n",
      "🔧 KEY OPERATIONS:\n",
      "- Add column: df['new'] = values\n",
      "- Delete column: del df['col']\n",
      "- Access column: df.col or df['col']\n",
      "- Access row: df.loc[index]\n",
      "- Transpose: df.T\n",
      "- Head/Tail: df.head(3), df.tail(2)\n",
      "\n",
      "\n",
      "🌟 ESSENTIAL INSIGHTS 🌟\n",
      "1. Heterogeneous Data: Columns can have different data types\n",
      "2. Flexible Indexing: Both row and column labels support complex operations\n",
      "3. Alignment: Operations automatically align data by index/columns\n",
      "4. Missing Data: NaN represents missing values (handle with dropna/fillna)\n",
      "5. Column Types: Access via attribute (valid names) or dict-style (any names)\n",
      "6. Performance: Underlying NumPy arrays enable vectorized operations\n",
      "\n",
      "💡 PRO TIPS:\n",
      "- Use .copy() when creating DataFrame copies to avoid view vs copy issues\n",
      "- Prefer .loc[] for explicit label-based indexing\n",
      "- Set index/column names for better visualizations and merges\n",
      "- Use .assign() for chainable column creation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PANDAS DATAFRAME ESSENTIALS\n",
    "============================\n",
    "A comprehensive guide to pandas DataFrame with practical examples.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# DataFrame Creation\n",
    "# ---------------------------\n",
    "\n",
    "def create_dataframes():\n",
    "    # From dictionary of lists\n",
    "    data = {\n",
    "        'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n",
    "        'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]\n",
    "    }\n",
    "    df1 = pd.DataFrame(data)\n",
    "    \n",
    "    # With specified column order and index\n",
    "    df2 = pd.DataFrame(data, \n",
    "                      columns=['year', 'state', 'pop', 'debt'],\n",
    "                      index=['one', 'two', 'three', 'four', 'five', 'six'])\n",
    "    \n",
    "    # From nested dictionary\n",
    "    pop_data = {\n",
    "        'Nevada': {2001: 2.4, 2002: 2.9},\n",
    "        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}\n",
    "    }\n",
    "    df3 = pd.DataFrame(pop_data)\n",
    "    \n",
    "    return df1, df2, df3\n",
    "\n",
    "# ---------------------------\n",
    "# Column Operations\n",
    "# ---------------------------\n",
    "\n",
    "def column_operations(df):\n",
    "    # Add new column\n",
    "    df['debt'] = np.arange(6.)\n",
    "    \n",
    "    # Modify with Series (alignment)\n",
    "    val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])\n",
    "    df['debt'] = val\n",
    "    \n",
    "    # Create boolean column\n",
    "    df['eastern'] = df.state == 'Ohio'\n",
    "    \n",
    "    # Delete column\n",
    "    del df['eastern']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# DataFrame Properties\n",
    "# ---------------------------\n",
    "\n",
    "def dataframe_properties(df):\n",
    "    # Access columns\n",
    "    year_col = df.year        # Attribute-style\n",
    "    state_col = df['state']   # Dict-style\n",
    "    \n",
    "    # Row access\n",
    "    row = df.loc['three']\n",
    "    \n",
    "    # Metadata\n",
    "    df.index.name = 'entry'\n",
    "    df.columns.name = 'info'\n",
    "    \n",
    "    # Values array\n",
    "    values = df.values\n",
    "    \n",
    "    return {\n",
    "        'year_col': year_col.head(),\n",
    "        'row_data': row,\n",
    "        'values_sample': values[:2]\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Cheatsheet & Takeaways\n",
    "# ---------------------------\n",
    "\n",
    "DATAFRAME_CHEATSHEET = \"\"\"\n",
    "📋 DATAFRAME CREATION CHEATSHEET\n",
    "---------------------------------\n",
    "| Source                 | Example                          |\n",
    "|------------------------|----------------------------------|\n",
    "| Dict of lists          | pd.DataFrame({'col': [1,2,3]})   |\n",
    "| List of dicts          | pd.DataFrame([{'a':1}, {'a':2}])|\n",
    "| Nested dictionary      | pd.DataFrame({'A': {1: 'a'}})   |\n",
    "| 2D NumPy array         | pd.DataFrame(np.array([[1,2]])) |\n",
    "| CSV file               | pd.read_csv('data.csv')          |\n",
    "| Excel file             | pd.read_excel('data.xlsx')       |\n",
    "\n",
    "🔧 KEY OPERATIONS:\n",
    "- Add column: df['new'] = values\n",
    "- Delete column: del df['col']\n",
    "- Access column: df.col or df['col']\n",
    "- Access row: df.loc[index]\n",
    "- Transpose: df.T\n",
    "- Head/Tail: df.head(3), df.tail(2)\n",
    "\"\"\"\n",
    "\n",
    "KEY_TAKEAWAYS = \"\"\"\n",
    "🌟 ESSENTIAL INSIGHTS 🌟\n",
    "1. Heterogeneous Data: Columns can have different data types\n",
    "2. Flexible Indexing: Both row and column labels support complex operations\n",
    "3. Alignment: Operations automatically align data by index/columns\n",
    "4. Missing Data: NaN represents missing values (handle with dropna/fillna)\n",
    "5. Column Types: Access via attribute (valid names) or dict-style (any names)\n",
    "6. Performance: Underlying NumPy arrays enable vectorized operations\n",
    "\n",
    "💡 PRO TIPS:\n",
    "- Use .copy() when creating DataFrame copies to avoid view vs copy issues\n",
    "- Prefer .loc[] for explicit label-based indexing\n",
    "- Set index/column names for better visualizations and merges\n",
    "- Use .assign() for chainable column creation\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and demonstrate DataFrames\n",
    "    df1, df2, df3 = create_dataframes()\n",
    "    print(\"🔷 Basic DataFrame:\")\n",
    "    print(df1.head())\n",
    "    \n",
    "    print(\"\\n🔷 DataFrame with Custom Index:\")\n",
    "    print(df2)\n",
    "    \n",
    "    # Column operations demo\n",
    "    modified_df = column_operations(df2.copy())\n",
    "    print(\"\\n🔷 Modified DataFrame:\")\n",
    "    print(modified_df)\n",
    "    \n",
    "    # Properties demonstration\n",
    "    props = dataframe_properties(df2.copy())\n",
    "    print(\"\\n🔷 DataFrame Properties:\")\n",
    "    print(\"Year column sample:\\n\", props['year_col'])\n",
    "    print(\"\\nRow 'three' data:\\n\", props['row_data'])\n",
    "    print(\"\\nValues array sample:\\n\", props['values_sample'])\n",
    "    \n",
    "    # Show transposed nested dict DataFrame\n",
    "    print(\"\\n🔷 Transposed Nested Dict DataFrame:\")\n",
    "    print(df3.T)\n",
    "    \n",
    "    # Display cheatsheet and takeaways\n",
    "    print(DATAFRAME_CHEATSHEET)\n",
    "    print(KEY_TAKEAWAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Series by index:\n",
      " a    1\n",
      "b    2\n",
      "c    3\n",
      "d    0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame sorted by row index:\n",
      "        d  a  b  c\n",
      "one    4  5  6  7\n",
      "three  0  1  2  3\n",
      "\n",
      "DataFrame sorted by column index:\n",
      "        a  b  c  d\n",
      "three  1  2  3  0\n",
      "one    5  6  7  4\n",
      "\n",
      "Series sorted by values (NaN at end):\n",
      " 4   -3.0\n",
      "5    2.0\n",
      "0    4.0\n",
      "2    7.0\n",
      "1    NaN\n",
      "3    NaN\n",
      "dtype: float64\n",
      "\n",
      "DataFrame sorted by column 'b':\n",
      "    b  a\n",
      "2 -3  0\n",
      "3  2  1\n",
      "0  4  0\n",
      "1  7  1\n",
      "\n",
      "DataFrame sorted by multiple columns:\n",
      "    b  a\n",
      "2 -3  0\n",
      "0  4  0\n",
      "3  2  1\n",
      "1  7  1\n",
      "\n",
      "Default ranking (average ties):\n",
      " 0    6.5\n",
      "1    1.0\n",
      "2    6.5\n",
      "3    4.5\n",
      "4    3.0\n",
      "5    2.0\n",
      "6    4.5\n",
      "dtype: float64\n",
      "\n",
      "Ranking by first occurrence:\n",
      " 0    6.0\n",
      "1    1.0\n",
      "2    7.0\n",
      "3    4.0\n",
      "4    3.0\n",
      "5    2.0\n",
      "6    5.0\n",
      "dtype: float64\n",
      "\n",
      "Descending ranking (max method):\n",
      " 0    2.0\n",
      "1    7.0\n",
      "2    2.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "6    4.0\n",
      "dtype: float64\n",
      "\n",
      "DataFrame ranked across columns:\n",
      "      b    a    c\n",
      "0  3.0  2.0  1.0\n",
      "1  3.0  1.0  2.0\n",
      "2  1.0  2.0  3.0\n",
      "3  3.0  2.0  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nKey Takeaways:\\n• sort_index() sorts by index labels (ascending by default)\\n• sort_values() sorts by data values, handles NaN by sending to end\\n• DataFrame sorting: use `by` parameter for column-based sorting\\n• rank() handles ties with methods: 'average' (default), 'min', 'max', 'first', 'dense'\\n• Ranking can be applied to both Series and DataFrame axes\\n• ascending=False reverses sort/rank order\\n• axis=1 parameter operates on columns instead of rows\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Sorting Examples ---\n",
    "\n",
    "# Sorting Series by index\n",
    "obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])\n",
    "print(\"Sorted Series by index:\\n\", obj.sort_index())\n",
    "\n",
    "# Sorting DataFrame by index (rows)\n",
    "frame = pd.DataFrame(np.arange(8).reshape((2, 4)),\n",
    "                     index=['three', 'one'],\n",
    "                     columns=['d', 'a', 'b', 'c'])\n",
    "print(\"\\nDataFrame sorted by row index:\\n\", frame.sort_index())\n",
    "\n",
    "# Sorting DataFrame columns\n",
    "print(\"\\nDataFrame sorted by column index:\\n\", frame.sort_index(axis=1))\n",
    "\n",
    "# Sorting Series by values (handles NaN)\n",
    "obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])\n",
    "print(\"\\nSeries sorted by values (NaN at end):\\n\", obj.sort_values())\n",
    "\n",
    "# Sorting DataFrame by column values\n",
    "frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})\n",
    "print(\"\\nDataFrame sorted by column 'b':\\n\", frame.sort_values(by='b'))\n",
    "print(\"\\nDataFrame sorted by multiple columns:\\n\", frame.sort_values(by=['a', 'b']))\n",
    "\n",
    "# --- Ranking Examples ---\n",
    "\n",
    "# Default ranking (average for ties)\n",
    "obj = pd.Series([7, -5, 7, 4, 2, 0, 4])\n",
    "print(\"\\nDefault ranking (average ties):\\n\", obj.rank())\n",
    "\n",
    "# Ranking by first occurrence\n",
    "print(\"\\nRanking by first occurrence:\\n\", obj.rank(method='first'))\n",
    "\n",
    "# Descending ranking with max method\n",
    "print(\"\\nDescending ranking (max method):\\n\", obj.rank(ascending=False, method='max'))\n",
    "\n",
    "# Ranking across DataFrame columns\n",
    "frame = pd.DataFrame({'b': [4.3, 7, -3, 2], \n",
    "                     'a': [0, 1, 0, 1],\n",
    "                     'c': [-2, 5, 8, -2.5]})\n",
    "print(\"\\nDataFrame ranked across columns:\\n\", frame.rank(axis='columns'))\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• sort_index() sorts by index labels (ascending by default)\n",
    "• sort_values() sorts by data values, handles NaN by sending to end\n",
    "• DataFrame sorting: use `by` parameter for column-based sorting\n",
    "• rank() handles ties with methods: 'average' (default), 'min', 'max', 'first', 'dense'\n",
    "• Ranking can be applied to both Series and DataFrame axes\n",
    "• ascending=False reverses sort/rank order\n",
    "• axis=1 parameter operates on columns instead of rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Series with Duplicate Indices ---\n",
    "obj = pd.Series(range(5), index=['a', 'a', 'b', 'b', 'c'])\n",
    "print(\"Series with duplicate indices:\\n\", obj)\n",
    "\n",
    "# Check index uniqueness\n",
    "print(\"\\nIs index unique?\", obj.index.is_unique)  # False\n",
    "\n",
    "# Data selection behavior\n",
    "print(\"\\nSelecting 'a' (multiple entries):\\n\", obj['a'])  # Returns Series\n",
    "print(\"\\nSelecting 'c' (single entry):\\n\", obj['c'])      # Returns scalar\n",
    "\n",
    "# --- DataFrame with Duplicate Row Indices ---\n",
    "df = pd.DataFrame(np.random.randn(4, 3), \n",
    "                  index=['a', 'a', 'b', 'b'],\n",
    "                  columns=['X', 'Y', 'Z'])\n",
    "print(\"\\nDataFrame with duplicate row indices:\\n\", df)\n",
    "\n",
    "# Indexing with duplicate labels\n",
    "print(\"\\nSelecting 'b' rows:\\n\", df.loc['b'])  # Returns DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• Pandas allows duplicate labels in indices (is_unique = False)\n",
    "• Indexing behavior changes with duplicates:\n",
    "   - Multiple entries return Series/DataFrame\n",
    "   - Single entries return scalar value\n",
    "• This affects:\n",
    "   - Data selection (loc/iloc)\n",
    "   - Aggregation operations\n",
    "   - Merge/join operations\n",
    "• Use index.duplicated() to identify duplicates\n",
    "• Consider using reset_index() for unique indices when needed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT-IBM Correlation: 1.0000\n",
      "MSFT-IBM Covariance: 0.000074\n",
      "\n",
      "Correlation Matrix:\n",
      "           AAPL       IBM      MSFT      GOOG\n",
      "AAPL  1.000000 -0.243195 -0.245647  0.083972\n",
      "IBM  -0.243195  1.000000  0.999996  0.907900\n",
      "MSFT -0.245647  0.999996  1.000000  0.906824\n",
      "GOOG  0.083972  0.907900  0.906824  1.000000\n",
      "\n",
      "Covariance Matrix:\n",
      "               AAPL           IBM          MSFT          GOOG\n",
      "AAPL  1.571054e-08 -3.017291e-07 -2.291559e-07  3.482984e-08\n",
      "IBM  -3.017291e-07  9.797924e-05  7.366969e-05  2.973894e-05\n",
      "MSFT -2.291559e-07  7.366969e-05  5.539197e-05  2.233400e-05\n",
      "GOOG  3.482984e-08  2.973894e-05  2.233400e-05  1.095067e-05\n",
      "\n",
      "Correlation with IBM:\n",
      " AAPL   -0.243195\n",
      "IBM     1.000000\n",
      "MSFT    0.999996\n",
      "GOOG    0.907900\n",
      "dtype: float64\n",
      "\n",
      "Correlation with Shifted Returns:\n",
      " AAPL    1.000000\n",
      "IBM    -0.509879\n",
      "MSFT   -0.507432\n",
      "GOOG   -0.824131\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nKey Takeaways:\\n• corr() computes Pearson correlation coefficient between Series/DataFrame columns\\n• cov() computes covariance between data points\\n• DataFrame.corr()/cov() return full matrices showing pairwise relationships\\n• corrwith() enables:\\n   - Column-wise correlation with a Series\\n   - Row-wise correlation with axis='rows'\\n   - Cross-correlation between two DataFrames\\n• Automatic handling of:\\n   - Missing values (aligned by index)\\n   - Non-overlapping indices (excluded from calculations)\\n• pct_change() is commonly used to calculate returns in financial time series\\n• Correlation values range [-1, 1], measuring linear relationships\\n• Covariance values depend on data scale (use correlation for standardized comparison)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "\n",
    "# --- Example Setup (using mock data if Yahoo API unavailable) ---\n",
    "try:\n",
    "    # Attempt to fetch real stock data (may fail due to API changes)\n",
    "    all_data = {\n",
    "        ticker: web.get_data_yahoo(ticker, start=datetime.datetime(2020, 1, 1))\n",
    "        for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']\n",
    "    }\n",
    "    price = pd.DataFrame({ticker: data['Adj Close'] \n",
    "                         for ticker, data in all_data.items()})\n",
    "except:\n",
    "    # Fallback mock data if API fails\n",
    "    dates = pd.date_range('20200101', periods=5)\n",
    "    price = pd.DataFrame({\n",
    "        'AAPL': [100, 101, 102, 103, 104],\n",
    "        'IBM': [150, 152, 151, 153, 155],\n",
    "        'MSFT': [200, 202, 201, 203, 205],\n",
    "        'GOOG': [1000, 1005, 1003, 1008, 1010]\n",
    "    }, index=dates)\n",
    "\n",
    "# Calculate percent changes\n",
    "returns = price.pct_change()\n",
    "\n",
    "# --- Correlation and Covariance Calculations ---\n",
    "# Correlation between two Series\n",
    "msft_ibm_corr = returns['MSFT'].corr(returns['IBM'])\n",
    "print(f\"MSFT-IBM Correlation: {msft_ibm_corr:.4f}\")\n",
    "\n",
    "# Covariance between two Series\n",
    "msft_ibm_cov = returns['MSFT'].cov(returns['IBM'])\n",
    "print(f\"MSFT-IBM Covariance: {msft_ibm_cov:.6f}\")\n",
    "\n",
    "# Full correlation matrix\n",
    "corr_matrix = returns.corr()\n",
    "print(\"\\nCorrelation Matrix:\\n\", corr_matrix)\n",
    "\n",
    "# Full covariance matrix\n",
    "cov_matrix = returns.cov()\n",
    "print(\"\\nCovariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "# Pairwise correlation with a Series\n",
    "corr_with_ibm = returns.corrwith(returns['IBM'])\n",
    "print(\"\\nCorrelation with IBM:\\n\", corr_with_ibm)\n",
    "\n",
    "# Pairwise correlation with another DataFrame (example with shifted returns)\n",
    "shifted = returns.shift(1)\n",
    "corr_with_shifted = returns.corrwith(shifted)\n",
    "print(\"\\nCorrelation with Shifted Returns:\\n\", corr_with_shifted)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• corr() computes Pearson correlation coefficient between Series/DataFrame columns\n",
    "• cov() computes covariance between data points\n",
    "• DataFrame.corr()/cov() return full matrices showing pairwise relationships\n",
    "• corrwith() enables:\n",
    "   - Column-wise correlation with a Series\n",
    "   - Row-wise correlation with axis='rows'\n",
    "   - Cross-correlation between two DataFrames\n",
    "• Automatic handling of:\n",
    "   - Missing values (aligned by index)\n",
    "   - Non-overlapping indices (excluded from calculations)\n",
    "• pct_change() is commonly used to calculate returns in financial time series\n",
    "• Correlation values range [-1, 1], measuring linear relationships\n",
    "• Covariance values depend on data scale (use correlation for standardized comparison)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Unique Values and Value Counts ---\n",
    "obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\n",
    "\n",
    "# Extract unique values (order preserved)\n",
    "uniques = obj.unique()\n",
    "print(\"Unique values:\\n\", uniques)\n",
    "\n",
    "# Frequency counts (sorted descending by default)\n",
    "print(\"\\nValue counts:\\n\", obj.value_counts())\n",
    "\n",
    "# Top-level value_counts with sort=False\n",
    "print(\"\\nUnsorted value counts:\\n\", pd.value_counts(obj.values, sort=False))\n",
    "\n",
    "# --- Set Membership Checks ---\n",
    "# Filter using isin()\n",
    "mask = obj.isin(['b', 'c'])\n",
    "print(\"\\nFiltered Series:\\n\", obj[mask])\n",
    "\n",
    "# --- Index Alignment with get_indexer ---\n",
    "to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])\n",
    "unique_vals = pd.Series(['c', 'b', 'a'])\n",
    "indices = pd.Index(unique_vals).get_indexer(to_match)\n",
    "print(\"\\nIndex alignment:\\n\", indices)\n",
    "\n",
    "# --- DataFrame Value Counts ---\n",
    "data = pd.DataFrame({\n",
    "    'Qu1': [1, 3, 4, 3, 4],\n",
    "    'Qu2': [2, 3, 1, 2, 3],\n",
    "    'Qu3': [1, 5, 2, 4, 4]\n",
    "})\n",
    "\n",
    "# Apply value_counts across columns\n",
    "result = data.apply(pd.value_counts).fillna(0)\n",
    "print(\"\\nDataFrame value counts:\\n\", result)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• unique() preserves insertion order of first occurrence\n",
    "• value_counts() returns sorted frequency counts (descending)\n",
    "• isin() enables vectorized membership filtering\n",
    "• get_indexer() maps values to positions in a unique list\n",
    "• DataFrame.apply(value_counts) creates cross-column frequency tables\n",
    "• These methods handle:\n",
    "   - Categorical data analysis\n",
    "   - Data alignment operations\n",
    "   - Frequency-based feature engineering\n",
    "• Missing values are excluded by default in counts\n",
    "• Use sort=False to preserve original value order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 6: Data Loading, Storage, and File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Basic CSV Reading ---\n",
    "# Read CSV with header\n",
    "df = pd.read_csv('examples/ex1.csv')\n",
    "print(\"Basic CSV read:\\n\", df)\n",
    "\n",
    "# Equivalent using read_table with explicit separator\n",
    "df = pd.read_table('examples/ex1.csv', sep=',')\n",
    "print(\"\\nUsing read_table:\\n\", df)\n",
    "\n",
    "# --- Handling No Headers ---\n",
    "# Read CSV without header, auto-generate column names\n",
    "df_no_header = pd.read_csv('examples/ex2.csv', header=None)\n",
    "print(\"\\nNo header CSV:\\n\", df_no_header)\n",
    "\n",
    "# Specify custom column names\n",
    "names = ['a', 'b', 'c', 'd', 'message']\n",
    "df_custom_names = pd.read_csv('examples/ex2.csv', names=names)\n",
    "print(\"\\nCustom column names:\\n\", df_custom_names)\n",
    "\n",
    "# Set index column during import\n",
    "df_indexed = pd.read_csv('examples/ex2.csv', names=names, index_col='message')\n",
    "print(\"\\nMessage as index:\\n\", df_indexed)\n",
    "\n",
    "# --- Hierarchical Indexing ---\n",
    "# Create multi-level index from multiple columns\n",
    "df_mindex = pd.read_csv('examples/csv_mindex.csv', index_col=['key1', 'key2'])\n",
    "print(\"\\nHierarchical index:\\n\", df_mindex)\n",
    "\n",
    "# --- Custom Delimiters ---\n",
    "# Read whitespace-separated file using regex\n",
    "df_whitespace = pd.read_table('examples/ex3.txt', sep='\\s+')\n",
    "print(\"\\nWhitespace-delimited data:\\n\", df_whitespace)\n",
    "\n",
    "# --- Skipping Rows ---\n",
    "# Skip specific rows during import\n",
    "df_skipped = pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])\n",
    "print(\"\\nAfter skipping rows:\\n\", df_skipped)\n",
    "\n",
    "# --- Handling Missing Data ---\n",
    "# Default NA value handling\n",
    "df_missing = pd.read_csv('examples/ex5.csv')\n",
    "print(\"\\nDefault missing values:\\n\", df_missing.isnull())\n",
    "\n",
    "# Custom NA sentinels\n",
    "df_custom_na = pd.read_csv('examples/ex5.csv', na_values=['NULL', 'foo'])\n",
    "print(\"\\nCustom NA values:\\n\", df_custom_na)\n",
    "\n",
    "# Column-specific NA values\n",
    "sentinels = {'message': ['NA'], 'something': ['two']}\n",
    "df_col_na = pd.read_csv('examples/ex5.csv', na_values=sentinels)\n",
    "print(\"\\nColumn-specific NA handling:\\n\", df_col_na)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• Primary text loading functions: read_csv() and read_table()\n",
    "• Core parameters:\n",
    "   - sep/delimiter for field separation\n",
    "   - header for column name handling\n",
    "   - index_col for index specification\n",
    "   - skiprows to omit specific lines\n",
    "   - na_values for missing data customization\n",
    "• Automatic type inference for columns\n",
    "• Support for hierarchical indexing via multi-column keys\n",
    "• Flexible missing value handling with per-column sentinels\n",
    "• Ability to parse whitespace and regex-delimited formats\n",
    "• Built-in support for:\n",
    "   - Comment handling\n",
    "   - Date parsing\n",
    "   - Thousands separators\n",
    "• Use verbose=True for parsing diagnostics\n",
    "• Consider chunksize for large files\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# --- Reading Large Files in Chunks ---\n",
    "# Set display options for readability\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# Read first 5 rows of large file\n",
    "small_chunk = pd.read_csv('examples/ex6.csv', nrows=5)\n",
    "print(\"First 5 rows:\\n\", small_chunk)\n",
    "\n",
    "# Process file in 1,000-row chunks\n",
    "chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)\n",
    "key_counts = pd.Series(dtype=float)\n",
    "\n",
    "for chunk in chunker:\n",
    "    key_counts = key_counts.add(chunk['key'].value_counts(), fill_value=0)\n",
    "\n",
    "key_counts = key_counts.sort_values(ascending=False)\n",
    "print(\"\\nTop 10 key frequencies:\\n\", key_counts[:10])\n",
    "\n",
    "# --- Writing Data to Text Files ---\n",
    "# Sample data from earlier example\n",
    "data = pd.read_csv('examples/ex5.csv')\n",
    "\n",
    "# Basic CSV export\n",
    "data.to_csv('examples/out.csv')  # Writes to file\n",
    "print(\"\\nDefault CSV output:\")\n",
    "data.to_csv(sys.stdout)  # Print to console\n",
    "\n",
    "# Custom delimiter and missing value handling\n",
    "print(\"\\nPipe-delimited with NULLs:\")\n",
    "data.to_csv(sys.stdout, sep='|', na_rep='NULL')\n",
    "\n",
    "# Exclude index and headers\n",
    "print(\"\\nNo index or headers:\")\n",
    "data.to_csv(sys.stdout, index=False, header=False)\n",
    "\n",
    "# Select specific columns\n",
    "print(\"\\nSelected columns output:\")\n",
    "data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])\n",
    "\n",
    "# --- Time Series Export ---\n",
    "dates = pd.date_range('1/1/2000', periods=7)\n",
    "ts = pd.Series(np.arange(7), index=dates)\n",
    "ts.to_csv('examples/tseries.csv')\n",
    "print(\"\\nTime series CSV:\")\n",
    "with open('examples/tseries.csv') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# --- Manual CSV Processing Example ---\n",
    "# Simulated CSV content (replace with real file path for actual use)\n",
    "csv_content = '''\"a\",\"b\",\"c\"\n",
    "\"1\",\"2\",\"3\"\n",
    "\"1\",\"2\",\"3\"'''\n",
    "\n",
    "# Using StringIO to simulate file handling\n",
    "with StringIO(csv_content) as f:\n",
    "    reader = csv.reader(f)\n",
    "    lines = list(reader)\n",
    "\n",
    "header, values = lines[0], lines[1:]\n",
    "data_dict = {h: tuple(v) for h, v in zip(header, zip(*values))}\n",
    "\n",
    "print(\"Data Dictionary:\\n\", data_dict)\n",
    "\n",
    "# --- Custom CSV Dialect ---\n",
    "class MyDialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "    skipinitialspace = True\n",
    "\n",
    "# Writing CSV with custom dialect\n",
    "output = StringIO()\n",
    "writer = csv.writer(output, dialect=MyDialect)\n",
    "writer.writerow(('one', 'two', 'three'))\n",
    "writer.writerows([\n",
    "    ('1', '2', '3'),\n",
    "    ('4', '5', '6'),\n",
    "    ('7', '8', '9')\n",
    "])\n",
    "\n",
    "print(\"\\nCSV Output with Custom Dialect:\\n\", output.getvalue())\n",
    "\n",
    "# --- Handling Different Delimiters ---\n",
    "# Read semicolon-separated values\n",
    "ssv_content = \"\"\"col1;col2;col3\n",
    "value1;value2;value3\"\"\"\n",
    "\n",
    "with StringIO(ssv_content) as f:\n",
    "    reader = csv.reader(f, delimiter=';')\n",
    "    print(\"\\nSemicolon-delimited data:\\n\", list(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Create Sample DataFrame ---\n",
    "data = {\n",
    "    'a': [1, 5, 9],\n",
    "    'b': [2, 6, 10],\n",
    "    'c': [3, 7, 11],\n",
    "    'd': [4, 8, 12],\n",
    "    'message': ['hello', 'world', 'foo']\n",
    "}\n",
    "frame = pd.DataFrame(data)\n",
    "\n",
    "# --- Pickle Serialization ---\n",
    "# Save to pickle format\n",
    "frame.to_pickle('frame_pickle')\n",
    "\n",
    "# Load from pickle\n",
    "loaded_frame = pd.read_pickle('frame_pickle')\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"Original DataFrame:\")\n",
    "print(frame)\n",
    "print(\"\\nLoaded DataFrame:\")\n",
    "print(loaded_frame)\n",
    "print(\"DataFrames equal:\", frame.equals(loaded_frame))\n",
    "\n",
    "# --- HDF5 Example (requires pytables) ---\n",
    "try:\n",
    "    frame.to_hdf('frame.h5', 'data', format='table')\n",
    "    hdf_frame = pd.read_hdf('frame.h5', 'data')\n",
    "    print(\"\\nHDF5 load successful\")\n",
    "except ImportError:\n",
    "    print(\"\\nHDF5 support requires pytables installation\")\n",
    "\n",
    "# --- Feather Format (requires feather-format) ---\n",
    "try:\n",
    "    frame.to_feather('frame.feather')\n",
    "    feather_frame = pd.read_feather('frame.feather')\n",
    "    print(\"Feather load successful\")\n",
    "except ImportError:\n",
    "    print(\"Feather support requires feather-format installation\")\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "• Pickle format:\n",
    "  - Python-specific binary serialization\n",
    "  - Fast and convenient for temporary storage\n",
    "  - Not recommended for long-term storage (version compatibility risks)\n",
    "  \n",
    "• HDF5:\n",
    "  - Hierarchical Data Format for large datasets\n",
    "  - Supports on-disk storage and partial reads\n",
    "  - Requires pytables package\n",
    "\n",
    "• Feather:\n",
    "  - Cross-language (Python/R) columnar format\n",
    "  - Uses Apache Arrow for efficient storage\n",
    "  - Requires feather-format package\n",
    "\n",
    "• Other formats:\n",
    "  - MessagePack (binary JSON-like)\n",
    "  - bcolz (compressed columnar storage)\n",
    "  - Parquet (via pyarrow)\n",
    "\n",
    "• Best practices:\n",
    "  - Use pickle for short-term caching\n",
    "  - Prefer HDF5/Feather/Parquet for production workflows\n",
    "  - Always verify data after deserialization\n",
    "  - Consider compression options for large datasets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Sample Data Preparation ---\n",
    "data = {\n",
    "    'a': [1, 5, 9],\n",
    "    'b': [2, 6, 10],\n",
    "    'c': [3, 7, 11],\n",
    "    'd': [4, 8, 12],\n",
    "    'message': ['hello', 'world', 'foo']\n",
    "}\n",
    "frame = pd.DataFrame(data)\n",
    "\n",
    "# --- Writing to Excel ---\n",
    "# Write to .xlsx file (requires openpyxl)\n",
    "try:\n",
    "    frame.to_excel('examples/ex2.xlsx', sheet_name='Sheet1', index=False)\n",
    "    print(\"Excel file written successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing Excel: {e}\")\n",
    "\n",
    "# --- Reading from Excel ---\n",
    "# Method 1: Using ExcelFile for multiple sheets\n",
    "try:\n",
    "    xlsx = pd.ExcelFile('examples/ex1.xlsx')\n",
    "    df1 = xlsx.parse('Sheet1')\n",
    "    print(\"\\nData from ExcelFile method:\\n\", df1.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found - ensure xlrd is installed for .xls files\")\n",
    "\n",
    "# Method 2: Direct read_excel (for single sheet)\n",
    "try:\n",
    "    df2 = pd.read_excel('examples/ex1.xlsx', sheet_name='Sheet1')\n",
    "    print(\"\\nData from read_excel:\\n\", df2.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel: {e}\")\n",
    "\n",
    "# --- Handling Multiple Sheets ---\n",
    "# Read all sheets\n",
    "with pd.ExcelFile('examples/ex1.xlsx') as xls:\n",
    "    sheets = {sh: xls.parse(sh) for sh in xls.sheet_names}\n",
    "    print(\"\\nSheet names:\", xls.sheet_names)\n",
    "\n",
    "# --- Writing Multiple Sheets ---\n",
    "with pd.ExcelWriter('examples/multi_sheet.xlsx') as writer:\n",
    "    frame.to_excel(writer, sheet_name='Data1', index=False)\n",
    "    frame.describe().to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Excel I/O Requirements:\n",
    "   - xlrd: For reading .xls files\n",
    "   - openpyxl: For writing .xlsx files\n",
    "   - Install via: pip install xlrd openpyxl\n",
    "\n",
    "2. Reading Options:\n",
    "   - ExcelFile.parse() for multiple sheets\n",
    "   - read_excel() for single sheets\n",
    "   - Specify sheet names/indices\n",
    "   - Handle headers and skiprows\n",
    "\n",
    "3. Writing Options:\n",
    "   - ExcelWriter for multiple sheets\n",
    "   - Direct to_excel() for simple writes\n",
    "   - Control index inclusion/exclusion\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use context managers (with) for writers\n",
    "   - Verify file extensions match engine\n",
    "   - Handle exceptions for missing files/dependencies\n",
    "   - Prefer .xlsx format for new files\n",
    "\n",
    "5. Performance Notes:\n",
    "   - ExcelFile faster for multiple sheet reads\n",
    "   - Set index=False to avoid extra index column\n",
    "   - Use converters for data type control\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# --- GitHub API Example ---\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "\n",
    "try:\n",
    "    # Make GET request to GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise HTTP errors\n",
    "    \n",
    "    # Parse JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Create DataFrame from selected fields\n",
    "    issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state'])\n",
    "    print(\"First 5 GitHub Issues:\\n\", issues.head())\n",
    "    \n",
    "    # Show labels from first issue\n",
    "    print(\"\\nLabels from first issue:\\n\", issues['labels'].iloc[0])\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "\n",
    "# --- Handling Pagination Example ---\n",
    "def get_all_issues():\n",
    "    \"\"\"Fetch all issues (handling pagination)\"\"\"\n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {'page': page, 'per_page': 100}\n",
    "        resp = requests.get(url, params=params)\n",
    "        current_data = resp.json()\n",
    "        if not current_data:\n",
    "            break\n",
    "        all_issues.extend(current_data)\n",
    "        page += 1\n",
    "    return pd.DataFrame(all_issues)\n",
    "\n",
    "# Uncomment to test pagination (may hit rate limits)\n",
    "# full_issues = get_all_issues()\n",
    "# print(f\"Total issues: {len(full_issues)}\")\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. API Interaction:\n",
    "   - Use requests.get() to access web APIs\n",
    "   - response.json() parses JSON to Python objects\n",
    "   - Handle HTTP errors with response.raise_for_status()\n",
    "\n",
    "2. DataFrame Construction:\n",
    "   - Select specific columns using the columns parameter\n",
    "   - Nested data (like labels) remains as JSON objects in cells\n",
    "\n",
    "3. Practical Considerations:\n",
    "   - GitHub API rate limits (authenticate for higher limits)\n",
    "   - Pagination handling required for large datasets\n",
    "   - Data cleaning often needed for nested JSON structures\n",
    "\n",
    "4. Advanced Patterns:\n",
    "   - Parameterize requests (e.g., page numbers)\n",
    "   - Use while loops for pagination\n",
    "   - Combine data from multiple requests\n",
    "\n",
    "5. Error Handling:\n",
    "   - Catch RequestException for network issues\n",
    "   - Validate response status codes\n",
    "   - Handle potential JSON parsing errors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# --- Create SQLite Database ---\n",
    "# Establish connection\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "\n",
    "# Create table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS test (\n",
    "    a VARCHAR(20),\n",
    "    b VARCHAR(20),\n",
    "    c REAL,\n",
    "    d INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "con.execute(create_table_query)\n",
    "con.commit()\n",
    "\n",
    "# Insert data\n",
    "data = [\n",
    "    ('Atlanta', 'Georgia', 1.25, 6),\n",
    "    ('Tallahassee', 'Florida', 2.6, 3),\n",
    "    ('Sacramento', 'California', 1.7, 5)\n",
    "]\n",
    "insert_query = \"INSERT INTO test VALUES (?, ?, ?, ?)\"\n",
    "con.executemany(insert_query, data)\n",
    "con.commit()\n",
    "\n",
    "# --- Manual Data Retrieval ---\n",
    "cursor = con.execute(\"SELECT * FROM test\")\n",
    "rows = cursor.fetchall()\n",
    "columns = [col[0] for col in cursor.description]\n",
    "\n",
    "manual_df = pd.DataFrame(rows, columns=columns)\n",
    "print(\"Manual DataFrame:\\n\", manual_df)\n",
    "\n",
    "# --- Using SQLAlchemy ---\n",
    "engine = sqlalchemy.create_engine('sqlite:///mydata.sqlite')\n",
    "sql_df = pd.read_sql(\"SELECT * FROM test\", engine)\n",
    "print(\"\\nSQLAlchemy DataFrame:\\n\", sql_df)\n",
    "\n",
    "# --- Cleanup ---\n",
    "con.close()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Database Interaction Workflow:\n",
    "   - Create connection → execute DDL → insert data → query data\n",
    "   - Use context managers (with) for automatic connection handling\n",
    "\n",
    "2. Data Retrieval Methods:\n",
    "   - Manual: cursor.fetchall() + DataFrame construction\n",
    "   - Preferred: pd.read_sql() with SQLAlchemy\n",
    "\n",
    "3. SQLAlchemy Advantages:\n",
    "   - Database-agnostic connection strings\n",
    "   - Simplifies query execution and result parsing\n",
    "   - Works with multiple DBMS (PostgreSQL, MySQL, etc.)\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use parameterized queries (security/performance)\n",
    "   - Explicitly define column names for robustness\n",
    "   - Close connections after operations\n",
    "   - Use ORMs (SQLAlchemy) for complex operations\n",
    "\n",
    "5. pandas Integration:\n",
    "   - read_sql() automatically handles:\n",
    "     - Data type conversion\n",
    "     - Column naming\n",
    "     - Connection management\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 7: Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Missing Data Representation ---\n",
    "# Create Series with NaN (numeric missing value)\n",
    "string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])\n",
    "print(\"Original Series with NaN:\\n\", string_data)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nisnull() detection:\\n\", string_data.isnull())\n",
    "\n",
    "# Replace with None (object missing value)\n",
    "string_data[0] = None\n",
    "print(\"\\nAfter replacing with None:\\n\", string_data)\n",
    "print(\"isnull() now shows:\\n\", string_data.isnull())\n",
    "\n",
    "# --- Handling Missing Data ---\n",
    "# Fill missing values\n",
    "filled = string_data.fillna(\"missing\")\n",
    "print(\"\\nAfter fillna('missing'):\\n\", filled)\n",
    "\n",
    "# Drop missing values\n",
    "cleaned = string_data.dropna()\n",
    "print(\"\\nAfter dropna():\\n\", cleaned)\n",
    "\n",
    "# --- DataFrame Example ---\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan],\n",
    "    'B': ['X', np.nan, 'Z'],\n",
    "    'C': [np.nan, np.nan, np.nan]\n",
    "})\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\\n\", df)\n",
    "print(\"DataFrame isnull():\\n\", df.isnull())\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "print(\"\\nAfter filling column A with mean:\\n\", df)\n",
    "\n",
    "# Drop columns with all NaN\n",
    "df_clean = df.dropna(axis=1, how='all')\n",
    "print(\"\\nAfter dropping all-NaN columns:\\n\", df_clean)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Missing Data Representation:\n",
    "   - NaN (np.nan) for numeric missing values\n",
    "   - None treated as NA in object types\n",
    "   - Both detected by isnull()/notnull()\n",
    "\n",
    "2. Core Methods:\n",
    "   - isnull(): Identify missing values\n",
    "   - notnull(): Inverse of isnull()\n",
    "   - dropna(): Filter missing data\n",
    "      - how: 'any' (default) or 'all'\n",
    "      - thresh: Minimum non-NA values\n",
    "   - fillna(): Replace missing values\n",
    "      - Value, method (ffill/bfill), or interpolation\n",
    "\n",
    "3. Important Notes:\n",
    "   - Aggregations (mean, sum) automatically exclude NaN\n",
    "   - Object columns preserve None/np.nan distinction\n",
    "   - Always verify after fill/drop operations\n",
    "   - Use inplace=True for direct modification\n",
    "\n",
    "4. Best Practices:\n",
    "   - Analyze missing data patterns\n",
    "   - Choose appropriate filling strategy (mean, median, etc.)\n",
    "   - Consider domain knowledge for imputation\n",
    "   - Document handling decisions for reproducibility\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Filtering Missing Data ---\n",
    "# Create sample Series with NaN\n",
    "data_series = pd.Series([1, np.nan, 3.5, np.nan, 7])\n",
    "print(\"Original Series:\\n\", data_series)\n",
    "\n",
    "# Drop NaN values from Series\n",
    "print(\"\\nAfter dropna():\\n\", data_series.dropna())\n",
    "print(\"\\nUsing boolean indexing:\\n\", data_series[data_series.notnull()])\n",
    "\n",
    "# Create sample DataFrame with NaN\n",
    "data_df = pd.DataFrame([\n",
    "    [1.0, 6.5, 3.0],\n",
    "    [1.0, np.nan, np.nan],\n",
    "    [np.nan, np.nan, np.nan],\n",
    "    [np.nan, 6.5, 3.0]\n",
    "])\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\\n\", data_df)\n",
    "\n",
    "# Drop rows with any NaN\n",
    "print(\"\\nDrop rows with any NaN:\\n\", data_df.dropna())\n",
    "\n",
    "# Drop rows where ALL values are NaN\n",
    "print(\"\\nDrop rows where all NaN:\\n\", data_df.dropna(how='all'))\n",
    "\n",
    "# Add column of NaN and drop columns where all NaN\n",
    "data_df[3] = np.nan\n",
    "print(\"\\nDataFrame with new NaN column:\\n\", data_df)\n",
    "print(\"\\nDrop columns where all NaN:\\n\", data_df.dropna(axis=1, how='all'))\n",
    "\n",
    "# Keep rows with at least 2 non-null values\n",
    "df_random = pd.DataFrame(np.random.randn(7, 3))\n",
    "df_random.iloc[:4, 1] = np.nan\n",
    "df_random.iloc[:2, 2] = np.nan\n",
    "print(\"\\nRandom DataFrame with NaN:\\n\", df_random)\n",
    "print(\"\\nDrop rows with <2 non-null values:\\n\", df_random.dropna(thresh=2))\n",
    "\n",
    "# --- Filling Missing Data ---\n",
    "# Fill NaN with constant\n",
    "print(\"\\nFill NaN with 0:\\n\", df_random.fillna(0))\n",
    "\n",
    "# Fill different values per column\n",
    "print(\"\\nFill with column-specific values:\\n\", df_random.fillna({1: 0.5, 2: -1}))\n",
    "\n",
    "# Forward fill with limit\n",
    "df_ffill = pd.DataFrame({\n",
    "    0: [1, np.nan, np.nan, 4, np.nan],\n",
    "    1: [np.nan, 2, np.nan, 5, 6]\n",
    "})\n",
    "print(\"\\nOriginal for ffill:\\n\", df_ffill)\n",
    "print(\"\\nForward fill (limit=1):\\n\", df_ffill.fillna(method='ffill', limit=1))\n",
    "\n",
    "# Fill with mean value\n",
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n",
    "print(\"\\nSeries filled with mean:\\n\", data.fillna(data.mean()))\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Filtering Options:\n",
    "   - dropna() removes missing values\n",
    "   - how='any' (default) vs how='all'\n",
    "   - thresh parameter sets minimum non-null counts\n",
    "   - axis parameter controls row/column filtering\n",
    "\n",
    "2. Filling Strategies:\n",
    "   - Scalar values or column-specific mappings\n",
    "   - method='ffill'/'bfill' for propagation\n",
    "   - limit parameter controls fill continuity\n",
    "   - Statistical fills (mean/median) for numeric data\n",
    "\n",
    "3. Performance Considerations:\n",
    "   - inplace=True modifies original data\n",
    "   - Interpolation methods (linear, time) for time series\n",
    "   - Prefer fillna() over manual loops for efficiency\n",
    "\n",
    "4. Best Practices:\n",
    "   - Analyze missing data patterns first\n",
    "   - Document imputation strategies\n",
    "   - Validate results after operations\n",
    "   - Consider domain knowledge for appropriate fills\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Removing Duplicates ---\n",
    "data = pd.DataFrame({\n",
    "    'k1': ['one', 'two'] * 3 + ['two'],\n",
    "    'k2': [1, 1, 2, 3, 3, 4, 4]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame with duplicates:\")\n",
    "print(data)\n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nDuplicate indicators:\\n\", data.duplicated())\n",
    "\n",
    "# Drop duplicates\n",
    "print(\"\\nAfter dropping duplicates:\\n\", data.drop_duplicates())\n",
    "\n",
    "# Drop duplicates based on subset\n",
    "data['v1'] = range(7)\n",
    "print(\"\\nDataFrame with new column:\\n\", data)\n",
    "print(\"\\nDrop duplicates by 'k1':\\n\", data.drop_duplicates(['k1']))\n",
    "\n",
    "# Keep last occurrence\n",
    "print(\"\\nKeep last duplicates:\\n\", data.drop_duplicates(['k1', 'k2'], keep='last'))\n",
    "\n",
    "# --- Mapping Transformations ---\n",
    "meat_data = pd.DataFrame({\n",
    "    'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', \n",
    "             'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],\n",
    "    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]\n",
    "})\n",
    "\n",
    "meat_to_animal = {\n",
    "    'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow',\n",
    "    'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'\n",
    "}\n",
    "\n",
    "# Normalize and map\n",
    "meat_data['animal'] = meat_data['food'].str.lower().map(meat_to_animal)\n",
    "print(\"\\nMeat data with animal mapping:\\n\", meat_data)\n",
    "\n",
    "# Alternative using lambda\n",
    "meat_data['animal_lambda'] = meat_data['food'].map(lambda x: meat_to_animal[x.lower()])\n",
    "print(\"\\nUsing lambda mapping:\\n\", meat_data[['food', 'animal_lambda']])\n",
    "\n",
    "# --- Replacing Values ---\n",
    "data_series = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "print(\"\\nOriginal Series:\\n\", data_series)\n",
    "\n",
    "# Replace single value\n",
    "print(\"\\nReplace -999 with NaN:\\n\", data_series.replace(-999, np.nan))\n",
    "\n",
    "# Replace multiple values\n",
    "print(\"\\nReplace multiple values:\\n\", data_series.replace([-999, -1000], np.nan))\n",
    "print(\"\\nDifferent replacements:\\n\", data_series.replace({-999: np.nan, -1000: 0}))\n",
    "\n",
    "# --- Renaming Axis Indexes ---\n",
    "df = pd.DataFrame(np.arange(12).reshape((3,4)),\n",
    "                  index=['Ohio', 'Colorado', 'New York'],\n",
    "                  columns=['one', 'two', 'three', 'four'])\n",
    "\n",
    "# Transform index\n",
    "print(\"\\nOriginal DataFrame:\\n\", df)\n",
    "df.index = df.index.map(lambda x: x[:4].upper())\n",
    "print(\"\\nModified index:\\n\", df)\n",
    "\n",
    "# Rename columns/indices\n",
    "renamed_df = df.rename(index={'OHIO': 'INDIANA'}, columns={'three': 'peekaboo'})\n",
    "print(\"\\nRenamed DataFrame:\\n\", renamed_df)\n",
    "\n",
    "# In-place rename\n",
    "df.rename(index={'COLO': 'TEXAS'}, inplace=True)\n",
    "print(\"\\nIn-place rename:\\n\", df)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Duplicate Handling:\n",
    "   - duplicated() identifies row duplicates\n",
    "   - drop_duplicates() removes duplicates\n",
    "   - subset parameter targets specific columns\n",
    "   - keep='last' retains last occurrence\n",
    "\n",
    "2. Data Mapping:\n",
    "   - map() applies element-wise transformations\n",
    "   - Use str.lower() for case normalization\n",
    "   - Dictionary mappings handle value replacements\n",
    "   - Lambda functions provide inline transformation logic\n",
    "\n",
    "3. Value Replacement:\n",
    "   - replace() handles multiple value substitutions\n",
    "   - List input for multiple -> single replacement\n",
    "   - Dictionary input for targeted replacements\n",
    "   - Differs from str.replace() (string pattern substitution)\n",
    "\n",
    "4. Renaming Operations:\n",
    "   - map() modifies index/column labels\n",
    "   - rename() creates transformed versions\n",
    "   - Works with functions or dictionaries\n",
    "   - inplace=True modifies original object\n",
    "\n",
    "5. Best Practices:\n",
    "   - Normalize data before mapping\n",
    "   - Verify replacements with dry runs\n",
    "   - Use descriptive names when renaming\n",
    "   - Prefer vectorized operations over loops\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python and Pandas String Manipulation Guide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================\n",
    "# 1. Python String Methods\n",
    "# =============================================\n",
    "sample_str = '  a,b, guido  '\n",
    "print(\"Original string:\", repr(sample_str))\n",
    "\n",
    "# Basic splitting and stripping\n",
    "split_result = sample_str.split(',')\n",
    "stripped_pieces = [x.strip() for x in split_result]\n",
    "joined_str = '::'.join(stripped_pieces)\n",
    "\n",
    "print(\"\\n1. Splitting and joining:\")\n",
    "print(\"Split result:\", split_result)\n",
    "print(\"Stripped pieces:\", stripped_pieces)\n",
    "print(\"Joined string:\", joined_str)\n",
    "\n",
    "# Search and replace methods\n",
    "print(\"\\n2. Search and replace:\")\n",
    "print(\"Contains 'guido':\", 'guido' in sample_str)\n",
    "print(\"Find comma position:\", sample_str.find(','))\n",
    "print(\"Count commas:\", sample_str.count(','))\n",
    "print(\"Replace commas:\", sample_str.replace(',', '|'))\n",
    "\n",
    "# Case conversion and validation\n",
    "print(\"\\n3. Case methods:\")\n",
    "print(\"Uppercase:\", sample_str.upper())\n",
    "print(\"Starts with 'a':\", sample_str.strip().startswith('a'))\n",
    "print(\"Ends with 'do':\", sample_str.strip().endswith('do'))\n",
    "\n",
    "# =============================================\n",
    "# 2. Pandas String Operations\n",
    "# =============================================\n",
    "data = pd.Series(['  alice ', 'bob  ', 'carol', None, '  dave'])\n",
    "print(\"\\nPandas Series before cleaning:\\n\", data)\n",
    "\n",
    "# Pandas string methods with na handling\n",
    "print(\"\\n4. Pandas string operations:\")\n",
    "print(\"Stripped whitespace:\\n\", data.str.strip())\n",
    "print(\"Uppercase:\\n\", data.str.upper())\n",
    "print(\"Contains 'o':\\n\", data.str.contains('o', na=False))\n",
    "print(\"First character:\\n\", data.str[0])\n",
    "print(\"Lengths:\\n\", data.str.len())\n",
    "\n",
    "# Split into DataFrame\n",
    "split_df = data.str.split('a', expand=True)\n",
    "print(\"\\nSplit on 'a':\\n\", split_df)\n",
    "\n",
    "# =============================================\n",
    "# 3. Handling Missing Data\n",
    "# =============================================\n",
    "print(\"\\n5. Handling missing values:\")\n",
    "print(\"Default NaN handling:\\n\", data.str.upper())\n",
    "print(\"With fillna:\\n\", data.str.upper().fillna('MISSING'))\n",
    "\n",
    "# Safe operations with na_action\n",
    "print(\"\\nSafe contains check:\\n\", \n",
    "      data.str.contains('o', na_action='ignore'))\n",
    "\n",
    "# =============================================\n",
    "# 4. Regular Expressions\n",
    "# =============================================\n",
    "emails = pd.Series([\n",
    "    'john.doe@example.com',\n",
    "    'invalid_email',\n",
    "    'sarah.smith@company.org',\n",
    "    np.nan\n",
    "])\n",
    "\n",
    "print(\"\\n6. Regular expression examples:\")\n",
    "# Extract domains where valid\n",
    "domains = emails.str.extract(r'@([\\w.]+)', expand=False)\n",
    "print(\"Extracted domains:\\n\", domains)\n",
    "\n",
    "# Validate email format\n",
    "valid_emails = emails.str.match(r'^[\\w.]+@[\\w]+\\.[\\w]{2,3}$')\n",
    "print(\"Valid emails:\\n\", valid_emails)\n",
    "\n",
    "# =============================================\n",
    "# 5. Advanced DataFrame Operations\n",
    "# =============================================\n",
    "df = pd.DataFrame({\n",
    "    'name': [' Alice ', 'Bob ', 'Charlie', None],\n",
    "    'address': ['123 Main St', '456 Oak Ave', '789 Pine Rd', np.nan]\n",
    "})\n",
    "\n",
    "print(\"\\n7. DataFrame string operations:\")\n",
    "# Clean entire DataFrame\n",
    "df_clean = df.apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "df_clean['name'] = df_clean['name'].str.upper()\n",
    "print(\"Cleaned DataFrame:\\n\", df_clean)\n",
    "\n",
    "# =============================================\n",
    "# Key Takeaways\n",
    "# =============================================\n",
    "\"\"\"\n",
    "STRING MANIPULATION ESSENTIALS:\n",
    "\n",
    "1. Core Python Methods:\n",
    "   - split/join/strip for basic cleaning\n",
    "   - find/index for substring search\n",
    "   - replace for pattern substitution\n",
    "   - Case methods: lower/upper/casefold\n",
    "\n",
    "2. Pandas Enhancements:\n",
    "   - .str accessor for vectorized operations\n",
    "   - na_action parameter for missing values\n",
    "   - Regular expression integration\n",
    "   - DataFrame-wide operations\n",
    "\n",
    "3. Regular Expressions:\n",
    "   - match() for pattern validation\n",
    "   - extract() for pattern capture\n",
    "   - replace() with regex substitution\n",
    "\n",
    "4. Best Practices:\n",
    "   - Always consider whitespace cleaning\n",
    "   - Handle missing values explicitly\n",
    "   - Use vectorized pandas methods instead of loops\n",
    "   - Precompile regex patterns for repeated use\n",
    "\n",
    "5. Common Pitfalls:\n",
    "   - Forgetting to strip whitespace\n",
    "   - Case sensitivity in searches\n",
    "   - NaN handling in pandas operations\n",
    "   - Overlooking regex special characters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Discretization and Binning ---\n",
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "\n",
    "# Using pd.cut to bin continuous data\n",
    "cats = pd.cut(ages, bins)\n",
    "print(\"\\nCategorical Bins:\\n\", cats)\n",
    "print(\"\\nBin Codes:\\n\", cats.codes)\n",
    "print(\"\\nBin Categories:\\n\", cats.categories)\n",
    "print(\"\\nBin Counts:\\n\", pd.value_counts(cats))\n",
    "\n",
    "# Customizing bin labels\n",
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\n",
    "labeled_cats = pd.cut(ages, bins, labels=group_names)\n",
    "print(\"\\nLabeled Bins:\\n\", labeled_cats)\n",
    "\n",
    "# Using pd.qcut for quantile-based binning\n",
    "data = np.random.randn(1000)\n",
    "quartiles = pd.qcut(data, 4)\n",
    "print(\"\\nQuartile Bins:\\n\", quartiles)\n",
    "print(\"\\nQuartile Counts:\\n\", pd.value_counts(quartiles))\n",
    "\n",
    "# --- Detecting and Filtering Outliers ---\n",
    "data_df = pd.DataFrame(np.random.randn(1000, 4))\n",
    "outliers = data_df[(np.abs(data_df) > 3).any(axis=1)]\n",
    "print(\"\\nOutliers:\\n\", outliers)\n",
    "\n",
    "# Capping values outside the range [-3, 3]\n",
    "data_df[np.abs(data_df) > 3] = np.sign(data_df) * 3\n",
    "print(\"\\nCapped Data Summary:\\n\", data_df.describe())\n",
    "\n",
    "# --- Permutation and Random Sampling ---\n",
    "df = pd.DataFrame(np.arange(20).reshape(5, 4))\n",
    "sampler = np.random.permutation(5)\n",
    "permuted_df = df.take(sampler)\n",
    "print(\"\\nPermuted DataFrame:\\n\", permuted_df)\n",
    "\n",
    "# Random sampling without replacement\n",
    "sampled_df = df.sample(n=3)\n",
    "print(\"\\nSampled DataFrame (No Replacement):\\n\", sampled_df)\n",
    "\n",
    "# Random sampling with replacement\n",
    "choices = pd.Series([5, 7, -1, 6, 4])\n",
    "draws = choices.sample(n=10, replace=True)\n",
    "print(\"\\nRandom Draws (With Replacement):\\n\", draws)\n",
    "\n",
    "# --- Computing Indicator/Dummy Variables ---\n",
    "df_key = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "dummy_vars = pd.get_dummies(df_key['key'], prefix='key')\n",
    "df_with_dummy = df_key[['data1']].join(dummy_vars)\n",
    "print(\"\\nDataFrame with Dummy Variables:\\n\", df_with_dummy)\n",
    "\n",
    "# Multi-category indicator variables\n",
    "movies_data = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Movie A', 'Movie B', 'Movie C'],\n",
    "    'genres': ['Action|Drama', 'Comedy|Romance', 'Action|Thriller']\n",
    "})\n",
    "all_genres = set('|'.join(movies_data['genres']).split('|'))\n",
    "zero_matrix = np.zeros((len(movies_data), len(all_genres)))\n",
    "dummies = pd.DataFrame(zero_matrix, columns=sorted(all_genres))\n",
    "\n",
    "for i, genre_list in enumerate(movies_data['genres']):\n",
    "    indices = dummies.columns.get_indexer(genre_list.split('|'))\n",
    "    dummies.iloc[i, indices] = 1\n",
    "\n",
    "movies_with_indicators = movies_data.join(dummies.add_prefix('Genre_'))\n",
    "print(\"\\nMovies with Genre Indicators:\\n\", movies_with_indicators)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Discretization:\n",
    "   - pd.cut divides continuous data into bins.\n",
    "   - pd.qcut creates quantile-based bins for equal-sized groups.\n",
    "   - Custom labels can be applied using the `labels` parameter.\n",
    "\n",
    "2. Outlier Handling:\n",
    "   - Use boolean indexing to detect outliers.\n",
    "   - Cap values outside a range using np.sign and array operations.\n",
    "\n",
    "3. Random Sampling:\n",
    "   - np.random.permutation generates random orderings.\n",
    "   - df.sample supports sampling with or without replacement.\n",
    "\n",
    "4. Dummy Variables:\n",
    "   - pd.get_dummies converts categorical variables into binary indicators.\n",
    "   - For multi-category data, construct a zero matrix and populate it based on category membership.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use vectorized operations for efficiency.\n",
    "   - Combine discretization with other transformations (e.g., dummy variables) for feature engineering.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 8: Data Wrangling: Join, Combine, and Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Hierarchical Indexing ---\n",
    "# Creating a Series with a MultiIndex\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]])\n",
    "print(\"\\nSeries with Hierarchical Index:\\n\", data)\n",
    "\n",
    "# Accessing subsets using partial indexing\n",
    "print(\"\\nSubset for 'a':\\n\", data['a'])\n",
    "print(\"\\nSubset for 'b':\\n\", data['b'])\n",
    "\n",
    "# Unstacking and stacking\n",
    "unstacked = data.unstack()\n",
    "print(\"\\nUnstacked DataFrame:\\n\", unstacked)\n",
    "restacked = unstacked.stack()\n",
    "print(\"\\nRestacked Series:\\n\", restacked)\n",
    "\n",
    "# --- Hierarchical Index in DataFrame ---\n",
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n",
    "                     index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                     columns=[['Ohio', 'Ohio', 'Colorado'],\n",
    "                              ['Green', 'Red', 'Green']])\n",
    "print(\"\\nDataFrame with Hierarchical Index:\\n\", frame)\n",
    "\n",
    "# Sorting by hierarchical index\n",
    "frame_sorted = frame.sort_index(level=0)\n",
    "print(\"\\nLexicographically Sorted DataFrame:\\n\", frame_sorted)\n",
    "\n",
    "# Summary statistics by level\n",
    "level_summary = frame.sum(level=0, axis=0)\n",
    "print(\"\\nSummary Statistics by Level (Rows):\\n\", level_summary)\n",
    "\n",
    "# --- Merging DataFrames ---\n",
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n",
    "                    'data1': range(7)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'],\n",
    "                    'data2': range(3)})\n",
    "\n",
    "# Many-to-one merge\n",
    "merged_df = pd.merge(df1, df2, on='key')\n",
    "print(\"\\nMerged DataFrame (Many-to-One):\\n\", merged_df)\n",
    "\n",
    "# Merge with different column names\n",
    "df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n",
    "                    'data1': range(7)})\n",
    "df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],\n",
    "                    'data2': range(3)})\n",
    "merged_diff_keys = pd.merge(df3, df4, left_on='lkey', right_on='rkey')\n",
    "print(\"\\nMerged DataFrame (Different Keys):\\n\", merged_diff_keys)\n",
    "\n",
    "# --- Setting and Resetting Index ---\n",
    "frame2 = pd.DataFrame({'a': range(7), 'b': range(7, 0, -1),\n",
    "                       'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'],\n",
    "                       'd': [0, 1, 2, 0, 1, 2, 3]})\n",
    "frame_with_index = frame2.set_index(['c', 'd'])\n",
    "print(\"\\nDataFrame with Hierarchical Index (set_index):\\n\", frame_with_index)\n",
    "\n",
    "# Resetting the index\n",
    "reset_frame = frame_with_index.reset_index()\n",
    "print(\"\\nDataFrame After Resetting Index:\\n\", reset_frame)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Hierarchical Indexing:\n",
    "   - Enables multi-level indexing for both rows and columns.\n",
    "   - Supports partial indexing and slicing.\n",
    "   - `unstack()` and `stack()` convert between hierarchical indices and flat structures.\n",
    "\n",
    "2. Sorting and Aggregation:\n",
    "   - Sorting by hierarchical levels improves performance.\n",
    "   - Aggregation functions like `sum()` can operate on specific levels of the index.\n",
    "\n",
    "3. Merging DataFrames:\n",
    "   - `pd.merge()` combines DataFrames based on common keys.\n",
    "   - Handles many-to-one and one-to-many relationships.\n",
    "   - Allows merging on differently named columns using `left_on` and `right_on`.\n",
    "\n",
    "4. Setting and Resetting Index:\n",
    "   - `set_index()` creates hierarchical indices from existing columns.\n",
    "   - `reset_index()` flattens hierarchical indices back into columns.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use hierarchical indexing for multidimensional data representation.\n",
    "   - Sort indices before performing operations for better performance.\n",
    "   - Specify merge keys explicitly to avoid ambiguity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Merging on Index ---\n",
    "# Example DataFrames for merging on index\n",
    "left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)})\n",
    "right1 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\n",
    "\n",
    "print(\"\\nLeft DataFrame (left1):\\n\", left1)\n",
    "print(\"\\nRight DataFrame (right1):\\n\", right1)\n",
    "\n",
    "# Merge using the index of the right DataFrame as the merge key\n",
    "merged_index = pd.merge(left1, right1, left_on='key', right_index=True)\n",
    "print(\"\\nMerge on Index (Inner Join):\\n\", merged_index)\n",
    "\n",
    "# Outer join to include all keys\n",
    "merged_outer = pd.merge(left1, right1, left_on='key', right_index=True, how='outer')\n",
    "print(\"\\nMerge on Index (Outer Join):\\n\", merged_outer)\n",
    "\n",
    "# --- Hierarchical Index Merge ---\n",
    "lefth = pd.DataFrame({\n",
    "    'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "    'key2': [2000, 2001, 2002, 2001, 2002],\n",
    "    'data': np.arange(5.)\n",
    "})\n",
    "righth = pd.DataFrame(\n",
    "    np.arange(12).reshape((6, 2)),\n",
    "    index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]],\n",
    "    columns=['event1', 'event2']\n",
    ")\n",
    "\n",
    "print(\"\\nHierarchical Left DataFrame (lefth):\\n\", lefth)\n",
    "print(\"\\nHierarchical Right DataFrame (righth):\\n\", righth)\n",
    "\n",
    "# Merge on multiple keys with hierarchical index\n",
    "merged_hierarchical = pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)\n",
    "print(\"\\nMerge on Multiple Keys (Inner Join):\\n\", merged_hierarchical)\n",
    "\n",
    "# Outer join for hierarchical index\n",
    "merged_hierarchical_outer = pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer')\n",
    "print(\"\\nMerge on Multiple Keys (Outer Join):\\n\", merged_hierarchical_outer)\n",
    "\n",
    "# --- Joining DataFrames by Index ---\n",
    "left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])\n",
    "right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\n",
    "\n",
    "print(\"\\nLeft DataFrame (left2):\\n\", left2)\n",
    "print(\"\\nRight DataFrame (right2):\\n\", right2)\n",
    "\n",
    "# Join using index\n",
    "joined_index = left2.join(right2, how='outer')\n",
    "print(\"\\nJoin on Index (Outer Join):\\n\", joined_index)\n",
    "\n",
    "# Join on a column from one DataFrame and the index of another\n",
    "joined_on_key = left1.join(right1, on='key')\n",
    "print(\"\\nJoin Using Column and Index:\\n\", joined_on_key)\n",
    "\n",
    "# --- Joining Multiple DataFrames ---\n",
    "another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])\n",
    "print(\"\\nAnother DataFrame (another):\\n\", another)\n",
    "\n",
    "# Join multiple DataFrames\n",
    "joined_multiple = left2.join([right2, another])\n",
    "print(\"\\nJoin Multiple DataFrames (Inner Join):\\n\", joined_multiple)\n",
    "\n",
    "# Outer join for multiple DataFrames\n",
    "joined_multiple_outer = left2.join([right2, another], how='outer')\n",
    "print(\"\\nJoin Multiple DataFrames (Outer Join):\\n\", joined_multiple_outer)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Merging on Index:\n",
    "   - Use `left_index=True` or `right_index=True` to merge on index.\n",
    "   - Combine hierarchical indices with multiple keys for complex merges.\n",
    "\n",
    "2. Join Method:\n",
    "   - `DataFrame.join()` is a convenient way to merge by index.\n",
    "   - Supports joining multiple DataFrames with overlapping or non-overlapping indices.\n",
    "\n",
    "3. Handling Missing Data:\n",
    "   - Outer joins include all keys, filling missing values with NaN.\n",
    "   - Useful for combining datasets with partial overlaps.\n",
    "\n",
    "4. Combining Multiple DataFrames:\n",
    "   - Pass a list of DataFrames to `join()` for merging multiple objects.\n",
    "   - Specify `how='outer'` to include all rows from all DataFrames.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use `merge()` for general-purpose joins with flexibility.\n",
    "   - Use `join()` for simpler index-based merges.\n",
    "   - Handle duplicate indices carefully when performing outer joins.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Concatenating Along an Axis ---\n",
    "\n",
    "# Example NumPy array concatenation\n",
    "arr = np.arange(12).reshape((3, 4))\n",
    "print(\"\\nOriginal NumPy Array:\\n\", arr)\n",
    "concatenated_arr = np.concatenate([arr, arr], axis=1)\n",
    "print(\"\\nConcatenated NumPy Array (axis=1):\\n\", concatenated_arr)\n",
    "\n",
    "# --- Concatenating pandas Series ---\n",
    "s1 = pd.Series([0, 1], index=['a', 'b'])\n",
    "s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])\n",
    "s3 = pd.Series([5, 6], index=['f', 'g'])\n",
    "\n",
    "# Concatenate Series along rows (default axis=0)\n",
    "concat_series_rows = pd.concat([s1, s2, s3])\n",
    "print(\"\\nConcatenated Series (axis=0):\\n\", concat_series_rows)\n",
    "\n",
    "# Concatenate Series along columns (axis=1)\n",
    "concat_series_cols = pd.concat([s1, s2, s3], axis=1)\n",
    "print(\"\\nConcatenated Series (axis=1):\\n\", concat_series_cols)\n",
    "\n",
    "# Concatenate with inner join\n",
    "concat_inner_join = pd.concat([s1, s3], axis=1, join='inner')\n",
    "print(\"\\nConcatenated Series with Inner Join:\\n\", concat_inner_join)\n",
    "\n",
    "# Concatenate with specific join_axes\n",
    "concat_join_axes = pd.concat([s1, s3], axis=1, join_axes=[['a', 'c', 'b', 'e']])\n",
    "print(\"\\nConcatenated Series with join_axes:\\n\", concat_join_axes)\n",
    "\n",
    "# Concatenate with hierarchical index using keys\n",
    "concat_with_keys = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])\n",
    "print(\"\\nConcatenated Series with Hierarchical Index:\\n\", concat_with_keys)\n",
    "\n",
    "# Unstack hierarchical index\n",
    "unstacked_result = concat_with_keys.unstack()\n",
    "print(\"\\nUnstacked Result:\\n\", unstacked_result)\n",
    "\n",
    "# Concatenate Series with keys for column headers\n",
    "concat_series_keys_cols = pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])\n",
    "print(\"\\nConcatenated Series with Keys as Column Headers:\\n\", concat_series_keys_cols)\n",
    "\n",
    "# --- Concatenating pandas DataFrames ---\n",
    "df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])\n",
    "df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'], columns=['three', 'four'])\n",
    "\n",
    "# Concatenate DataFrames along columns (axis=1) with keys\n",
    "concat_df_cols_keys = pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])\n",
    "print(\"\\nConcatenated DataFrames with Hierarchical Columns:\\n\", concat_df_cols_keys)\n",
    "\n",
    "# Concatenate DataFrames using a dictionary for keys\n",
    "concat_dict_keys = pd.concat({'level1': df1, 'level2': df2}, axis=1)\n",
    "print(\"\\nConcatenated DataFrames Using Dictionary Keys:\\n\", concat_dict_keys)\n",
    "\n",
    "# Concatenate DataFrames with named hierarchical levels\n",
    "concat_named_levels = pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'])\n",
    "print(\"\\nConcatenated DataFrames with Named Hierarchical Levels:\\n\", concat_named_levels)\n",
    "\n",
    "# Concatenate DataFrames ignoring indexes\n",
    "df3 = pd.DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])\n",
    "df4 = pd.DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])\n",
    "concat_ignore_index = pd.concat([df3, df4], ignore_index=True)\n",
    "print(\"\\nConcatenated DataFrames with Ignored Index:\\n\", concat_ignore_index)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Concatenation Basics:\n",
    "   - Use `pd.concat()` to combine pandas objects along an axis.\n",
    "   - Default behavior concatenates along rows (`axis=0`).\n",
    "\n",
    "2. Handling Non-Overlapping Indices:\n",
    "   - By default, concatenation uses the union of indices (`outer join`).\n",
    "   - Use `join='inner'` to retain only overlapping indices.\n",
    "\n",
    "3. Hierarchical Indexing:\n",
    "   - Use the `keys` argument to create a hierarchical index on the concatenation axis.\n",
    "   - Specify `names` to name hierarchical levels.\n",
    "\n",
    "4. DataFrame Concatenation:\n",
    "   - When concatenating DataFrames along columns, `keys` become column headers.\n",
    "   - Use `ignore_index=True` to reset the index when row indices are irrelevant.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use `join_axes` for precise control over non-concatenation axes.\n",
    "   - Use hierarchical indexing for structured concatenation results.\n",
    "   - Reset indices when combining DataFrames with meaningless row indices.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Combining Data with Overlap ---\n",
    "# Example Series with overlapping indices\n",
    "a = pd.Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])\n",
    "b = pd.Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])\n",
    "b[-1] = np.nan\n",
    "\n",
    "print(\"\\nSeries a:\\n\", a)\n",
    "print(\"\\nSeries b:\\n\", b)\n",
    "\n",
    "# Combine using NumPy's where function\n",
    "combined_np = np.where(pd.isnull(a), b, a)\n",
    "print(\"\\nCombined Using NumPy's where:\\n\", combined_np)\n",
    "\n",
    "# Combine using combine_first\n",
    "combined_series = b[:-2].combine_first(a[2:])\n",
    "print(\"\\nCombined Using combine_first (Series):\\n\", combined_series)\n",
    "\n",
    "# Example DataFrames with overlapping data\n",
    "df1 = pd.DataFrame({'a': [1., np.nan, 5., np.nan],\n",
    "                    'b': [np.nan, 2., np.nan, 6.],\n",
    "                    'c': range(2, 18, 4)})\n",
    "df2 = pd.DataFrame({'a': [5., 4., np.nan, 3., 7.],\n",
    "                    'b': [np.nan, 3., 4., 6., 8.]})\n",
    "\n",
    "print(\"\\nDataFrame df1:\\n\", df1)\n",
    "print(\"\\nDataFrame df2:\\n\", df2)\n",
    "\n",
    "combined_df = df1.combine_first(df2)\n",
    "print(\"\\nCombined Using combine_first (DataFrame):\\n\", combined_df)\n",
    "\n",
    "# --- Reshaping with Hierarchical Indexing ---\n",
    "# Reshaping DataFrames using stack and unstack\n",
    "data = pd.DataFrame(np.arange(6).reshape((2, 3)),\n",
    "                    index=pd.Index(['Ohio', 'Colorado'], name='state'),\n",
    "                    columns=pd.Index(['one', 'two', 'three'], name='number'))\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\\n\", data)\n",
    "\n",
    "# Stack converts columns into rows\n",
    "stacked = data.stack()\n",
    "print(\"\\nStacked Series:\\n\", stacked)\n",
    "\n",
    "# Unstack converts rows back into columns\n",
    "unstacked = stacked.unstack()\n",
    "print(\"\\nUnstacked DataFrame:\\n\", unstacked)\n",
    "\n",
    "# Unstacking a specific level\n",
    "unstacked_level_0 = stacked.unstack(0)\n",
    "print(\"\\nUnstacked Level 0:\\n\", unstacked_level_0)\n",
    "\n",
    "unstacked_state = stacked.unstack('state')\n",
    "print(\"\\nUnstacked by State:\\n\", unstacked_state)\n",
    "\n",
    "# Handling missing data during stacking and unstacking\n",
    "s1 = pd.Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])\n",
    "s2 = pd.Series([4, 5, 6], index=['c', 'd', 'e'])\n",
    "data2 = pd.concat([s1, s2], keys=['one', 'two'])\n",
    "\n",
    "print(\"\\nConcatenated Series:\\n\", data2)\n",
    "\n",
    "unstacked_data2 = data2.unstack()\n",
    "print(\"\\nUnstacked Concatenated Series:\\n\", unstacked_data2)\n",
    "\n",
    "stacked_data2 = unstacked_data2.stack()\n",
    "print(\"\\nStacked Data (Default Drop Missing):\\n\", stacked_data2)\n",
    "\n",
    "stacked_data2_no_drop = unstacked_data2.stack(dropna=False)\n",
    "print(\"\\nStacked Data (Keep Missing):\\n\", stacked_data2_no_drop)\n",
    "\n",
    "# --- Reshaping MultiIndexed DataFrames ---\n",
    "df = pd.DataFrame({'left': stacked, 'right': stacked + 5},\n",
    "                  columns=pd.Index(['left', 'right'], name='side'))\n",
    "print(\"\\nMultiIndexed DataFrame:\\n\", df)\n",
    "\n",
    "# Unstacking a specific level\n",
    "unstacked_df = df.unstack('state')\n",
    "print(\"\\nUnstacked by State:\\n\", unstacked_df)\n",
    "\n",
    "# Stacking a specific axis\n",
    "stacked_df = unstacked_df.stack('side')\n",
    "print(\"\\nStacked by Side:\\n\", stacked_df)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Combining Data with Overlap:\n",
    "   - Use `combine_first` to patch missing data in one object with values from another.\n",
    "   - Aligns data based on indices for both Series and DataFrames.\n",
    "\n",
    "2. Reshaping with Hierarchical Indexing:\n",
    "   - `stack()` pivots columns into rows, creating a hierarchical index.\n",
    "   - `unstack()` pivots rows into columns, optionally specifying the level to unstack.\n",
    "   - Handles missing data gracefully during stacking and unstacking.\n",
    "\n",
    "3. MultiIndexed DataFrames:\n",
    "   - Can reshape along specific axes or levels.\n",
    "   - Provides a flexible way to organize and transform multidimensional data.\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use `combine_first` for filling missing data in overlapping datasets.\n",
    "   - Leverage `stack` and `unstack` for reshaping hierarchical data.\n",
    "   - Handle missing data explicitly when reshaping to avoid unintended results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Pivoting \"Long\" to \"Wide\" Format ---\n",
    "# Load example data\n",
    "data = pd.read_csv('examples/macrodata.csv')\n",
    "\n",
    "# Create a PeriodIndex for date and select specific columns\n",
    "periods = pd.PeriodIndex(year=data['year'], quarter=data['quarter'], name='date')\n",
    "columns = pd.Index(['realgdp', 'infl', 'unemp'], name='item')\n",
    "data = data.reindex(columns=columns)\n",
    "data.index = periods.to_timestamp('D', 'end')\n",
    "\n",
    "# Stack the data into long format and reset index\n",
    "ldata = data.stack().reset_index().rename(columns={0: 'value'})\n",
    "\n",
    "print(\"\\nLong Format Data (ldata):\\n\", ldata[:10])\n",
    "\n",
    "# Pivot the long format data into wide format\n",
    "pivoted = ldata.pivot('date', 'item', 'value')\n",
    "print(\"\\nPivoted Wide Format Data:\\n\", pivoted[:5])\n",
    "\n",
    "# Add an additional value column and pivot again\n",
    "ldata['value2'] = np.random.randn(len(ldata))\n",
    "pivoted_with_value2 = ldata.pivot('date', 'item')\n",
    "print(\"\\nPivoted with Hierarchical Columns:\\n\", pivoted_with_value2[:5])\n",
    "\n",
    "# Alternative method using set_index and unstack\n",
    "unstacked = ldata.set_index(['date', 'item']).unstack('item')\n",
    "print(\"\\nUnstacked Data (Alternative Method):\\n\", unstacked[:5])\n",
    "\n",
    "# --- Pivoting \"Wide\" to \"Long\" Format ---\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'key': ['foo', 'bar', 'baz'],\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "print(\"\\nOriginal Wide Format DataFrame:\\n\", df)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "melted = pd.melt(df, id_vars=['key'])\n",
    "print(\"\\nMelted Long Format DataFrame:\\n\", melted)\n",
    "\n",
    "# Pivot back to wide format\n",
    "reshaped = melted.pivot('key', 'variable', 'value')\n",
    "print(\"\\nReshaped Back to Wide Format:\\n\", reshaped)\n",
    "\n",
    "# Reset index to move the row labels back into a column\n",
    "reshaped_reset = reshaped.reset_index()\n",
    "print(\"\\nReshaped with Index Reset:\\n\", reshaped_reset)\n",
    "\n",
    "# Melt with a subset of value columns\n",
    "melted_subset = pd.melt(df, id_vars=['key'], value_vars=['A', 'B'])\n",
    "print(\"\\nMelted with Subset of Value Columns:\\n\", melted_subset)\n",
    "\n",
    "# Melt without group identifiers\n",
    "melted_no_id = pd.melt(df, value_vars=['A', 'B', 'C'])\n",
    "print(\"\\nMelted Without Group Identifiers:\\n\", melted_no_id)\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Pivoting \"Long\" to \"Wide\":\n",
    "   - Use `pivot()` to reshape long-format data into wide format.\n",
    "   - Specify row and column indices, and optionally a value column to fill the DataFrame.\n",
    "   - For multiple value columns, hierarchical columns are created.\n",
    "\n",
    "2. Pivoting \"Wide\" to \"Long\":\n",
    "   - Use `pd.melt()` to transform wide-format data into long format.\n",
    "   - Specify group indicators (`id_vars`) and value columns (`value_vars`).\n",
    "   - Melted data can be reshaped back to wide format using `pivot()`.\n",
    "\n",
    "3. Alternative Methods:\n",
    "   - `set_index()` followed by `unstack()` achieves the same result as `pivot()`.\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use `pivot()` for clean transformations between long and wide formats.\n",
    "   - Use `pd.melt()` for combining multiple columns into a single column for analysis.\n",
    "   - Reset the index after pivoting if the original row labels need to be preserved as a column.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 9: Plotting and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Simple Line Plot ---\n",
    "# Create data for plotting\n",
    "data = np.arange(10)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(data)\n",
    "plt.title(\"Simple Line Plot\")\n",
    "plt.show()\n",
    "\n",
    "# --- Figures and Subplots ---\n",
    "# Create a new figure\n",
    "fig = plt.figure(figsize=(8, 6))  # Set figure size\n",
    "\n",
    "# Add subplots to the figure (2x2 grid)\n",
    "ax1 = fig.add_subplot(2, 2, 1)  # First subplot\n",
    "ax2 = fig.add_subplot(2, 2, 2)  # Second subplot\n",
    "ax3 = fig.add_subplot(2, 2, 3)  # Third subplot\n",
    "\n",
    "# Plot on the first subplot (histogram)\n",
    "ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)\n",
    "ax1.set_title(\"Histogram\")\n",
    "\n",
    "# Plot on the second subplot (scatter plot)\n",
    "ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))\n",
    "ax2.set_title(\"Scatter Plot\")\n",
    "\n",
    "# Plot on the third subplot (line plot with random cumulative sum)\n",
    "ax3.plot(np.random.randn(50).cumsum(), 'k--')\n",
    "ax3.set_title(\"Line Plot with Cumulative Sum\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Using plt.subplots for Grid of Subplots ---\n",
    "# Create a figure and a grid of subplots (2x3)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the 2D array of axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Example: Plot on each subplot\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(np.random.randn(50).cumsum())\n",
    "    ax.set_title(f\"Plot {i+1}\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Creating Figures and Subplots:\n",
    "   - Use `plt.figure()` to create a new figure.\n",
    "   - Use `add_subplot()` to add individual subplots to the figure.\n",
    "\n",
    "2. Plotting Types:\n",
    "   - Line plots (`plot`): For visualizing trends or relationships.\n",
    "   - Histograms (`hist`): For visualizing distributions.\n",
    "   - Scatter plots (`scatter`): For visualizing correlations between variables.\n",
    "\n",
    "3. Convenience with `plt.subplots`:\n",
    "   - Use `plt.subplots(nrows, ncols)` to create a grid of subplots.\n",
    "   - The returned `axes` array allows easy indexing and customization.\n",
    "\n",
    "4. Sharing Axes:\n",
    "   - Use `sharex=True` and `sharey=True` to synchronize x-axis or y-axis across subplots.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use `plt.tight_layout()` to automatically adjust subplot spacing.\n",
    "   - Customize plots with titles, labels, and styles for better readability.\n",
    "   - Combine multiple plots in a single cell in Jupyter notebooks for consistent visualization.\n",
    "\n",
    "6. Additional Resources:\n",
    "   - Refer to the matplotlib gallery and documentation for advanced features and plot types.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Adjusting Spacing Around Subplots ---\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(8, 6))\n",
    "\n",
    "# Plot histograms on each subplot\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.suptitle(\"Subplots with No Spacing\")\n",
    "plt.show()\n",
    "\n",
    "# --- Colors, Markers, and Line Styles ---\n",
    "# Generate random data\n",
    "data = np.random.randn(30).cumsum()\n",
    "\n",
    "# Plot with green dashed line and markers\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(data, 'g--', label='Green Dashed Line')\n",
    "plt.plot(data, color='k', linestyle='dashed', marker='o', label='Explicit Style')\n",
    "plt.plot(data, 'k-', drawstyle='steps-post', label='Steps-Post')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Line Plots with Different Styles\")\n",
    "plt.show()\n",
    "\n",
    "# --- Ticks, Labels, and Legends ---\n",
    "# Create a simple plot\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, label='Sine Wave')\n",
    "\n",
    "# Adjust ticks and labels using pyplot interface\n",
    "plt.xlim(0, 10)  # Set x-axis limits\n",
    "plt.xticks([0, 2.5, 5, 7.5, 10])  # Set custom tick locations\n",
    "plt.xlabel(\"X-Axis\")  # Add x-axis label\n",
    "plt.ylabel(\"Y-Axis\")  # Add y-axis label\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Alternatively, use object-oriented API\n",
    "ax = plt.gca()  # Get current AxesSubplot\n",
    "ax.set_xlim(0, 10)  # Set x-axis limits\n",
    "ax.set_xticks([0, 2.5, 5, 7.5, 10])  # Set custom tick locations\n",
    "ax.set_xlabel(\"X-Axis (Object-Oriented)\")  # Add x-axis label\n",
    "ax.set_ylabel(\"Y-Axis (Object-Oriented)\")  # Add y-axis label\n",
    "\n",
    "plt.title(\"Ticks, Labels, and Legends\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Adjusting Spacing Around Subplots:\n",
    "   - Use `plt.subplots_adjust()` to control spacing between subplots.\n",
    "   - Parameters like `wspace` and `hspace` adjust horizontal and vertical spacing.\n",
    "\n",
    "2. Colors, Markers, and Line Styles:\n",
    "   - Specify colors, markers, and line styles using shorthand strings (e.g., 'g--') or explicitly (e.g., `color='g', linestyle='--'`).\n",
    "   - Use `drawstyle` to change how points are connected (e.g., `steps-post`).\n",
    "\n",
    "3. Ticks, Labels, and Legends:\n",
    "   - Use `plt.xlim()`, `plt.xticks()`, and similar methods to control axis limits and ticks.\n",
    "   - Add labels and legends using `plt.xlabel()`, `plt.ylabel()`, and `plt.legend()`.\n",
    "   - The object-oriented API provides more explicit control over subplot properties.\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use `subplots_adjust` to prevent overlapping elements when working with multiple subplots.\n",
    "   - Prefer explicit styling for programmatic plots to ensure clarity and maintainability.\n",
    "   - Use legends to identify different plot components, especially when comparing multiple datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Random Walk Plot ---\n",
    "# Create a figure and subplot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Generate random walk data\n",
    "random_walk = np.random.randn(1000).cumsum()\n",
    "ax.plot(random_walk)\n",
    "\n",
    "# Customize x-axis ticks and labels\n",
    "ticks = ax.set_xticks([0, 250, 500, 750, 1000])\n",
    "labels = ax.set_xticklabels(['One', 'Two', 'Three', 'Four', 'Five'],\n",
    "                            rotation=30, fontsize='small')\n",
    "\n",
    "# Set title and x-axis label\n",
    "ax.set_title('My First Matplotlib Plot')\n",
    "ax.set_xlabel('Stages')\n",
    "\n",
    "# Alternatively, batch set properties using a dictionary\n",
    "props = {\n",
    "    'title': 'My First Matplotlib Plot',\n",
    "    'xlabel': 'Stages'\n",
    "}\n",
    "ax.set(**props)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Adding Legends ---\n",
    "# Create another figure and subplot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Plot multiple lines with labels\n",
    "ax.plot(np.random.randn(1000).cumsum(), 'k', label='Line One')\n",
    "ax.plot(np.random.randn(1000).cumsum(), 'k--', label='Line Two')\n",
    "ax.plot(np.random.randn(1000).cumsum(), 'k.', label='Line Three')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Set title and axis labels\n",
    "ax.set_title('Plot with Legends')\n",
    "ax.set_xlabel('X-Axis')\n",
    "ax.set_ylabel('Y-Axis')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Setting Titles, Axis Labels, Ticks, and Tick Labels:\n",
    "   - Use `set_title()` to add a title to the plot.\n",
    "   - Use `set_xlabel()` and `set_ylabel()` to label the axes.\n",
    "   - Use `set_xticks()` and `set_xticklabels()` to customize tick locations and labels.\n",
    "   - Use `rotation` and `fontsize` in `set_xticklabels()` to adjust tick label appearance.\n",
    "\n",
    "2. Batch Setting Properties:\n",
    "   - Use a dictionary with `set()` to apply multiple properties at once.\n",
    "\n",
    "3. Adding Legends:\n",
    "   - Pass the `label` argument when plotting to assign labels to plot elements.\n",
    "   - Use `ax.legend()` or `plt.legend()` to display the legend.\n",
    "   - The `loc` parameter controls the legend's position; `'best'` automatically chooses a non-overlapping location.\n",
    "\n",
    "4. Best Practices:\n",
    "   - Use `plt.tight_layout()` to prevent overlapping elements in the layout.\n",
    "   - Customize tick labels for better readability, especially with long labels.\n",
    "   - Include legends to identify different plot components, especially when comparing multiple datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Annotations and Drawing on a Subplot ---\n",
    "# Load S&P 500 data\n",
    "data = pd.read_csv('examples/spx.csv', index_col=0, parse_dates=True)\n",
    "spx = data['SPX']\n",
    "\n",
    "# Create a figure and subplot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Plot the S&P 500 closing prices\n",
    "spx.plot(ax=ax, style='k-', label='S&P 500')\n",
    "\n",
    "# Define important dates for annotation\n",
    "crisis_data = [\n",
    "    (datetime(2007, 10, 11), 'Peak of bull market'),\n",
    "    (datetime(2008, 3, 12), 'Bear Stearns Fails'),\n",
    "    (datetime(2008, 9, 15), 'Lehman Bankruptcy')\n",
    "]\n",
    "\n",
    "# Add annotations with arrows\n",
    "for date, label in crisis_data:\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        xy=(date, spx.asof(date) + 75),  # Arrow point\n",
    "        xytext=(date, spx.asof(date) + 225),  # Text location\n",
    "        arrowprops=dict(facecolor='black', headwidth=4, width=2, headlength=4),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top'\n",
    "    )\n",
    "\n",
    "# Set plot limits and title\n",
    "ax.set_xlim(['1/1/2007', '1/1/2011'])\n",
    "ax.set_ylim([600, 1800])\n",
    "ax.set_title('Important Dates in the 2008-2009 Financial Crisis')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Drawing Shapes on a Subplot ---\n",
    "# Create a new figure and subplot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Define shapes: Rectangle, Circle, and Polygon\n",
    "rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\n",
    "circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\n",
    "pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)\n",
    "\n",
    "# Add shapes to the subplot\n",
    "ax.add_patch(rect)\n",
    "ax.add_patch(circ)\n",
    "ax.add_patch(pgon)\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Data Visualization with Shapes')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Saving Plots to File ---\n",
    "# Save the first plot to a file\n",
    "plt.figure(figsize=(10, 6))\n",
    "spx.plot(style='k-', label='S&P 500')\n",
    "plt.title('S&P 500 Closing Prices')\n",
    "plt.legend()\n",
    "\n",
    "# Save as SVG\n",
    "plt.savefig('spx_plot.svg')\n",
    "\n",
    "# Save as PNG with high DPI and tight bounding box\n",
    "plt.savefig('spx_plot.png', dpi=400, bbox_inches='tight')\n",
    "\n",
    "# Save to a BytesIO buffer\n",
    "from io import BytesIO\n",
    "buffer = BytesIO()\n",
    "plt.savefig(buffer, format='png')\n",
    "plot_data = buffer.getvalue()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Annotations and Drawing:\n",
    "   - Use `ax.annotate()` to add text and arrows to highlight specific points on a plot.\n",
    "   - Customize annotations with properties like `xy`, `xytext`, and `arrowprops`.\n",
    "   - Use `ax.add_patch()` to draw shapes like rectangles, circles, and polygons.\n",
    "\n",
    "2. Saving Plots:\n",
    "   - Use `plt.savefig()` to save plots to files in various formats (e.g., PNG, SVG, PDF).\n",
    "   - Specify options like `dpi` for resolution and `bbox_inches='tight'` to trim whitespace.\n",
    "   - Save plots to memory using a `BytesIO` buffer for further processing.\n",
    "\n",
    "3. Best Practices:\n",
    "   - Use `set_xlim` and `set_ylim` to manually control plot boundaries.\n",
    "   - Combine annotations and shapes to create informative and visually appealing visualizations.\n",
    "   - Save plots in high-resolution formats for publication-quality graphics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Matplotlib Configuration ---\n",
    "# Customize global parameters using plt.rc\n",
    "plt.rc('figure', figsize=(10, 6))  # Set default figure size\n",
    "font_options = {\n",
    "    'family': 'monospace',\n",
    "    'weight': 'bold',\n",
    "    'size': 'small'\n",
    "}\n",
    "plt.rc('font', **font_options)  # Customize font settings\n",
    "\n",
    "# --- Line Plots with pandas ---\n",
    "# Create a Series and plot it\n",
    "s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))\n",
    "plt.figure(figsize=(8, 4))\n",
    "s.plot(label='Random Walk', style='ko-', alpha=0.7)\n",
    "plt.title(\"Simple Series Plot\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame and plot multiple lines\n",
    "df = pd.DataFrame(\n",
    "    np.random.randn(10, 4).cumsum(axis=0),\n",
    "    columns=['A', 'B', 'C', 'D'],\n",
    "    index=np.arange(0, 100, 10)\n",
    ")\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.plot(style=['r--', 'g-.', 'b:', 'k-'], alpha=0.7)\n",
    "plt.title(\"Simple DataFrame Plot\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend(title=\"Legend\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Using Seaborn for Enhanced Aesthetics ---\n",
    "# Import seaborn to modify default styles\n",
    "sns.set_style(\"whitegrid\")  # Use a built-in seaborn style\n",
    "\n",
    "# Plot the same DataFrame with seaborn aesthetics\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.plot(style=['r--', 'g-.', 'b:', 'k-'], alpha=0.7)\n",
    "plt.title(\"DataFrame Plot with Seaborn Aesthetics\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend(title=\"Legend\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Matplotlib Configuration:\n",
    "   - Use `plt.rc` to customize global parameters like figure size, fonts, and grid styles.\n",
    "   - Modify the matplotlibrc file for persistent configuration changes.\n",
    "\n",
    "2. Line Plots with pandas:\n",
    "   - `Series.plot()` creates line plots by default, using the index for the x-axis.\n",
    "   - `DataFrame.plot()` plots each column as a separate line on the same subplot, with an automatic legend.\n",
    "   - Additional keyword arguments (e.g., `style`, `alpha`, `xticks`) allow further customization.\n",
    "\n",
    "3. Seaborn for Enhanced Aesthetics:\n",
    "   - Importing seaborn modifies default matplotlib styles for better readability and visual appeal.\n",
    "   - Use `sns.set_style()` to apply built-in themes like \"whitegrid\" or \"darkgrid.\"\n",
    "\n",
    "4. Best Practices:\n",
    "   - Customize plots programmatically using matplotlib's API or pandas' built-in methods.\n",
    "   - Use seaborn for quick aesthetic improvements without extensive manual configuration.\n",
    "   - Leverage pandas' `subplots=True` option for plotting DataFrame columns in separate subplots when needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Bar Plots with pandas ---\n",
    "# Create a Series for bar plots\n",
    "data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))\n",
    "\n",
    "# Plot vertical and horizontal bar plots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
    "data.plot.bar(ax=axes[0], color='k', alpha=0.7)\n",
    "data.plot.barh(ax=axes[1], color='k', alpha=0.7)\n",
    "axes[0].set_title(\"Vertical Bar Plot\")\n",
    "axes[1].set_title(\"Horizontal Bar Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for grouped bar plots\n",
    "df = pd.DataFrame(\n",
    "    np.random.rand(6, 4),\n",
    "    index=['one', 'two', 'three', 'four', 'five', 'six'],\n",
    "    columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus')\n",
    ")\n",
    "\n",
    "# Plot grouped bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "df.plot.bar(alpha=0.7)\n",
    "plt.title(\"DataFrame Grouped Bar Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot stacked bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "df.plot.barh(stacked=True, alpha=0.5)\n",
    "plt.title(\"DataFrame Stacked Bar Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Visualizing Value Frequencies ---\n",
    "# Example: Frequency of values in a Series\n",
    "s = pd.Series(np.random.choice(['a', 'b', 'c'], size=100))\n",
    "s.value_counts().plot.bar(color='k', alpha=0.7)\n",
    "plt.title(\"Frequency of Values in Series\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Cross-Tabulation and Normalized Bar Plots ---\n",
    "# Load tipping dataset\n",
    "tips = pd.read_csv('examples/tips.csv')\n",
    "\n",
    "# Create a cross-tabulation of day vs. party size\n",
    "party_counts = pd.crosstab(tips['day'], tips['size'])\n",
    "\n",
    "# Focus on party sizes between 2 and 5\n",
    "party_counts = party_counts.loc[:, 2:5]\n",
    "\n",
    "# Normalize each row to sum to 1\n",
    "party_pcts = party_counts.div(party_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot normalized bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "party_pcts.plot.bar()\n",
    "plt.title(\"Fraction of Parties by Size on Each Day\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Bar Plots with seaborn ---\n",
    "# Add tip percentage column\n",
    "tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])\n",
    "\n",
    "# Bar plot of tip percentage by day\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='tip_pct', y='day', data=tips, orient='h')\n",
    "plt.title(\"Tipping Percentage by Day with Error Bars\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot with hue for additional categorical variable (time)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='tip_pct', y='day', hue='time', data=tips, orient='h')\n",
    "plt.title(\"Tipping Percentage by Day and Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Customize seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Bar Plots with pandas:\n",
    "   - Use `plot.bar()` and `plot.barh()` for vertical and horizontal bar plots.\n",
    "   - For DataFrames, bars are grouped by default; use `stacked=True` for stacked bars.\n",
    "\n",
    "2. Visualizing Value Frequencies:\n",
    "   - Use `value_counts()` to compute frequencies and visualize them with `plot.bar()`.\n",
    "\n",
    "3. Cross-Tabulation and Normalization:\n",
    "   - Use `pd.crosstab()` to create cross-tabulations.\n",
    "   - Normalize rows using `div()` and `.sum()` to compare proportions across categories.\n",
    "\n",
    "4. Bar Plots with seaborn:\n",
    "   - Seaborn simplifies plotting aggregated data with `sns.barplot()`.\n",
    "   - Use the `hue` parameter to split bars by an additional categorical variable.\n",
    "   - Seaborn provides enhanced aesthetics and confidence intervals by default.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use normalization when comparing proportions across categories.\n",
    "   - Leverage seaborn for quick, aesthetically pleasing visualizations with minimal code.\n",
    "   - Customize styles using `sns.set()` for consistent appearance across plots.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Histograms and Density Plots ---\n",
    "# Load tipping dataset\n",
    "tips = pd.read_csv('examples/tips.csv')\n",
    "\n",
    "# Add tip percentage column\n",
    "tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])\n",
    "\n",
    "# Plot histogram of tip percentages\n",
    "plt.figure(figsize=(8, 4))\n",
    "tips['tip_pct'].plot.hist(bins=50, color='k', alpha=0.7)\n",
    "plt.title(\"Histogram of Tip Percentages\")\n",
    "plt.xlabel(\"Tip Percentage\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot density plot of tip percentages\n",
    "plt.figure(figsize=(8, 4))\n",
    "tips['tip_pct'].plot.density(color='k')\n",
    "plt.title(\"Density Plot of Tip Percentages\")\n",
    "plt.xlabel(\"Tip Percentage\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use seaborn's distplot for combined histogram and density plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.distplot(tips['tip_pct'], bins=50, color='k')\n",
    "plt.title(\"Histogram and Density Plot of Tip Percentages\")\n",
    "plt.xlabel(\"Tip Percentage\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Scatter Plots ---\n",
    "# Load macrodata dataset\n",
    "macro = pd.read_csv('examples/macrodata.csv')\n",
    "data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]\n",
    "trans_data = np.log(data).diff().dropna()\n",
    "\n",
    "# Scatter plot with regression line\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.regplot('m1', 'unemp', data=trans_data, scatter_kws={'alpha': 0.5})\n",
    "plt.title(\"Scatter Plot: Changes in log(m1) vs log(unemp)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Pair Plots ---\n",
    "# Create pair plot matrix\n",
    "sns.pairplot(trans_data, diag_kind='kde', plot_kws={'alpha': 0.2})\n",
    "plt.suptitle(\"Pair Plot Matrix of Statsmodels Macro Data\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Facet Grids and Categorical Data ---\n",
    "# Bar plot with facet grid by smoker and time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.factorplot(x='day', y='tip_pct', hue='time', col='smoker',\n",
    "               kind='bar', data=tips[tips.tip_pct < 1])\n",
    "plt.suptitle(\"Tipping Percentage by Day/Time/Smoker\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Expand facet grid by adding rows for time\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.factorplot(x='day', y='tip_pct', row='time', col='smoker',\n",
    "               kind='bar', data=tips[tips.tip_pct < 1])\n",
    "plt.suptitle(\"Tipping Percentage by Day; Facet by Time/Smoker\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot of tip_pct by day\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.factorplot(x='tip_pct', y='day', kind='box',\n",
    "               data=tips[tips.tip_pct < 0.5])\n",
    "plt.title(\"Box Plot of Tip Percentage by Day\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. Histograms and Density Plots:\n",
    "   - Use `plot.hist()` to create histograms for visualizing value frequency.\n",
    "   - Use `plot.density()` or `sns.kdeplot()` for kernel density estimates (KDE).\n",
    "   - Combine histograms and KDEs using `sns.distplot()`.\n",
    "\n",
    "2. Scatter Plots:\n",
    "   - Use `sns.regplot()` to create scatter plots with linear regression lines.\n",
    "   - Scatter plots are useful for examining relationships between two variables.\n",
    "\n",
    "3. Pair Plots:\n",
    "   - Use `sns.pairplot()` to visualize pairwise relationships in a dataset.\n",
    "   - Diagonal elements can display histograms or KDEs for individual variables.\n",
    "\n",
    "4. Facet Grids and Categorical Data:\n",
    "   - Use `sns.factorplot()` to create faceted plots for categorical data.\n",
    "   - Facet grids allow splitting data by additional grouping dimensions.\n",
    "   - Box plots are effective for summarizing distributions with medians, quartiles, and outliers.\n",
    "\n",
    "5. Best Practices:\n",
    "   - Use transparency (`alpha`) to improve readability in dense plots.\n",
    "   - Customize titles, labels, and legends for better interpretability.\n",
    "   - Leverage seaborn's built-in functions for quick and aesthetically pleasing visualizations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 10 - Data Aggregation and Group Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'key1': ['a', 'a', 'b', 'b', 'a'],\n",
    "    'key2': ['one', 'two', 'one', 'two', 'one'],\n",
    "    'data1': np.random.randn(5),\n",
    "    'data2': np.random.randn(5)\n",
    "})\n",
    "\n",
    "# --- Grouping and Aggregation ---\n",
    "# Group by a single column and compute the mean\n",
    "grouped_single = df['data1'].groupby(df['key1'])\n",
    "print(\"Grouped by 'key1' and computed mean:\\n\", grouped_single.mean())\n",
    "\n",
    "# Group by multiple columns and compute the mean\n",
    "grouped_multiple = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "print(\"\\nGrouped by 'key1' and 'key2' and computed mean:\\n\", grouped_multiple)\n",
    "\n",
    "# Unstack the hierarchical index\n",
    "print(\"\\nUnstacked result:\\n\", grouped_multiple.unstack())\n",
    "\n",
    "# Group using external arrays\n",
    "states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\n",
    "years = np.array([2005, 2005, 2006, 2005, 2006])\n",
    "grouped_external = df['data1'].groupby([states, years]).mean()\n",
    "print(\"\\nGrouped by external arrays (states and years):\\n\", grouped_external)\n",
    "\n",
    "# --- Aggregating Multiple Columns ---\n",
    "# Group by a single column and aggregate all numeric columns\n",
    "grouped_all_numeric = df.groupby('key1').mean()\n",
    "print(\"\\nGrouped by 'key1' and aggregated all numeric columns:\\n\", grouped_all_numeric)\n",
    "\n",
    "# Group by multiple columns and aggregate all numeric columns\n",
    "grouped_multi_numeric = df.groupby(['key1', 'key2']).mean()\n",
    "print(\"\\nGrouped by 'key1' and 'key2' and aggregated all numeric columns:\\n\", grouped_multi_numeric)\n",
    "\n",
    "# --- Group Sizes ---\n",
    "# Get group sizes\n",
    "group_sizes = df.groupby(['key1', 'key2']).size()\n",
    "print(\"\\nGroup sizes:\\n\", group_sizes)\n",
    "\n",
    "# --- Iterating Over Groups ---\n",
    "# Iterate over groups\n",
    "print(\"\\nIterating over groups:\")\n",
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(group)\n",
    "\n",
    "# Iterate over groups with multiple keys\n",
    "print(\"\\nIterating over groups with multiple keys:\")\n",
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print((k1, k2))\n",
    "    print(group)\n",
    "\n",
    "# Create a dictionary of groups\n",
    "pieces = dict(list(df.groupby('key1')))\n",
    "print(\"\\nDictionary of groups:\\n\", pieces)\n",
    "\n",
    "# --- Grouping on Other Axes ---\n",
    "# Group columns by dtype\n",
    "grouped_by_dtype = df.groupby(df.dtypes, axis=1)\n",
    "print(\"\\nGrouped columns by dtype:\")\n",
    "for dtype, group in grouped_by_dtype:\n",
    "    print(dtype)\n",
    "    print(group)\n",
    "\n",
    "# --- Selecting Columns for Aggregation ---\n",
    "# Select a single column for aggregation\n",
    "grouped_data2 = df.groupby(['key1', 'key2'])['data2'].mean()\n",
    "print(\"\\nMean of 'data2' grouped by 'key1' and 'key2':\\n\", grouped_data2)\n",
    "\n",
    "# Select multiple columns for aggregation\n",
    "grouped_subset = df.groupby(['key1', 'key2'])[['data2']].mean()\n",
    "print(\"\\nMean of 'data2' as a DataFrame grouped by 'key1' and 'key2':\\n\", grouped_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================================\n",
    "# 1. Create Sample DataFrames\n",
    "# =============================================\n",
    "\n",
    "# Create main DataFrame with random data\n",
    "people = pd.DataFrame(\n",
    "    np.random.randn(8, 6),\n",
    "    columns=['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis', 'Alice', 'Bob', 'Carol']\n",
    ")\n",
    "\n",
    "# Introduce some NaN values\n",
    "people.iloc[2:4, [1, 2]] = np.nan\n",
    "people.iloc[5, 3:] = np.nan\n",
    "\n",
    "# Create tipping dataset example\n",
    "data = {\n",
    "    'total_bill': [16.99, 10.34, 21.01, 23.68, 24.59, 25.29],\n",
    "    'tip': [1.01, 1.66, 3.50, 3.31, 3.61, 4.71],\n",
    "    'sex': ['Female', 'Male', 'Male', 'Male', 'Female', 'Female'],\n",
    "    'smoker': ['No', 'Yes', 'No', 'No', 'No', 'Yes'],\n",
    "    'day': ['Sun', 'Sun', 'Sun', 'Sun', 'Sun', 'Sun'],\n",
    "    'time': ['Dinner', 'Dinner', 'Dinner', 'Dinner', 'Dinner', 'Dinner'],\n",
    "    'size': [2, 3, 3, 2, 4, 4]\n",
    "}\n",
    "tips = pd.DataFrame(data)\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']\n",
    "\n",
    "# Create MultiIndex DataFrame\n",
    "arrays = [\n",
    "    ['US', 'US', 'US', 'JP', 'JP', 'UK'],\n",
    "    [1, 3, 5, 1, 3, 2]\n",
    "]\n",
    "columns = pd.MultiIndex.from_arrays(arrays, names=['cty', 'tenor'])\n",
    "hier_df = pd.DataFrame(np.random.randn(4, 6), columns=columns)\n",
    "\n",
    "# =============================================\n",
    "# 2. Basic Grouping Operations\n",
    "# =============================================\n",
    "\n",
    "# Group columns using dictionary\n",
    "mapping = {\n",
    "    'a': 'red', 'b': 'red', 'c': 'blue',\n",
    "    'd': 'blue', 'e': 'red', 'f': 'green'\n",
    "}\n",
    "by_column = people.groupby(mapping, axis=1)\n",
    "\n",
    "# Group rows by index length\n",
    "by_name_length = people.groupby(len)\n",
    "\n",
    "# Group by multiple columns in tipping data\n",
    "by_day_smoker = tips.groupby(['day', 'smoker'])\n",
    "\n",
    "# =============================================\n",
    "# 3. Aggregation Examples\n",
    "# =============================================\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"Sum by color groups:\")\n",
    "print(by_column.sum())\n",
    "\n",
    "print(\"\\nMean by name length:\")\n",
    "print(by_name_length.mean())\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nMultiple aggregations by day/smoker:\")\n",
    "print(by_day_smoker['tip_pct'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "# Custom aggregation function\n",
    "def data_range(arr):\n",
    "    return arr.max() - arr.min()\n",
    "\n",
    "print(\"\\nCustom range aggregation:\")\n",
    "print(by_column.agg(data_range))\n",
    "\n",
    "# Different aggregations per column\n",
    "print(\"\\nColumn-specific aggregations:\")\n",
    "print(tips.groupby('sex').agg({\n",
    "    'total_bill': ['sum', 'mean'],\n",
    "    'tip': ['max', 'min'],\n",
    "    'tip_pct': 'std'\n",
    "}))\n",
    "\n",
    "# =============================================\n",
    "# 4. Advanced Grouping Operations\n",
    "# =============================================\n",
    "\n",
    "# Grouping with functions\n",
    "print(\"\\nGroup by first letter of index:\")\n",
    "print(people.groupby(lambda x: x[0]).mean())\n",
    "\n",
    "# Grouping with multiple keys\n",
    "keys = ['group1', 'group1', 'group1', 'group2', 'group2', 'group1', 'group2', 'group2']\n",
    "print(\"\\nGroup by name length and custom keys:\")\n",
    "print(people.groupby([len, keys]).sum())\n",
    "\n",
    "# Grouping by index level\n",
    "print(\"\\nGroup by country level:\")\n",
    "print(hier_df.groupby(level='cty', axis=1).mean())\n",
    "\n",
    "# =============================================\n",
    "# 5. Apply and Transform Examples\n",
    "# =============================================\n",
    "\n",
    "# Top N values per group\n",
    "def top_n(df, n=3, column='a'):\n",
    "    return df.sort_values(by=column, ascending=False).head(n)\n",
    "\n",
    "print(\"\\nTop 3 values per group:\")\n",
    "print(people.groupby(len).apply(top_n))\n",
    "\n",
    "# Normalize within groups\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "print(\"\\nZ-scores within groups:\")\n",
    "print(people.groupby(len).transform(zscore))\n",
    "\n",
    "# Filter groups\n",
    "def filter_func(x):\n",
    "    return x['a'].mean() > 0\n",
    "\n",
    "print(\"\\nGroups with mean of 'a' > 0:\")\n",
    "print(people.groupby(len).filter(filter_func))\n",
    "\n",
    "# =============================================\n",
    "# 6. Additional Examples\n",
    "# =============================================\n",
    "\n",
    "# Time-based grouping (if datetime index)\n",
    "dates = pd.date_range('2023-01-01', periods=8)\n",
    "people_time = people.copy()\n",
    "people_time.index = dates\n",
    "print(\"\\nMonthly resampling:\")\n",
    "print(people_time.resample('M').mean())\n",
    "\n",
    "# Expanding window operations\n",
    "print(\"\\nExpanding mean per column:\")\n",
    "print(people.expanding().mean())\n",
    "\n",
    "# Rolling window operations\n",
    "print(\"\\nRolling 3-row mean:\")\n",
    "print(people.rolling(3).mean())\n",
    "\n",
    "# =============================================\n",
    "# 7. Output All Results to CSV\n",
    "# =============================================\n",
    "\n",
    "# Save results to CSV files\n",
    "by_column.sum().to_csv('groupby_sum.csv')\n",
    "by_name_length.mean().to_csv('groupby_mean.csv')\n",
    "tips.groupby(['day', 'smoker']).mean().to_csv('tips_grouped.csv')\n",
    "\n",
    "print(\"\\nAll operations completed. Results saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  # Optional for enhanced aesthetics\n",
    "\n",
    "# --- Sample Data Preparation ---\n",
    "# Create synthetic datasets to mimic examples\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Synthetic 'tips' dataset\n",
    "tips = pd.DataFrame({\n",
    "    'total_bill': np.random.uniform(10, 50, 244),\n",
    "    'tip': np.random.uniform(1, 10, 244),\n",
    "    'smoker': np.random.choice(['Yes', 'No'], 244),\n",
    "    'day': np.random.choice(['Sun', 'Sat', 'Thur', 'Fri'], 244),\n",
    "    'time': np.random.choice(['Lunch', 'Dinner'], 244),\n",
    "    'size': np.random.randint(1, 6, 244)\n",
    "})\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']\n",
    "\n",
    "# Synthetic 'movies' dataset\n",
    "movies = pd.DataFrame({\n",
    "    'movie_id': range(1, 11),\n",
    "    'title': [f\"Movie {i}\" for i in range(1, 11)],\n",
    "    'genres': [\n",
    "        'Animation|Children|Comedy', 'Adventure|Children|Fantasy',\n",
    "        'Comedy|Romance', 'Comedy|Drama', 'Comedy',\n",
    "        'Action|Crime|Thriller', 'Comedy|Romance',\n",
    "        'Adventure|Children', 'Action', 'Action|Adventure|Thriller'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# --- GroupBy Mechanics ---\n",
    "# Group by single column and compute mean\n",
    "grouped_single = tips['tip_pct'].groupby(tips['day'])\n",
    "print(\"Grouped by 'day' (mean):\\n\", grouped_single.mean())\n",
    "\n",
    "# Group by multiple columns and compute mean\n",
    "grouped_multiple = tips.groupby(['time', 'smoker'])['tip_pct'].mean()\n",
    "print(\"\\nGrouped by 'time' and 'smoker' (mean):\\n\", grouped_multiple)\n",
    "\n",
    "# Iterate over groups\n",
    "print(\"\\nIterating over groups:\")\n",
    "for (time, smoker), group in tips.groupby(['time', 'smoker']):\n",
    "    print(f\"Time: {time}, Smoker: {smoker}\")\n",
    "    print(group.head())\n",
    "\n",
    "# --- Pivot Tables ---\n",
    "# Basic pivot table with mean aggregation\n",
    "pivot_table_basic = tips.pivot_table(\n",
    "    values='tip_pct', \n",
    "    index='day', \n",
    "    columns='smoker', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nBasic Pivot Table:\\n\", pivot_table_basic)\n",
    "\n",
    "# Pivot table with margins (partial totals)\n",
    "pivot_table_margins = tips.pivot_table(\n",
    "    values='tip_pct', \n",
    "    index='day', \n",
    "    columns='smoker', \n",
    "    aggfunc='mean', \n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nPivot Table with Margins:\\n\", pivot_table_margins)\n",
    "\n",
    "# --- Cross-Tabulation ---\n",
    "# Cross-tabulation of 'day' and 'smoker'\n",
    "cross_tab = pd.crosstab(tips['day'], tips['smoker'], margins=True)\n",
    "print(\"\\nCross-Tabulation:\\n\", cross_tab)\n",
    "\n",
    "# Cross-tabulation with multiple groupings\n",
    "cross_tab_multi = pd.crosstab([tips['time'], tips['day']], tips['smoker'], margins=True)\n",
    "print(\"\\nCross-Tabulation with Time/Day:\\n\", cross_tab_multi)\n",
    "\n",
    "# --- Data Transformation ---\n",
    "# Create indicator/dummy variables for genres\n",
    "all_genres = set('|'.join(movies['genres']).split('|'))\n",
    "dummies = pd.DataFrame(0, index=movies.index, columns=sorted(all_genres))\n",
    "for i, row in movies.iterrows():\n",
    "    genres = row['genres'].split('|')\n",
    "    dummies.loc[i, genres] = 1\n",
    "movies_with_dummies = pd.concat([movies, dummies], axis=1)\n",
    "print(\"\\nMovies with Dummy Variables:\\n\", movies_with_dummies.head())\n",
    "\n",
    "# Discretization with pd.cut\n",
    "values = np.random.randn(1000)\n",
    "cats = pd.cut(values, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "print(\"\\nDiscretized Data (pd.cut):\\n\", pd.value_counts(cats))\n",
    "\n",
    "# Detect and filter outliers\n",
    "data = pd.DataFrame(np.random.randn(1000, 4))\n",
    "outliers = data[(np.abs(data) > 3).any(axis=1)]\n",
    "print(\"\\nOutliers Detected:\\n\", outliers.head())\n",
    "\n",
    "# --- Merging Data ---\n",
    "# Many-to-one merge example\n",
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "merged = pd.merge(df1, df2, on='key')\n",
    "print(\"\\nMerged DataFrame (Many-to-One):\\n\", merged)\n",
    "\n",
    "# Merge with suffixes for overlapping columns\n",
    "df3 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data': np.random.rand(7)})\n",
    "df4 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data': np.random.rand(3)})\n",
    "merged_suffixes = pd.merge(df3, df4, on='key', suffixes=('_left', '_right'))\n",
    "print(\"\\nMerged with Suffixes:\\n\", merged_suffixes)\n",
    "\n",
    "# --- Visualization (Optional) ---\n",
    "try:\n",
    "    # Plot distributions of tip_pct\n",
    "    sns.histplot(tips['tip_pct'], kde=True, bins=30)\n",
    "    plt.title(\"Distribution of Tip Percentage\")\n",
    "    plt.show()\n",
    "\n",
    "    # Pair plot of transformed macro data\n",
    "    macro = pd.DataFrame({\n",
    "        'cpi': np.random.randn(201),\n",
    "        'm1': np.random.randn(201),\n",
    "        'tbilrate': np.random.randn(201),\n",
    "        'unemp': np.random.randn(201)\n",
    "    })\n",
    "    sns.pairplot(macro, diag_kind='kde')\n",
    "    plt.suptitle(\"Pair Plot of Economic Indicators\", y=1.02)\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Seaborn not imported; skipping visualization examples.\")\n",
    "\n",
    "\"\"\"\n",
    "Key Takeaways:\n",
    "1. **GroupBy Mechanics**:\n",
    "   - Split data using `groupby`, apply aggregations (mean, sum, etc.), and combine results.\n",
    "   - Iterate over groups for custom processing.\n",
    "   - Use hierarchical indexing for multi-level groups.\n",
    "\n",
    "2. **Pivot Tables**:\n",
    "   - `pivot_table` simplifies multi-dimensional aggregation.\n",
    "   - Use `margins=True` to include partial and grand totals.\n",
    "\n",
    "3. **Cross-Tabulation**:\n",
    "   - `pd.crosstab` computes frequency tables for categorical variables.\n",
    "   - Combine multiple columns for faceted analysis.\n",
    "\n",
    "4. **Data Transformation**:\n",
    "   - Create dummy variables for categorical data using loops and `pd.DataFrame`.\n",
    "   - Use `pd.cut`/`pd.qcut` for discretization.\n",
    "   - Detect outliers with boolean indexing and `np.abs`.\n",
    "\n",
    "5. **Merging Data**:\n",
    "   - `pd.merge` handles relational joins (many-to-one, many-to-many).\n",
    "   - Use `suffixes` to resolve column name conflicts.\n",
    "\n",
    "6. **Best Practices**:\n",
    "   - Use `aggfunc` to specify custom aggregation logic.\n",
    "   - Handle missing data explicitly during transformations.\n",
    "   - Leverage vectorized operations for efficiency.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tipping dataset\n",
    "tips = pd.read_csv('examples/tips.csv')\n",
    "\n",
    "# Add tip percentage column\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']\n",
    "\n",
    "# --- Pivot Tables ---\n",
    "# Basic pivot table: group means arranged by day and smoker\n",
    "pivot_basic = tips.pivot_table(index=['day', 'smoker'])\n",
    "print(\"Basic Pivot Table:\\n\", pivot_basic)\n",
    "\n",
    "# Pivot table with selected columns and multiple groupings\n",
    "pivot_custom = tips.pivot_table(\n",
    "    ['tip_pct', 'size'], \n",
    "    index=['time', 'day'], \n",
    "    columns='smoker'\n",
    ")\n",
    "print(\"\\nPivot Table with Selected Columns and Multiple Groupings:\\n\", pivot_custom)\n",
    "\n",
    "# Pivot table with margins (partial totals)\n",
    "pivot_margins = tips.pivot_table(\n",
    "    ['tip_pct', 'size'], \n",
    "    index=['time', 'day'], \n",
    "    columns='smoker', \n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nPivot Table with Margins:\\n\", pivot_margins)\n",
    "\n",
    "# Pivot table with a different aggregation function (e.g., count or len)\n",
    "pivot_count = tips.pivot_table(\n",
    "    'tip_pct', \n",
    "    index=['time', 'smoker'], \n",
    "    columns='day', \n",
    "    aggfunc=len, \n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nPivot Table with Count Aggregation:\\n\", pivot_count)\n",
    "\n",
    "# Pivot table with fill_value to handle missing values\n",
    "pivot_fill_value = tips.pivot_table(\n",
    "    'tip_pct', \n",
    "    index=['time', 'size', 'smoker'], \n",
    "    columns='day', \n",
    "    aggfunc='mean', \n",
    "    fill_value=0\n",
    ")\n",
    "print(\"\\nPivot Table with Fill Value:\\n\", pivot_fill_value)\n",
    "\n",
    "# --- Cross-Tabulations ---\n",
    "# Example survey data\n",
    "data = pd.DataFrame({\n",
    "    'Sample': range(1, 11),\n",
    "    'Nationality': ['USA', 'Japan', 'USA', 'Japan', 'Japan', 'Japan', 'USA', 'USA', 'Japan', 'USA'],\n",
    "    'Handedness': ['Right-handed', 'Left-handed', 'Right-handed', 'Right-handed', \n",
    "                   'Left-handed', 'Right-handed', 'Right-handed', 'Left-handed', \n",
    "                   'Right-handed', 'Right-handed']\n",
    "})\n",
    "\n",
    "# Cross-tabulation of Nationality and Handedness\n",
    "crosstab_basic = pd.crosstab(data['Nationality'], data['Handedness'], margins=True)\n",
    "print(\"\\nBasic Cross-Tabulation:\\n\", crosstab_basic)\n",
    "\n",
    "# Cross-tabulation with multiple groupings\n",
    "crosstab_multi = pd.crosstab([tips['time'], tips['day']], tips['smoker'], margins=True)\n",
    "print(\"\\nCross-Tabulation with Multiple Groupings:\\n\", crosstab_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 11 - Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🕒 **Time Series Basics with Python & pandas** 🕒\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse  # For flexible date parsing\n",
    "\n",
    "# --- 1. Working with datetime Objects ---\n",
    "# Get current date and time\n",
    "now = datetime.now()\n",
    "print(\"Current Date/Time:\", now)  # e.g., 2023-10-05 14:30:45.123456\n",
    "\n",
    "# Extract components (year, month, day)\n",
    "print(\"Year:\", now.year)\n",
    "print(\"Month:\", now.month)\n",
    "print(\"Day:\", now.day)\n",
    "\n",
    "# Calculate time difference (timedelta)\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = start_date + timedelta(days=10, hours=5)\n",
    "print(\"10 Days Later:\", end_date)  # 2023-01-11 05:00:00\n",
    "\n",
    "# --- 2. Converting Between Strings and Dates ---\n",
    "# Parse a string to datetime\n",
    "date_str = \"2023-10-05\"\n",
    "date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "print(\"Parsed Date:\", date_obj)  # 2023-10-05 00:00:00\n",
    "\n",
    "# Convert datetime to string\n",
    "formatted_date = date_obj.strftime(\"%A, %B %d, %Y\")\n",
    "print(\"Formatted Date:\", formatted_date)  # Thursday, October 05, 2023\n",
    "\n",
    "# Use dateutil.parser for convenience\n",
    "flexible_date = parse(\"6/12/2011\", dayfirst=True)  # Parses as December 6, 2011\n",
    "print(\"Parsed with dateutil:\", flexible_date)\n",
    "\n",
    "# --- 3. pandas Timestamps and Time Series ---\n",
    "# Convert strings to pandas datetime\n",
    "dates = [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", None]\n",
    "ts = pd.to_datetime(dates)\n",
    "print(\"pandas DatetimeIndex:\\n\", ts)\n",
    "print(\"Missing Date (NaT):\", ts[3])  # NaT = \"Not a Time\"\n",
    "\n",
    "# Check for missing dates\n",
    "print(\"Is NaT?\", pd.isnull(ts[3]))  # True\n",
    "\n",
    "# --- 4. Time Series Operations ---\n",
    "# Create a time series DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"date\": pd.date_range(start=\"2023-01-01\", periods=5, freq=\"D\"),\n",
    "    \"value\": [10, 20, 30, 40, 50]\n",
    "})\n",
    "df.set_index(\"date\", inplace=True)\n",
    "print(\"\\nTime Series DataFrame:\\n\", df)\n",
    "\n",
    "# Resample data (e.g., monthly average)\n",
    "monthly_avg = df.resample(\"M\").mean()\n",
    "print(\"\\nMonthly Average:\\n\", monthly_avg)\n",
    "\n",
    "# --- 5. Handling Time Zones (Optional) ---\n",
    "# Localize to a time zone\n",
    "df_utc = df.tz_localize(\"UTC\")\n",
    "print(\"\\nUTC Time Series:\\n\", df_utc)\n",
    "\n",
    "# Convert to another time zone\n",
    "df_eastern = df_utc.tz_convert(\"US/Eastern\")\n",
    "print(\"\\nUS/Eastern Time Series:\\n\", df_eastern)\n",
    "\n",
    "# --- 6. Date Ranges ---\n",
    "# Generate a range of dates\n",
    "date_range = pd.date_range(start=\"2023-01-01\", end=\"2023-01-10\", freq=\"2D\")\n",
    "print(\"\\nDate Range (every 2 days):\\n\", date_range)\n",
    "\n",
    "\"\"\"\n",
    "✨ **Key Takeaways** ✨\n",
    "1. **datetime Module**: \n",
    "   - Use `datetime.now()` for current timestamps.\n",
    "   - `timedelta` handles time differences (e.g., add days/hours).\n",
    "\n",
    "2. **String Conversion**:\n",
    "   - `strptime` parses strings to dates (with format codes).\n",
    "   - `strftime` formats dates into strings.\n",
    "   - `dateutil.parser.parse` simplifies parsing ambiguous dates.\n",
    "\n",
    "3. **pandas Time Series**:\n",
    "   - `pd.to_datetime()` converts lists/columns to datetime.\n",
    "   - `NaT` represents missing timestamp data.\n",
    "   - `resample()` aggregates time series (e.g., daily → monthly).\n",
    "\n",
    "4. **Time Zones**:\n",
    "   - Use `tz_localize()` and `tz_convert()` for time zone handling.\n",
    "\n",
    "5. **Common Pitfalls**:\n",
    "   - `dateutil.parser` may misinterpret strings like '42' as years.\n",
    "   - Always specify `dayfirst=True` for international dates (e.g., '6/12/2011' → Dec 6).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📅 **Time Series Basics in pandas** 📅\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Create a Time Series ---\n",
    "# Generate dates using pandas date_range\n",
    "dates = pd.date_range(start='2023-01-01', periods=6, freq='D')  # 6 daily dates starting Jan 1, 2023\n",
    "\n",
    "# Create a time series (Series with DatetimeIndex)\n",
    "ts = pd.Series(np.random.randn(6), index=dates)\n",
    "print(\"Time Series Example:\")\n",
    "print(ts)\n",
    "\n",
    "# --- Indexing and Selection ---\n",
    "# Access data using date strings\n",
    "print(\"\\nValue on 2023-01-03:\")\n",
    "print(ts['2023-01-03'])  # Equivalent to ts[dates[2]]\n",
    "\n",
    "# Slice by year/month (works for longer time series)\n",
    "long_ts = pd.Series(np.random.randn(365), index=pd.date_range('2023-01-01', periods=365))\n",
    "print(\"\\nJanuary 2023 Data:\")\n",
    "print(long_ts['2023-01'])\n",
    "\n",
    "# Slice between dates (even if exact dates don't exist!)\n",
    "print(\"\\nData from Jan 3 to Jan 5, 2023:\")\n",
    "print(ts['2023-01-03':'2023-01-05'])\n",
    "\n",
    "# --- Truncate Time Series ---\n",
    "# Keep data before/after a specific date\n",
    "truncated_ts = ts.truncate(before='2023-01-03', after='2023-01-05')\n",
    "print(\"\\nTruncated Time Series:\")\n",
    "print(truncated_ts)\n",
    "\n",
    "# --- Time Series in DataFrames ---\n",
    "# Create a DataFrame with daily dates\n",
    "df = pd.DataFrame({\n",
    "    'Value': np.random.rand(5),\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A']\n",
    "}, index=pd.date_range('2023-01-01', periods=5))\n",
    "\n",
    "print(\"\\nTime Series DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Slice DataFrame by year/month\n",
    "print(\"\\nJanuary 2023 DataFrame Slice:\")\n",
    "print(df.loc['2023-01'])\n",
    "\n",
    "# --- Time-Based Operations ---\n",
    "# Shift data (e.g., lag by 1 day)\n",
    "shifted_ts = ts.shift(1)\n",
    "print(\"\\nShifted Time Series (1-day lag):\")\n",
    "print(shifted_ts)\n",
    "\n",
    "# Calculate percentage change\n",
    "pct_change = ts.pct_change()\n",
    "print(\"\\nDaily Percentage Change:\")\n",
    "print(pct_change)\n",
    "\n",
    "# --- Handling Time Zones (Optional) ---\n",
    "# Convert to a time zone-aware index\n",
    "ts_utc = ts.tz_localize('UTC')\n",
    "print(\"\\nUTC Time Series:\")\n",
    "print(ts_utc)\n",
    "\n",
    "# Convert to another time zone\n",
    "ts_eastern = ts_utc.tz_convert('US/Eastern')\n",
    "print(\"\\nUS/Eastern Time Series:\")\n",
    "print(ts_eastern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📅 **Time Series with Duplicate Indices & Date Ranges** 📅\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Time Series with Duplicate Indices ---\n",
    "# Create a time series with duplicate dates\n",
    "dates = pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-02', '2023-01-03'])\n",
    "dup_ts = pd.Series([10, 20, 30, 40, 50], index=dates)\n",
    "print(\"Time Series with Duplicates:\")\n",
    "print(dup_ts)\n",
    "\n",
    "# Check if the index is unique\n",
    "print(\"\\nIs the index unique?\", dup_ts.index.is_unique)  # False\n",
    "\n",
    "# Access data (returns scalar if unique, slice if duplicated)\n",
    "print(\"\\nData for 2023-01-03 (unique):\")\n",
    "print(dup_ts['2023-01-03'])  # 50\n",
    "print(\"\\nData for 2023-01-02 (duplicated):\")\n",
    "print(dup_ts['2023-01-02'])  # Shows all entries for that date\n",
    "\n",
    "# Aggregate duplicates (e.g., mean, count)\n",
    "grouped = dup_ts.groupby(level=0)\n",
    "print(\"\\nMean of Duplicates:\")\n",
    "print(grouped.mean())\n",
    "print(\"\\nCount of Duplicates:\")\n",
    "print(grouped.count())\n",
    "\n",
    "# --- Generating Date Ranges ---\n",
    "# Daily frequency between two dates\n",
    "date_range_daily = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')\n",
    "print(\"\\nDaily Date Range:\")\n",
    "print(date_range_daily)\n",
    "\n",
    "# Business month end dates for 2023\n",
    "date_range_monthly = pd.date_range(start='2023-01-01', end='2023-12-31', freq='BM')\n",
    "print(\"\\nBusiness Month Ends:\")\n",
    "print(date_range_monthly)\n",
    "\n",
    "# Generate 5 dates starting from a specific datetime (with time)\n",
    "date_with_time = pd.date_range(start='2023-01-01 12:30:00', periods=5, freq='D')\n",
    "print(\"\\nDate Range with Time:\")\n",
    "print(date_with_time)\n",
    "\n",
    "# Normalize times to midnight\n",
    "date_normalized = pd.date_range(start='2023-01-01 12:30:00', periods=5, normalize=True)\n",
    "print(\"\\nNormalized Date Range (Midnight):\")\n",
    "print(date_normalized)\n",
    "\n",
    "# --- Shifting and Frequency Conversion ---\n",
    "# Convert irregular time series to fixed daily frequency\n",
    "irregular_dates = pd.to_datetime(['2023-01-01', '2023-01-03', '2023-01-05'])\n",
    "ts = pd.Series([1.1, 2.2, 3.3], index=irregular_dates)\n",
    "print(\"\\nOriginal Irregular Time Series:\")\n",
    "print(ts)\n",
    "\n",
    "# Resample to daily frequency (introduces NaNs for missing dates)\n",
    "resampled_ts = ts.resample('D').mean()\n",
    "print(\"\\nResampled to Daily Frequency:\")\n",
    "print(resampled_ts)\n",
    "\n",
    "# Shift data by 1 day (forward fill NaNs)\n",
    "shifted_ts = resampled_ts.shift(1, fill_value=0)\n",
    "print(\"\\nShifted Time Series (1 Day):\")\n",
    "print(shifted_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📅 **Working with Date Frequencies and Offsets** 📅\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import Day, MonthEnd, Hour, Minute\n",
    "\n",
    "# --- 1. Date Ranges with Frequencies ---\n",
    "# Daily frequency (default)\n",
    "daily_dates = pd.date_range(start='2023-01-01', periods=5, freq='D')\n",
    "print(\"Daily Dates:\\n\", daily_dates)\n",
    "\n",
    "# 4-hourly frequency\n",
    "hourly_dates = pd.date_range(start='2023-01-01', periods=5, freq='4H')\n",
    "print(\"\\n4-Hourly Dates:\\n\", hourly_dates)\n",
    "\n",
    "# Custom: Third Friday of each month (WOM-3FRI)\n",
    "custom_dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='WOM-3FRI')\n",
    "print(\"\\nThird Fridays of Each Month:\\n\", custom_dates)\n",
    "\n",
    "# --- 2. Shifting Data ---\n",
    "# Create a monthly time series\n",
    "ts = pd.Series(np.random.rand(4), index=pd.date_range('2023-01-01', periods=4, freq='M'))\n",
    "print(\"\\nOriginal Time Series:\\n\", ts)\n",
    "\n",
    "# Shift data by 1 period (introduces NaNs)\n",
    "shifted = ts.shift(1)\n",
    "print(\"\\nShifted by 1 Period:\\n\", shifted)\n",
    "\n",
    "# Shift dates by 1 month (preserves frequency)\n",
    "shifted_dates = ts.shift(1, freq='M')\n",
    "print(\"\\nShifted Dates by 1 Month:\\n\", shifted_dates)\n",
    "\n",
    "# --- 3. Custom Frequencies ---\n",
    "# Combine Hour and Minute offsets\n",
    "custom_freq = Hour(2) + Minute(30)  # Equivalent to '2h30min'\n",
    "print(\"\\nCustom Frequency (2h30min):\\n\", custom_freq)\n",
    "\n",
    "# Generate dates with 90-minute frequency\n",
    "date_range_90min = pd.date_range('2023-01-01', periods=5, freq='90T')\n",
    "print(\"\\n90-Minute Date Range:\\n\", date_range_90min)\n",
    "\n",
    "# --- 4. Anchored Offsets ---\n",
    "# Roll to end of month\n",
    "now = datetime(2023, 11, 17)\n",
    "end_of_month = now + MonthEnd()\n",
    "print(\"\\nRolled to End of Month:\\n\", end_of_month)\n",
    "\n",
    "# Roll back to start of month\n",
    "start_of_month = now - MonthEnd()\n",
    "print(\"\\nRolled Back to Start of Month:\\n\", start_of_month)\n",
    "\n",
    "# --- 5. Resampling vs. GroupBy ---\n",
    "# Create irregular time series\n",
    "ts = pd.Series(np.random.randn(20), index=pd.date_range('2023-01-15', periods=20, freq='4D'))\n",
    "print(\"\\nOriginal Irregular Time Series:\\n\", ts.head())\n",
    "\n",
    "# Group by rolled dates (old method)\n",
    "grouped = ts.groupby(MonthEnd().rollforward).mean()\n",
    "print(\"\\nGrouped by Month End (Old Method):\\n\", grouped)\n",
    "\n",
    "# Resample to monthly frequency (preferred)\n",
    "resampled = ts.resample('M').mean()\n",
    "print(\"\\nResampled to Monthly (Preferred):\\n\", resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌍 **Time Zone Handling in pandas** 🌍\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Create a Time Series ---\n",
    "# Generate naive datetime index (no time zone)\n",
    "dates = pd.date_range('2023-03-12 02:00', periods=3, freq='H')\n",
    "ts = pd.Series(np.random.rand(3), index=dates)\n",
    "print(\"Naive Time Series (No Time Zone):\")\n",
    "print(ts)\n",
    "\n",
    "# --- Localize to a Time Zone ---\n",
    "# Add time zone awareness (e.g., US/Eastern)\n",
    "ts_eastern = ts.tz_localize('US/Eastern')\n",
    "print(\"\\nLocalized to US/Eastern:\")\n",
    "print(ts_eastern)\n",
    "\n",
    "# --- Convert to Another Time Zone ---\n",
    "# Convert to UTC (Coordinated Universal Time)\n",
    "ts_utc = ts_eastern.tz_convert('UTC')\n",
    "print(\"\\nConverted to UTC:\")\n",
    "print(ts_utc)\n",
    "\n",
    "# Convert to Europe/Berlin time\n",
    "ts_berlin = ts_eastern.tz_convert('Europe/Berlin')\n",
    "print(\"\\nConverted to Berlin Time:\")\n",
    "print(ts_berlin)\n",
    "\n",
    "# --- Handle Daylight Saving Time (DST) Transitions ---\n",
    "# Ambiguous time example (fall DST transition)\n",
    "ambiguous_dates = pd.date_range('2023-11-05 01:00', periods=3, freq='30T', tz='US/Eastern')\n",
    "print(\"\\nAmbiguous Times During DST Transition:\")\n",
    "print(ambiguous_dates)\n",
    "\n",
    "# Handle ambiguous times with 'NaT' (Not a Time)\n",
    "try:\n",
    "    pd.Timestamp('2023-11-05 01:30', tz='US/Eastern')\n",
    "except pytz.AmbiguousTimeError:\n",
    "    print(\"\\nAmbiguousTimeError: Use 'ambiguous='NaT' to handle\")\n",
    "\n",
    "# --- Operations with Time Zone-Aware Data ---\n",
    "# Combine two time zone-aware series (result is UTC)\n",
    "ts1 = pd.Series([1], index=pd.date_range('2023-01-01', periods=1, tz='US/Eastern'))\n",
    "ts2 = pd.Series([2], index=pd.date_range('2023-01-01', periods=1, tz='Europe/London'))\n",
    "combined = ts1 + ts2\n",
    "print(\"\\nCombined Time Series (UTC):\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📅 **Timestamps ↔ Periods Conversion** 📅\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Convert Timestamps to Periods ---\n",
    "# Create a monthly timestamp index\n",
    "dates = pd.date_range('2023-01-01', periods=3, freq='M')\n",
    "ts = pd.Series([1, 2, 3], index=dates)\n",
    "print(\"Original Timestamp Series:\")\n",
    "print(ts)\n",
    "\n",
    "# Convert to PeriodIndex (monthly periods)\n",
    "periods = ts.to_period()\n",
    "print(\"\\nConverted to Periods:\")\n",
    "print(periods)\n",
    "\n",
    "# --- 2. Convert Periods Back to Timestamps ---\n",
    "# Convert PeriodIndex to timestamps (default: start of period)\n",
    "timestamps = periods.to_timestamp()\n",
    "print(\"\\nConverted Back to Timestamps (Start):\")\n",
    "print(timestamps)\n",
    "\n",
    "# Convert to end-of-period timestamps\n",
    "timestamps_end = periods.to_timestamp(how='end')\n",
    "print(\"\\nConverted to End-of-Period Timestamps:\")\n",
    "print(timestamps_end)\n",
    "\n",
    "# --- 3. Create PeriodIndex from Arrays ---\n",
    "# Synthetic data with year and quarter columns\n",
    "data = pd.DataFrame({\n",
    "    'year': [2021, 2021, 2022],\n",
    "    'quarter': [1, 3, 4],\n",
    "    'value': [100, 200, 300]\n",
    "})\n",
    "\n",
    "# Create quarterly PeriodIndex (Q-DEC = quarters ending in December)\n",
    "period_index = pd.PeriodIndex(year=data['year'], quarter=data['quarter'], freq='Q-DEC')\n",
    "data.set_index(period_index, inplace=True)\n",
    "print(\"\\nDataFrame with Quarterly PeriodIndex:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 **Resampling Time Series Made Simple** 🔄\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Create Sample Data ---\n",
    "# Daily time series (Jan 1-12, 2023)\n",
    "dates = pd.date_range('2023-01-01', periods=12, freq='D')\n",
    "ts = pd.Series(np.random.rand(12), index=dates)\n",
    "print(\"Daily Data:\")\n",
    "print(ts.head())\n",
    "\n",
    "# --- 2. Downsampling (Daily → Monthly) ---\n",
    "# Convert daily data to monthly averages\n",
    "monthly_avg = ts.resample('M').mean()\n",
    "print(\"\\nMonthly Averages:\")\n",
    "print(monthly_avg)\n",
    "\n",
    "# --- 3. Custom Resampling (Minute Data) ---\n",
    "# Minute-level data (12 minutes starting at 00:00)\n",
    "minute_dates = pd.date_range('2023-01-01', periods=12, freq='T')\n",
    "minute_ts = pd.Series(np.arange(12), index=minute_dates)\n",
    "print(\"\\nMinute-Level Data:\")\n",
    "print(minute_ts.head())\n",
    "\n",
    "# Resample to 5-minute bins (closed on right, labeled on right)\n",
    "resampled = minute_ts.resample('5T', closed='right', label='right').sum()\n",
    "print(\"\\n5-Minute Sum (Right-Closed):\")\n",
    "print(resampled)\n",
    "\n",
    "# Adjust labels by shifting 1 second earlier\n",
    "resampled_shifted = minute_ts.resample('5T', closed='right', label='right', loffset='-1s').sum()\n",
    "print(\"\\n5-Minute Sum (Adjusted Labels):\")\n",
    "print(resampled_shifted)\n",
    "\n",
    "# --- 4. OHLC Aggregation (Finance Use Case) ---\n",
    "# Compute Open, High, Low, Close for 5-minute intervals\n",
    "ohlc = minute_ts.resample('5T').ohlc()\n",
    "print(\"\\nOHLC Resampling (5-Minute):\")\n",
    "print(ohlc)\n",
    "\n",
    "# --- 5. Upsampling (Daily → Hourly) ---\n",
    "# Convert daily data to hourly with forward fill\n",
    "upsampled = ts.resample('H').ffill()\n",
    "print(\"\\nUpsampled to Hourly (Forward-Filled):\")\n",
    "print(upsampled.head())\n",
    "\n",
    "# --- 6. Period Index Resampling ---\n",
    "# Resample to monthly periods (instead of timestamps)\n",
    "period_monthly = ts.resample('M', kind='period').sum()\n",
    "print(\"\\nMonthly Period Resampling:\")\n",
    "print(period_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔼 **Upsampling Time Series Data** 🔼\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Create Weekly Data ---\n",
    "dates = pd.date_range('2023-01-01', periods=2, freq='W-MON')  # Weekly on Mondays\n",
    "frame = pd.DataFrame({\n",
    "    'Sales': [100, 200],\n",
    "    'Expenses': [50, 75]\n",
    "}, index=dates)\n",
    "print(\"Original Weekly Data:\")\n",
    "print(frame)\n",
    "\n",
    "# --- 2. Upsample to Daily (No Aggregation) ---\n",
    "# Convert to daily frequency (introduces NaNs)\n",
    "daily_na = frame.resample('D').asfreq()\n",
    "print(\"\\nUpsampled to Daily (NaNs):\")\n",
    "print(daily_na.head())\n",
    "\n",
    "# --- 3. Forward Fill (ffill) for Interpolation ---\n",
    "# Fill missing values with previous week's data\n",
    "daily_ffill = frame.resample('D').ffill()\n",
    "print(\"\\nForward-Filled Daily Data:\")\n",
    "print(daily_ffill.head())\n",
    "\n",
    "# Limit forward fill to 2 days\n",
    "daily_ffill_limited = frame.resample('D').ffill(limit=2)\n",
    "print(\"\\nForward Fill (Limited to 2 Days):\")\n",
    "print(daily_ffill_limited.head())\n",
    "\n",
    "# --- 4. Resampling with Periods ---\n",
    "# Create monthly PeriodIndex data\n",
    "periods = pd.period_range('2023-01', '2023-12', freq='M')\n",
    "annual_data = pd.DataFrame({\n",
    "    'Revenue': np.random.rand(12)\n",
    "}, index=periods)\n",
    "print(\"\\nMonthly Period Data:\")\n",
    "print(annual_data.head())\n",
    "\n",
    "# Resample to quarterly (default: 'start' convention)\n",
    "quarterly = annual_data.resample('Q').ffill()\n",
    "print(\"\\nQuarterly Resampled (Start Convention):\")\n",
    "print(quarterly)\n",
    "\n",
    "# Resample to quarterly (end convention)\n",
    "quarterly_end = annual_data.resample('Q', convention='end').ffill()\n",
    "print(\"\\nQuarterly Resampled (End Convention):\")\n",
    "print(quarterly_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 **Moving Window Functions for Time Series** 📈\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Create Synthetic Stock Data ---\n",
    "# Generate daily dates for 2023\n",
    "dates = pd.date_range('2023-01-01', periods=250, freq='B')\n",
    "prices = pd.DataFrame({\n",
    "    'AAPL': np.random.normal(150, 20, 250).cumsum() + 150,\n",
    "    'SPX': np.random.normal(4000, 100, 250).cumsum() + 4000\n",
    "}, index=dates)\n",
    "\n",
    "# --- 2. Rolling Window Calculations ---\n",
    "# Compute 30-day moving average (rolling mean)\n",
    "rolling_mean = prices['AAPL'].rolling(30).mean()\n",
    "print(\"\\n30-Day Rolling Mean (First 5):\\n\", rolling_mean.head())\n",
    "\n",
    "# Compute 30-day rolling standard deviation\n",
    "rolling_std = prices['AAPL'].rolling(30, min_periods=10).std()\n",
    "print(\"\\n30-Day Rolling Std (First 5):\\n\", rolling_std.head())\n",
    "\n",
    "# --- 3. Exponentially Weighted Moving Average (EWMA) ---\n",
    "# EWMA with span=30 (reacts faster to recent changes)\n",
    "ewma = prices['AAPL'].ewm(span=30).mean()\n",
    "print(\"\\nEWMA (First 5):\\n\", ewma.head())\n",
    "\n",
    "# --- 4. Plotting Moving Averages ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "prices['AAPL'].plot(label='Daily Prices', alpha=0.5)\n",
    "rolling_mean.plot(label='30-Day MA', style='k--')\n",
    "ewma.plot(label='30-Day EWMA', style='r-')\n",
    "plt.title(\"Apple Stock Price vs. Moving Averages\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Binary Rolling Functions (Correlation) ---\n",
    "# Compute percentage returns\n",
    "returns = prices.pct_change()\n",
    "\n",
    "# Rolling 60-day correlation between AAPL and SPX\n",
    "rolling_corr = returns['AAPL'].rolling(60).corr(returns['SPX'])\n",
    "print(\"\\n60-Day Rolling Correlation (First 5):\\n\", rolling_corr.head())\n",
    "\n",
    "# Plot the rolling correlation\n",
    "plt.figure(figsize=(10, 3))\n",
    "rolling_corr.plot(title=\"Rolling 60-Day Correlation: AAPL vs. SPX\")\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# --- 6. Expanding Window (Cumulative Mean) ---\n",
    "# Compute cumulative mean from start to current date\n",
    "expanding_mean = prices['AAPL'].expanding().mean()\n",
    "print(\"\\nExpanding Mean (First 5):\\n\", expanding_mean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 **Pandas Time Series & GroupBy Essentials** 📊\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from scipy.stats import percentileofscore  # For custom rolling function\n",
    "\n",
    "# --- 1. GroupBy Mechanics ---\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Values': np.random.rand(6),\n",
    "    'Weights': np.random.randint(1, 10, 6)\n",
    "})\n",
    "\n",
    "# Group by 'Category' and compute mean\n",
    "grouped = df.groupby('Category')\n",
    "print(\"GroupBy Mean:\\n\", grouped['Values'].mean())\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nGroupBy Aggregations:\\n\", grouped.agg(['mean', 'sum']))\n",
    "\n",
    "# Custom function: Weighted average\n",
    "def weighted_avg(group):\n",
    "    return (group['Values'] * group['Weights']).sum() / group['Weights'].sum()\n",
    "\n",
    "print(\"\\nWeighted Average by Category:\\n\", grouped.apply(weighted_avg))\n",
    "\n",
    "# --- 2. Pivot Tables & Cross-Tabulation ---\n",
    "# Create a synthetic dataset\n",
    "data = pd.DataFrame({\n",
    "    'Movie': ['Movie 1', 'Movie 2', 'Movie 3'],\n",
    "    'Genres': ['Action|Adventure', 'Comedy|Romance', 'Action|Comedy'],\n",
    "    'Rating': [8.5, 7.2, 6.9]\n",
    "})\n",
    "\n",
    "# Cross-tabulation of genres\n",
    "cross_tab = pd.crosstab(data['Genres'], data['Rating'])\n",
    "print(\"\\nCross-Tabulation (Genres vs Ratings):\\n\", cross_tab)\n",
    "\n",
    "# Pivot table for multi-level aggregation\n",
    "pivot = data.pivot_table(index='Genres', values='Rating', aggfunc=['mean', 'max'])\n",
    "print(\"\\nPivot Table (Genres Aggregation):\\n\", pivot)\n",
    "\n",
    "# --- 3. Time Series Basics ---\n",
    "# Generate daily dates\n",
    "dates = pd.date_range('2023-01-01', periods=5, freq='D')\n",
    "ts = pd.Series(np.random.rand(5), index=dates)\n",
    "print(\"\\nTime Series with Daily Index:\\n\", ts)\n",
    "\n",
    "# Convert to monthly periods\n",
    "periods = ts.to_period('M')\n",
    "print(\"\\nConverted to Monthly Periods:\\n\", periods)\n",
    "\n",
    "# --- 4. Time Zone Handling ---\n",
    "# Localize to UTC and convert to New York time\n",
    "ts_utc = ts.tz_localize('UTC')\n",
    "ts_ny = ts_utc.tz_convert('America/New_York')\n",
    "print(\"\\nTime Zone Conversion (UTC → NY):\\n\", ts_ny)\n",
    "\n",
    "# --- 5. Resampling ---\n",
    "# Resample daily data to monthly mean\n",
    "monthly = ts.resample('M').mean()\n",
    "print(\"\\nMonthly Resampled Data:\\n\", monthly)\n",
    "\n",
    "# Upsample with forward fill\n",
    "upsampled = ts.resample('12H').ffill()\n",
    "print(\"\\nUpsampled to 12-Hourly (Forward-Filled):\\n\", upsampled.head())\n",
    "\n",
    "# --- 6. Moving Window Functions ---\n",
    "# Rolling 3-day mean\n",
    "rolling_mean = ts.rolling(3).mean()\n",
    "print(\"\\n3-Day Rolling Mean:\\n\", rolling_mean)\n",
    "\n",
    "# Exponentially weighted moving average (EWMA)\n",
    "ewma = ts.ewm(span=3).mean()\n",
    "print(\"\\n3-Day EWMA:\\n\", ewma)\n",
    "\n",
    "# Custom rolling function: Percentile of score\n",
    "returns = pd.Series(np.random.normal(0, 0.1, 100), index=pd.date_range('2023-01-01', periods=100, freq='B'))\n",
    "percentile_rank = returns.rolling(30).apply(lambda x: percentileofscore(x, 0.02))\n",
    "print(\"\\n30-Day Rolling Percentile Rank (Sample):\\n\", percentile_rank.head())\n",
    "\n",
    "# Plot rolling statistics\n",
    "plt.figure(figsize=(10, 4))\n",
    "ts.plot(label='Daily Data')\n",
    "rolling_mean.plot(label='3-Day MA')\n",
    "ewma.plot(label='3-Day EWMA')\n",
    "plt.title(\"Moving Averages vs. Raw Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 12 - Advanced pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
